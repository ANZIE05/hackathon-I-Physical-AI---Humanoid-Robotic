"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[85],{5599:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"module-3/vslam-practical-labs","title":"VSLAM Practical Labs","description":"Overview","source":"@site/docs/module-3/vslam-practical-labs.md","sourceDirName":"module-3","slug":"/module-3/vslam-practical-labs","permalink":"/docs/module-3/vslam-practical-labs","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3/vslam-practical-labs.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5}}');var r=t(4848),s=t(8453);const i={sidebar_position:5},o="VSLAM Practical Labs",l={},p=[{value:"Overview",id:"overview",level:2},{value:"Lab 1: Basic Feature Detection and Tracking",id:"lab-1-basic-feature-detection-and-tracking",level:2},{value:"Objective",id:"objective",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Step 1: Feature Detection Implementation",id:"step-1-feature-detection-implementation",level:4},{value:"Step 2: Feature Matching and Validation",id:"step-2-feature-matching-and-validation",level:4},{value:"Lab Exercise 1: Feature Detection Optimization",id:"lab-exercise-1-feature-detection-optimization",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Lab 2: Pose Estimation and Essential Matrix",id:"lab-2-pose-estimation-and-essential-matrix",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Implementation Steps",id:"implementation-steps-1",level:3},{value:"Step 1: Essential Matrix Estimation",id:"step-1-essential-matrix-estimation",level:4},{value:"Step 2: Bundle Adjustment Implementation",id:"step-2-bundle-adjustment-implementation",level:4},{value:"Lab Exercise 2: Pose Estimation Evaluation",id:"lab-exercise-2-pose-estimation-evaluation",level:3},{value:"Expected Results",id:"expected-results-1",level:3},{value:"Lab 3: Map Building and Loop Closure",id:"lab-3-map-building-and-loop-closure",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Implementation Steps",id:"implementation-steps-2",level:3},{value:"Step 1: Map Building and Point Cloud Construction",id:"step-1-map-building-and-point-cloud-construction",level:4},{value:"Step 2: Loop Closure Detection",id:"step-2-loop-closure-detection",level:4},{value:"Lab Exercise 3: Map Building and Loop Closure Testing",id:"lab-exercise-3-map-building-and-loop-closure-testing",level:3},{value:"Expected Results",id:"expected-results-2",level:3},{value:"Lab 4: Isaac Sim Integration",id:"lab-4-isaac-sim-integration",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Implementation Steps",id:"implementation-steps-3",level:3},{value:"Step 1: Isaac Sim Environment Setup",id:"step-1-isaac-sim-environment-setup",level:4},{value:"Step 2: ROS 2 Bridge Integration",id:"step-2-ros-2-bridge-integration",level:4},{value:"Lab Exercise 4: Complete VSLAM Integration",id:"lab-exercise-4-complete-vslam-integration",level:3},{value:"Expected Results",id:"expected-results-3",level:3},{value:"Performance Optimization and Evaluation",id:"performance-optimization-and-evaluation",level:2},{value:"Real-time Performance Considerations",id:"real-time-performance-considerations",level:3},{value:"Evaluation and Validation",id:"evaluation-and-validation",level:2},{value:"VSLAM Evaluation Framework",id:"vslam-evaluation-framework",level:3},{value:"Troubleshooting and Best Practices",id:"troubleshooting-and-best-practices",level:2},{value:"Common VSLAM Issues and Solutions",id:"common-vslam-issues-and-solutions",level:3},{value:"Practical Lab: Complete VSLAM System",id:"practical-lab-complete-vslam-system",level:2},{value:"Lab Objective",id:"lab-objective",level:3},{value:"Implementation Steps",id:"implementation-steps-4",level:3},{value:"Expected Outcome",id:"expected-outcome",level:3},{value:"Review Questions",id:"review-questions",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vslam-practical-labs",children:"VSLAM Practical Labs"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This section provides hands-on practical laboratories for implementing and validating Visual Simultaneous Localization and Mapping (VSLAM) systems. The labs progress from basic concepts to advanced integration with Isaac Sim and ROS 2 for Physical AI and Humanoid Robotics applications."}),"\n",(0,r.jsx)(n.h2,{id:"lab-1-basic-feature-detection-and-tracking",children:"Lab 1: Basic Feature Detection and Tracking"}),"\n",(0,r.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,r.jsx)(n.p,{children:"Implement fundamental computer vision components for VSLAM including feature detection, description, and tracking using ORB features."}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Basic Python and OpenCV knowledge"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of feature detection concepts"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble installation"}),"\n",(0,r.jsx)(n.li,{children:"Basic familiarity with Isaac Sim concepts"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-feature-detection-implementation",children:"Step 1: Feature Detection Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# vslam_lab_1/feature_detector.py\nimport cv2\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\nclass FeatureDetector:\n    def __init__(self, max_features: int = 1000, scale_factor: float = 1.2):\n        """\n        Initialize feature detector with ORB\n\n        Args:\n            max_features: Maximum number of features to detect\n            scale_factor: Pyramid scale factor\n        """\n        self.max_features = max_features\n        self.orb = cv2.ORB_create(\n            nfeatures=max_features,\n            scaleFactor=scale_factor,\n            nlevels=8,\n            edgeThreshold=31,\n            patchSize=31,\n            fastThreshold=20\n        )\n\n    def detect_and_compute(self, image: np.ndarray) -> Tuple[List[cv2.KeyPoint], np.ndarray]:\n        """\n        Detect and compute ORB features for an image\n\n        Args:\n            image: Input image (grayscale or color)\n\n        Returns:\n            Tuple of (keypoints, descriptors)\n        """\n        # Convert to grayscale if needed\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image\n\n        # Detect keypoints and compute descriptors\n        keypoints, descriptors = self.orb.detectAndCompute(gray, None)\n\n        if descriptors is None:\n            # Return empty descriptors if no features detected\n            return [], np.array([]).reshape(0, 0)\n\n        return keypoints, descriptors\n\n    def visualize_features(self, image: np.ndarray, keypoints: List[cv2.KeyPoint]) -> np.ndarray:\n        """\n        Visualize detected features on image\n\n        Args:\n            image: Input image\n            keypoints: Detected keypoints\n\n        Returns:\n            Image with features drawn\n        """\n        if len(image.shape) == 2:\n            # Convert grayscale to color for visualization\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n\n        # Draw keypoints\n        result = cv2.drawKeypoints(\n            image, keypoints, None,\n            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n        )\n\n        return result\n\nclass FeatureTracker:\n    def __init__(self, match_threshold: float = 0.75):\n        """\n        Initialize feature tracker\n\n        Args:\n            match_threshold: Lowe\'s ratio test threshold\n        """\n        self.match_threshold = match_threshold\n        self.flann = cv2.FlannBasedMatcher(\n            {\'algorithm\': 6, \'table_number\': 6, \'key_size\': 12, \'multi_probe_level\': 1},\n            {\'checks\': 50}\n        )\n\n    def match_features(self, desc1: np.ndarray, desc2: np.ndarray) -> List[cv2.DMatch]:\n        """\n        Match features between two descriptor sets\n\n        Args:\n            desc1: Descriptors from first image\n            desc2: Descriptors from second image\n\n        Returns:\n            List of good matches after Lowe\'s ratio test\n        """\n        if desc1 is None or desc2 is None or desc1.size == 0 or desc2.size == 0:\n            return []\n\n        try:\n            # Find 2 nearest neighbors for each descriptor\n            matches = self.flann.knnMatch(desc1, desc2, k=2)\n\n            # Apply Lowe\'s ratio test\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance < self.match_threshold * n.distance:\n                        good_matches.append(m)\n\n            return good_matches\n        except cv2.error:\n            # Fallback to brute force matcher if FLANN fails\n            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n            matches = bf.knnMatch(desc1, desc2, k=2)\n\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance < self.match_threshold * n.distance:\n                        good_matches.append(m)\n\n            return good_matches\n\n    def track_features(self, kp1: List[cv2.KeyPoint], kp2: List[cv2.KeyPoint],\n                      matches: List[cv2.DMatch]) -> Tuple[np.ndarray, np.ndarray]:\n        """\n        Extract matched keypoint coordinates\n\n        Args:\n            kp1: Keypoints from first image\n            kp2: Keypoints from second image\n            matches: Good matches\n\n        Returns:\n            Tuple of (pts1, pts2) - matched point coordinates\n        """\n        if len(matches) == 0:\n            return np.array([]), np.array([])\n\n        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        return pts1, pts2\n\ndef main():\n    """Main function for testing feature detection and tracking"""\n    import sys\n\n    if len(sys.argv) != 2:\n        print("Usage: python feature_detector.py <image_path>")\n        sys.exit(1)\n\n    # Load image\n    image_path = sys.argv[1]\n    image = cv2.imread(image_path)\n\n    if image is None:\n        print(f"Could not load image: {image_path}")\n        sys.exit(1)\n\n    # Initialize feature detector\n    detector = FeatureDetector(max_features=500)\n\n    # Detect and compute features\n    keypoints, descriptors = detector.detect_and_compute(image)\n\n    print(f"Detected {len(keypoints)} features")\n\n    # Visualize features\n    feature_image = detector.visualize_features(image, keypoints)\n\n    # Display result\n    cv2.imshow(\'Detected Features\', feature_image)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(n.h4,{id:"step-2-feature-matching-and-validation",children:"Step 2: Feature Matching and Validation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# vslam_lab_1/matcher_validator.py\nimport cv2\nimport numpy as np\nfrom feature_detector import FeatureDetector, FeatureTracker\n\nclass FeatureMatcherValidator:\n    def __init__(self):\n        self.detector = FeatureDetector(max_features=1000)\n        self.tracker = FeatureTracker(match_threshold=0.75)\n\n    def validate_feature_matching(self, image1: np.ndarray, image2: np.ndarray) -> dict:\n        """\n        Validate feature matching between two images\n\n        Args:\n            image1: First image\n            image2: Second image (similar scene)\n\n        Returns:\n            Dictionary with validation metrics\n        """\n        # Detect features in both images\n        kp1, desc1 = self.detector.detect_and_compute(image1)\n        kp2, desc2 = self.detector.detect_and_compute(image2)\n\n        # Match features\n        matches = self.tracker.match_features(desc1, desc2)\n\n        # Calculate validation metrics\n        metrics = {\n            \'num_features_img1\': len(kp1),\n            \'num_features_img2\': len(kp2),\n            \'num_matches\': len(matches),\n            \'match_ratio\': len(matches) / max(len(kp1), len(kp2), 1) if len(kp1) > 0 or len(kp2) > 0 else 0,\n            \'match_quality\': self.assess_match_quality(matches, kp1, kp2)\n        }\n\n        return metrics\n\n    def assess_match_quality(self, matches: List[cv2.DMatch], kp1: List[cv2.KeyPoint],\n                           kp2: List[cv2.KeyPoint]) -> float:\n        """\n        Assess the quality of feature matches\n\n        Args:\n            matches: Good matches\n            kp1: Keypoints from first image\n            kp2: Keypoints from second image\n\n        Returns:\n            Quality score (0-1)\n        """\n        if len(matches) < 10:  # Need minimum matches for meaningful assessment\n            return 0.0\n\n        # Calculate average match distance\n        avg_distance = np.mean([m.distance for m in matches])\n\n        # Calculate spatial distribution of matches\n        if len(matches) > 1:\n            pts1, pts2 = self.tracker.track_features(kp1, kp2, matches)\n\n            # Calculate variance of match locations (should be spread out)\n            if pts1.size > 0:\n                variance1 = np.var(pts1, axis=0).mean()\n                variance2 = np.var(pts2, axis=0).mean()\n                spatial_score = min(variance1, variance2) / 1000.0  # Normalize\n            else:\n                spatial_score = 0.0\n        else:\n            spatial_score = 0.0\n\n        # Combine metrics (simple heuristic)\n        # Lower average distance is better (higher quality)\n        distance_score = max(0, 1.0 - (avg_distance / 100.0))  # Assume 100 is high distance\n        overall_score = 0.6 * distance_score + 0.4 * spatial_score\n\n        return min(1.0, max(0.0, overall_score))\n\n    def visualize_matches(self, image1: np.ndarray, image2: np.ndarray,\n                        kp1: List[cv2.KeyPoint], kp2: List[cv2.KeyPoint],\n                        matches: List[cv2.DMatch]) -> np.ndarray:\n        """\n        Visualize feature matches between two images\n\n        Args:\n            image1: First image\n            image2: Second image\n            kp1: Keypoints from first image\n            kp2: Keypoints from second image\n            matches: Good matches\n\n        Returns:\n            Image with matches drawn\n        """\n        # Convert to color if grayscale\n        if len(image1.shape) == 2:\n            image1 = cv2.cvtColor(image1, cv2.COLOR_GRAY2BGR)\n        if len(image2.shape) == 2:\n            image2 = cv2.cvtColor(image2, cv2.COLOR_GRAY2BGR)\n\n        # Create side-by-side image\n        h1, w1 = image1.shape[:2]\n        h2, w2 = image2.shape[:2]\n        vis_image = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n        vis_image[:h1, :w1] = image1\n        vis_image[:h2, w1:w1+w2] = image2\n\n        # Draw matches\n        for match in matches[:50]:  # Limit number of matches shown\n            pt1 = tuple(map(int, kp1[match.queryIdx].pt))\n            pt2 = tuple(map(int, kp2[match.trainIdx].pt))\n            pt2_offset = (pt2[0] + w1, pt2[1])  # Offset for second image\n\n            # Draw line and points\n            cv2.line(vis_image, pt1, pt2_offset, (0, 255, 0), 1)\n            cv2.circle(vis_image, pt1, 3, (255, 0, 0), -1)\n            cv2.circle(vis_image, pt2_offset, 3, (255, 0, 0), -1)\n\n        return vis_image\n\ndef test_feature_validation():\n    """Test function for feature validation"""\n    import sys\n\n    if len(sys.argv) != 3:\n        print("Usage: python matcher_validator.py <image1_path> <image2_path>")\n        sys.exit(1)\n\n    image1_path = sys.argv[1]\n    image2_path = sys.argv[2]\n\n    image1 = cv2.imread(image1_path)\n    image2 = cv2.imread(image2_path)\n\n    if image1 is None or image2 is None:\n        print("Could not load images")\n        sys.exit(1)\n\n    validator = FeatureMatcherValidator()\n\n    # Validate feature matching\n    metrics = validator.validate_feature_matching(image1, image2)\n\n    print("Feature Matching Validation Results:")\n    print(f"- Features in image 1: {metrics[\'num_features_img1\']}")\n    print(f"- Features in image 2: {metrics[\'num_features_img2\']}")\n    print(f"- Number of matches: {metrics[\'num_matches\']}")\n    print(f"- Match ratio: {metrics[\'match_ratio\']:.3f}")\n    print(f"- Match quality: {metrics[\'match_quality\']:.3f}")\n\n    # Show visualization if matches exist\n    if metrics[\'num_matches\'] > 0:\n        kp1, desc1 = validator.detector.detect_and_compute(image1)\n        kp2, desc2 = validator.detector.detect_and_compute(image2)\n        matches = validator.tracker.match_features(desc1, desc2)\n\n        vis_image = validator.visualize_matches(image1, image2, kp1, kp2, matches)\n        cv2.imshow(\'Feature Matches\', vis_image)\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n\nif __name__ == "__main__":\n    test_feature_validation()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"lab-exercise-1-feature-detection-optimization",children:"Lab Exercise 1: Feature Detection Optimization"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Test the feature detector with different image types (indoor/outdoor, textured/plain)"}),"\n",(0,r.jsx)(n.li,{children:"Experiment with different ORB parameters (nfeatures, scaleFactor, edgeThreshold)"}),"\n",(0,r.jsx)(n.li,{children:"Analyze the trade-off between feature count and matching quality"}),"\n",(0,r.jsx)(n.li,{children:"Implement a feature density optimization algorithm"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Working feature detection and matching system"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of ORB parameters and their effects"}),"\n",(0,r.jsx)(n.li,{children:"Ability to visualize and validate feature matches"}),"\n",(0,r.jsx)(n.li,{children:"Recognition of scenarios where feature detection works well/poorly"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"lab-2-pose-estimation-and-essential-matrix",children:"Lab 2: Pose Estimation and Essential Matrix"}),"\n",(0,r.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,r.jsx)(n.p,{children:"Implement pose estimation from feature correspondences using essential matrix decomposition and RANSAC for robust estimation."}),"\n",(0,r.jsx)(n.h3,{id:"implementation-steps-1",children:"Implementation Steps"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-essential-matrix-estimation",children:"Step 1: Essential Matrix Estimation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# vslam_lab_2/pose_estimator.py\nimport cv2\nimport numpy as np\nfrom typing import Tuple, Optional\nfrom feature_detector import FeatureDetector, FeatureTracker\n\nclass PoseEstimator:\n    def __init__(self, camera_matrix: np.ndarray):\n        \"\"\"\n        Initialize pose estimator with camera calibration\n\n        Args:\n            camera_matrix: 3x3 camera intrinsic matrix\n        \"\"\"\n        self.K = camera_matrix\n        self.ransac_threshold = 1.0  # Reprojection error threshold\n        self.min_inliers = 10        # Minimum inliers for valid pose\n\n    def estimate_pose_from_features(self,\n                                  pts1: np.ndarray,\n                                  pts2: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], np.ndarray]:\n        \"\"\"\n        Estimate relative pose from feature correspondences using essential matrix\n\n        Args:\n            pts1: Points in first image (Nx2)\n            pts2: Points in second image (Nx2)\n\n        Returns:\n            Tuple of (rotation, translation, inlier_mask) or (None, None, empty_mask)\n        \"\"\"\n        if pts1.shape[0] < 8 or pts2.shape[0] < 8:\n            return None, None, np.array([], dtype=bool)\n\n        try:\n            # Estimate essential matrix using RANSAC\n            E, mask = cv2.findEssentialMat(\n                pts1, pts2, self.K,\n                method=cv2.RANSAC,\n                prob=0.999,\n                threshold=self.ransac_threshold\n            )\n\n            if E is None or E.size == 0:\n                return None, None, np.array([], dtype=bool)\n\n            # Recover pose from essential matrix\n            _, R, t, mask_new = cv2.recoverPose(E, pts1, pts2, self.K, mask=mask)\n\n            # Combine rotation and translation into transformation matrix\n            T = np.eye(4)\n            T[:3, :3] = R\n            T[:3, 3] = t.ravel()\n\n            # Create inlier mask\n            if mask is not None and mask_new is not None:\n                inlier_mask = (mask.flatten() > 0) & (mask_new.flatten() > 0)\n            elif mask is not None:\n                inlier_mask = mask.flatten() > 0\n            elif mask_new is not None:\n                inlier_mask = mask_new.flatten() > 0\n            else:\n                inlier_mask = np.ones(pts1.shape[0], dtype=bool)\n\n            return R, t, inlier_mask\n\n        except cv2.error:\n            return None, None, np.array([], dtype=bool)\n\n    def triangulate_points(self,\n                          R1: np.ndarray, t1: np.ndarray,\n                          R2: np.ndarray, t2: np.ndarray,\n                          pts1: np.ndarray, pts2: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Triangulate 3D points from stereo correspondences\n\n        Args:\n            R1, t1: Pose of first camera\n            R2, t2: Pose of second camera\n            pts1: Points in first image\n            pts2: Points in second image\n\n        Returns:\n            3D points (Nx3)\n        \"\"\"\n        # Create projection matrices\n        P1 = self.K @ np.hstack([R1, t1.reshape(3, 1)])\n        P2 = self.K @ np.hstack([R2, t2.reshape(3, 1)])\n\n        # Triangulate points\n        points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)\n\n        # Convert from homogeneous to Euclidean coordinates\n        points_3d = points_4d[:3] / points_4d[3]\n\n        return points_3d.T\n\n    def compute_reprojection_error(self,\n                                 points_3d: np.ndarray,\n                                 R: np.ndarray, t: np.ndarray,\n                                 observed_points: np.ndarray) -> float:\n        \"\"\"\n        Compute reprojection error for 3D points\n\n        Args:\n            points_3d: 3D points (Nx3)\n            R: Rotation matrix\n            t: Translation vector\n            observed_points: Observed 2D points (Nx2)\n\n        Returns:\n            Average reprojection error\n        \"\"\"\n        # Project 3D points to 2D\n        projected_points = cv2.projectPoints(\n            points_3d.reshape(-1, 1, 3),\n            R, t, self.K, None\n        )[0].reshape(-1, 2)\n\n        # Calculate reprojection error\n        errors = np.linalg.norm(projected_points - observed_points, axis=1)\n        avg_error = np.mean(errors)\n\n        return avg_error\n\n    def validate_pose_estimation(self, R: np.ndarray, t: np.ndarray) -> bool:\n        \"\"\"\n        Validate estimated pose for physical plausibility\n\n        Args:\n            R: Rotation matrix\n            t: Translation vector\n\n        Returns:\n            True if pose is physically plausible\n        \"\"\"\n        # Check rotation matrix properties\n        det_R = np.linalg.det(R)\n        orthogonality_error = np.linalg.norm(R @ R.T - np.eye(3))\n\n        # Check translation magnitude (reasonable for robot motion)\n        translation_magnitude = np.linalg.norm(t)\n\n        # Validation criteria\n        valid_rotation = abs(det_R - 1.0) < 0.1 and orthogonality_error < 0.1\n        reasonable_translation = 0.01 < translation_magnitude < 10.0  # Between 1cm and 10m\n\n        return valid_rotation and reasonable_translation\n\nclass VSLAMPipeline:\n    def __init__(self, camera_matrix: np.ndarray):\n        self.camera_matrix = camera_matrix\n        self.detector = FeatureDetector(max_features=1000)\n        self.tracker = FeatureTracker(match_threshold=0.75)\n        self.pose_estimator = PoseEstimator(camera_matrix)\n\n        # State variables\n        self.current_pose = np.eye(4)\n        self.keyframes = []\n        self.map_points = []\n\n    def process_frame_pair(self, image1: np.ndarray, image2: np.ndarray) -> dict:\n        \"\"\"\n        Process a pair of frames to estimate relative motion\n\n        Args:\n            image1: First image\n            image2: Second image (subsequent frame)\n\n        Returns:\n            Dictionary with processing results\n        \"\"\"\n        # Extract features from both images\n        kp1, desc1 = self.detector.detect_and_compute(image1)\n        kp2, desc2 = self.detector.detect_and_compute(image2)\n\n        if len(kp1) < 10 or len(kp2) < 10:\n            return {\n                'success': False,\n                'reason': 'Insufficient features',\n                'num_features1': len(kp1),\n                'num_features2': len(kp2)\n            }\n\n        # Match features\n        matches = self.tracker.match_features(desc1, desc2)\n\n        if len(matches) < 10:\n            return {\n                'success': False,\n                'reason': 'Insufficient matches',\n                'num_matches': len(matches)\n            }\n\n        # Extract matched points\n        pts1, pts2 = self.tracker.track_features(kp1, kp2, matches)\n\n        if pts1.size < 8:\n            return {\n                'success': False,\n                'reason': 'Insufficient inliers after matching',\n                'num_points': pts1.size // 2\n            }\n\n        # Estimate relative pose\n        R_rel, t_rel, inlier_mask = self.pose_estimator.estimate_pose_from_features(\n            pts1[inlier_mask], pts2[inlier_mask]\n        )\n\n        if R_rel is None or t_rel is None:\n            return {\n                'success': False,\n                'reason': 'Pose estimation failed',\n                'num_inliers': inlier_mask.sum()\n            }\n\n        # Validate pose\n        is_valid = self.pose_estimator.validate_pose_estimation(R_rel, t_rel)\n\n        if not is_valid:\n            return {\n                'success': False,\n                'reason': 'Estimated pose is not physically plausible',\n                'R': R_rel,\n                't': t_rel\n            }\n\n        # Create transformation matrix\n        T_rel = np.eye(4)\n        T_rel[:3, :3] = R_rel\n        T_rel[:3, 3] = t_rel.ravel()\n\n        # Update global pose\n        self.current_pose = self.current_pose @ T_rel\n\n        # Calculate results\n        results = {\n            'success': True,\n            'relative_pose': T_rel,\n            'absolute_pose': self.current_pose,\n            'num_matches': len(matches),\n            'num_inliers': inlier_mask.sum(),\n            'inlier_ratio': inlier_mask.sum() / len(matches),\n            'translation_magnitude': np.linalg.norm(t_rel),\n            'rotation_angle': self.rotation_matrix_to_angle(R_rel)\n        }\n\n        return results\n\n    def rotation_matrix_to_angle(self, R: np.ndarray) -> float:\n        \"\"\"Convert rotation matrix to rotation angle in radians\"\"\"\n        trace = np.trace(R)\n        angle = np.arccos(max(-1, min(1, (trace - 1) / 2)))\n        return angle\n\ndef test_pose_estimation():\n    \"\"\"Test function for pose estimation\"\"\"\n    # Example camera matrix (replace with actual calibration)\n    K = np.array([\n        [525.0, 0.0, 319.5],\n        [0.0, 525.0, 239.5],\n        [0.0, 0.0, 1.0]\n    ])\n\n    # Initialize pipeline\n    pipeline = VSLAMPipeline(K)\n\n    import sys\n    if len(sys.argv) != 3:\n        print(\"Usage: python pose_estimator.py <image1_path> <image2_path>\")\n        sys.exit(1)\n\n    image1_path = sys.argv[1]\n    image2_path = sys.argv[2]\n\n    image1 = cv2.imread(image1_path)\n    image2 = cv2.imread(image2_path)\n\n    if image1 is None or image2 is None:\n        print(\"Could not load images\")\n        sys.exit(1)\n\n    # Process frame pair\n    results = pipeline.process_frame_pair(image1, image2)\n\n    print(\"Pose Estimation Results:\")\n    if results['success']:\n        print(f\"- Relative pose computed successfully\")\n        print(f\"- Translation magnitude: {results['translation_magnitude']:.3f}\")\n        print(f\"- Rotation angle: {np.degrees(results['rotation_angle']):.2f} degrees\")\n        print(f\"- Inlier ratio: {results['inlier_ratio']:.3f}\")\n        print(f\"- Number of inliers: {results['num_inliers']}\")\n        print(f\"- Absolute pose:\\n{results['absolute_pose']}\")\n    else:\n        print(f\"- Failed: {results['reason']}\")\n\nif __name__ == \"__main__\":\n    test_pose_estimation()\n"})}),"\n",(0,r.jsx)(n.h4,{id:"step-2-bundle-adjustment-implementation",children:"Step 2: Bundle Adjustment Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# vslam_lab_2/bundle_adjustment.py\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.optimize import least_squares\nfrom typing import List, Dict, Tuple\n\nclass BundleAdjustment:\n    def __init__(self):\n        self.max_iterations = 50\n        self.robust_loss = True  # Use robust loss function (Huber)\n\n    def residual_function(self, params,\n                         points_3d_indices,\n                         camera_indices,\n                         points_2d,\n                         camera_matrix):\n        \"\"\"\n        Residual function for bundle adjustment optimization\n\n        Args:\n            params: Flattened array of [camera_params, points_3d_params]\n            points_3d_indices: Indices of 3D points for each observation\n            camera_indices: Indices of cameras for each observation\n            points_2d: Observed 2D points\n            camera_matrix: Camera intrinsic matrix\n\n        Returns:\n            Flattened array of residuals\n        \"\"\"\n        n_cameras = len(np.unique(camera_indices))\n        n_points = len(np.unique(points_3d_indices))\n\n        # Reshape parameters\n        camera_params = params[:n_cameras * 6].reshape((n_cameras, 6))  # [R (3), t (3)]\n        points_3d = params[n_cameras * 6:].reshape((n_points, 3))\n\n        residuals = []\n\n        for i in range(len(points_2d)):\n            camera_idx = camera_indices[i]\n            point_idx = points_3d_indices[i]\n\n            # Extract camera pose (Rodrigues vector + translation)\n            rvec = camera_params[camera_idx, :3]\n            tvec = camera_params[camera_idx, 3:]\n\n            # Get 3D point\n            point_3d = points_3d[point_idx]\n\n            # Project 3D point to 2D\n            projected, _ = cv2.projectPoints(\n                point_3d.reshape(1, 1, 3),\n                rvec, tvec,\n                camera_matrix,\n                None\n            )\n\n            projected = projected[0, 0]\n            observed = points_2d[i]\n\n            # Calculate residual\n            residual = projected - observed\n            residuals.extend(residual)\n\n        return np.array(residuals)\n\n    def run_bundle_adjustment(self,\n                             camera_poses: List[np.ndarray],\n                             points_3d: List[np.ndarray],\n                             observations: List[Dict],\n                             camera_matrix: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n        \"\"\"\n        Run bundle adjustment optimization\n\n        Args:\n            camera_poses: List of 4x4 camera poses [R|t]\n            points_3d: List of 3D points\n            observations: List of observations (camera_idx, point_idx, point_2d)\n            camera_matrix: Camera intrinsic matrix\n\n        Returns:\n            Tuple of (optimized_camera_poses, optimized_points_3d)\n        \"\"\"\n        n_cameras = len(camera_poses)\n        n_points = len(points_3d)\n\n        # Convert camera poses to parameter form [R (Rodrigues), t]\n        camera_params = []\n        for pose in camera_poses:\n            R = pose[:3, :3]\n            t = pose[:3, 3]\n\n            # Convert rotation matrix to Rodrigues vector\n            rvec, _ = cv2.Rodrigues(R)\n            params = np.concatenate([rvec.ravel(), t.ravel()])\n            camera_params.append(params)\n\n        camera_params = np.array(camera_params).flatten()\n\n        # Prepare points and observations\n        points_3d_array = np.array(points_3d)\n        points_3d_flat = points_3d_array.flatten()\n\n        # Combine all parameters\n        all_params = np.concatenate([camera_params, points_3d_flat])\n\n        # Prepare observation data\n        points_3d_indices = np.array([obs['point_idx'] for obs in observations])\n        camera_indices = np.array([obs['camera_idx'] for obs in observations])\n        points_2d = np.array([obs['point_2d'] for obs in observations])\n\n        # Define Jacobian sparsity structure (sparse optimization)\n        def jac_sparsity(n_cameras, n_points, observations):\n            m = len(observations) * 2  # Each observation contributes 2 residuals (x, y)\n            n = n_cameras * 6 + n_points * 3  # 6 params per camera, 3 per point\n\n            S = lil_matrix((m, n), dtype=int)\n\n            for i, obs in enumerate(observations):\n                camera_idx = obs['camera_idx']\n                point_idx = obs['point_idx']\n\n                # Residuals for this observation (2 residuals: x, y)\n                for residual_j in range(2):  # x and y residuals\n                    # Camera parameters (6 parameters)\n                    for param_k in range(6):\n                        S[2*i + residual_j, camera_idx * 6 + param_k] = 1\n\n                    # Point parameters (3 parameters)\n                    for param_k in range(3):\n                        S[2*i + residual_j, n_cameras * 6 + point_idx * 3 + param_k] = 1\n\n            return S\n\n        # Create sparsity matrix\n        sparsity = jac_sparsity(n_cameras, n_points, observations)\n\n        # Run optimization\n        result = least_squares(\n            self.residual_function,\n            all_params,\n            jac_sparsity=sparsity,\n            method='trf',\n            ftol=1e-8,\n            xtol=1e-8,\n            gtol=1e-8,\n            max_nfev=200,\n            args=(points_3d_indices, camera_indices, points_2d, camera_matrix)\n        )\n\n        # Extract optimized parameters\n        optimized_camera_params = result.x[:n_cameras * 6].reshape((n_cameras, 6))\n        optimized_points_3d = result.x[n_cameras * 6:].reshape((n_points, 3))\n\n        # Convert back to poses\n        optimized_poses = []\n        for params in optimized_camera_params:\n            rvec = params[:3]\n            tvec = params[3:]\n\n            # Convert Rodrigues vector back to rotation matrix\n            R, _ = cv2.Rodrigues(rvec)\n\n            pose = np.eye(4)\n            pose[:3, :3] = R\n            pose[:3, 3] = tvec\n            optimized_poses.append(pose)\n\n        return optimized_poses, optimized_points_3d.tolist()\n\n    def optimize_local_window(self,\n                             keyframes: List[Dict],\n                             local_map: List[Dict],\n                             camera_matrix: np.ndarray) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Optimize a local window of keyframes and associated map points\n\n        Args:\n            keyframes: List of keyframes with poses and observations\n            local_map: List of local map points\n            camera_matrix: Camera intrinsic matrix\n\n        Returns:\n            Tuple of (optimized_keyframes, optimized_local_map)\n        \"\"\"\n        # Prepare data for bundle adjustment\n        camera_poses = [kf['pose'] for kf in keyframes]\n        points_3d = [mp['coordinates'] for mp in local_map]\n\n        # Collect observations\n        observations = []\n        for i, kf in enumerate(keyframes):\n            for obs in kf.get('observations', []):\n                # Find corresponding map point index\n                for j, mp in enumerate(local_map):\n                    if mp['id'] == obs['map_point_id']:\n                        observations.append({\n                            'camera_idx': i,\n                            'point_idx': j,\n                            'point_2d': obs['pixel_coordinates']\n                        })\n                        break\n\n        # Run bundle adjustment\n        optimized_poses, optimized_points = self.run_bundle_adjustment(\n            camera_poses, points_3d, observations, camera_matrix\n        )\n\n        # Update keyframes with optimized poses\n        optimized_keyframes = []\n        for i, (orig_kf, opt_pose) in enumerate(zip(keyframes, optimized_poses)):\n            new_kf = orig_kf.copy()\n            new_kf['pose'] = opt_pose\n            optimized_keyframes.append(new_kf)\n\n        # Update map points with optimized coordinates\n        optimized_local_map = []\n        for i, (orig_mp, opt_coords) in enumerate(zip(local_map, optimized_points)):\n            new_mp = orig_mp.copy()\n            new_mp['coordinates'] = opt_coords\n            optimized_local_map.append(new_mp)\n\n        return optimized_keyframes, optimized_local_map\n"})}),"\n",(0,r.jsx)(n.h3,{id:"lab-exercise-2-pose-estimation-evaluation",children:"Lab Exercise 2: Pose Estimation Evaluation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Test pose estimation on different types of motion (rotation, translation, combined)"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the impact of RANSAC parameters on pose estimation accuracy"}),"\n",(0,r.jsx)(n.li,{children:"Implement and compare different essential matrix estimation methods"}),"\n",(0,r.jsx)(n.li,{children:"Analyze the relationship between feature quality and pose accuracy"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"expected-results-1",children:"Expected Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Working pose estimation system"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of essential matrix decomposition"}),"\n",(0,r.jsx)(n.li,{children:"Ability to evaluate pose estimation quality"}),"\n",(0,r.jsx)(n.li,{children:"Implementation of bundle adjustment for optimization"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"lab-3-map-building-and-loop-closure",children:"Lab 3: Map Building and Loop Closure"}),"\n",(0,r.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,r.jsx)(n.p,{children:"Implement map building with 3D point cloud construction and loop closure detection for map consistency."}),"\n",(0,r.jsx)(n.h3,{id:"implementation-steps-2",children:"Implementation Steps"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-map-building-and-point-cloud-construction",children:"Step 1: Map Building and Point Cloud Construction"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# vslam_lab_3/map_builder.py\nimport numpy as np\nimport cv2\nfrom typing import List, Dict, Tuple, Optional\nfrom scipy.spatial import KDTree\nfrom sklearn.cluster import DBSCAN\nimport pickle\n\nclass MapPoint:\n    def __init__(self, coordinates: np.ndarray, color: np.ndarray = None,\n                 normal: np.ndarray = None, id: int = None):\n        self.coordinates = coordinates  # 3D coordinates [x, y, z]\n        self.color = color              # RGB color [r, g, b] (optional)\n        self.normal = normal           # Surface normal (optional)\n        self.id = id                   # Unique identifier\n        self.observations = []         # List of observations [camera_pose, pixel_coordinates]\n        self.descriptor = None         # Descriptor for loop closure\n        self.tracking_count = 0        # Number of successful trackings\n        self.first_observation_frame = 0  # Frame when first observed\n        self.last_observation_frame = 0   # Frame when last observed\n\nclass MapBuilder:\n    def __init__(self, max_map_size: int = 10000,\n                 min_triangulation_angle: float = 5.0,\n                 max_reprojection_error: float = 2.0):\n        """\n        Initialize map builder\n\n        Args:\n            max_map_size: Maximum number of map points to maintain\n            min_triangulation_angle: Minimum angle for stable triangulation (degrees)\n            max_reprojection_error: Maximum reprojection error for valid points (pixels)\n        """\n        self.max_map_size = max_map_size\n        self.min_triangulation_angle = np.radians(min_triangulation_angle)\n        self.max_reprojection_error = max_reprojection_error\n\n        self.map_points: List[MapPoint] = []\n        self.next_point_id = 0\n        self.kdtree = None\n        self.dirty = True  # Flag indicating if KD-tree needs rebuilding\n\n    def triangulate_point(self,\n                         pose1: np.ndarray,\n                         pose2: np.ndarray,\n                         point1: np.ndarray,\n                         point2: np.ndarray,\n                         camera_matrix: np.ndarray) -> Optional[np.ndarray]:\n        """\n        Triangulate a 3D point from two camera views\n\n        Args:\n            pose1, pose2: 4x4 camera poses\n            point1, point2: 2D points in respective images\n            camera_matrix: 3x3 camera intrinsic matrix\n\n        Returns:\n            3D coordinates of triangulated point or None if invalid\n        """\n        # Extract rotation and translation from poses\n        R1, t1 = pose1[:3, :3], pose1[:3, 3]\n        R2, t2 = pose2[:3, :3], pose2[:3, 3]\n\n        # Create projection matrices\n        P1 = camera_matrix @ np.hstack([R1, t1.reshape(3, 1)])\n        P2 = camera_matrix @ np.hstack([R2, t2.reshape(3, 1)])\n\n        # Triangulate point\n        point_4d = cv2.triangulatePoints(P1, P2, point1.reshape(2, 1), point2.reshape(2, 1))\n\n        if point_4d[3, 0] == 0:\n            return None\n\n        # Convert to 3D coordinates\n        point_3d = (point_4d[:3, 0] / point_4d[3, 0]).flatten()\n\n        # Check triangulation angle for stability\n        baseline = t2 - t1\n        ray1 = point_3d - t1\n        ray2 = point_3d - t2\n\n        if np.linalg.norm(ray1) == 0 or np.linalg.norm(ray2) == 0:\n            return None\n\n        # Calculate angle between viewing rays\n        cos_angle = np.dot(ray1, ray2) / (np.linalg.norm(ray1) * np.linalg.norm(ray2))\n        cos_angle = np.clip(cos_angle, -1, 1)\n        angle = np.arccos(abs(cos_angle))\n\n        if angle < self.min_triangulation_angle:\n            return None  # Too small angle for stable triangulation\n\n        # Check if point is in front of both cameras\n        if not self.is_point_in_front_of_camera(R1, t1, point_3d) or \\\n           not self.is_point_in_front_of_camera(R2, t2, point_3d):\n            return None\n\n        return point_3d\n\n    def is_point_in_front_of_camera(self, R: np.ndarray, t: np.ndarray, point_3d: np.ndarray) -> bool:\n        """Check if 3D point is in front of camera"""\n        # Transform point to camera coordinate system\n        cam_coords = R @ point_3d + t\n        return cam_coords[2] > 0  # Point is in front if z > 0\n\n    def add_observation(self, camera_pose: np.ndarray,\n                       pixel_coordinates: np.ndarray,\n                       point_3d: np.ndarray,\n                       descriptor: np.ndarray = None,\n                       color: np.ndarray = None) -> int:\n        """\n        Add a new map point or associate with existing point\n\n        Args:\n            camera_pose: 4x4 camera pose\n            pixel_coordinates: 2D pixel coordinates\n            point_3d: 3D coordinates of the point\n            descriptor: Feature descriptor (optional)\n            color: RGB color of the point (optional)\n\n        Returns:\n            ID of the map point\n        """\n        # Check if this point is already in the map (by proximity)\n        existing_point_id = self.find_closest_point(point_3d)\n\n        if existing_point_id is not None:\n            # Associate with existing point\n            point = self.map_points[existing_point_id]\n            point.observations.append({\n                \'camera_pose\': camera_pose.copy(),\n                \'pixel_coordinates\': pixel_coordinates.copy()\n            })\n            point.tracking_count += 1\n            point.last_observation_frame += 1\n            return existing_point_id\n        else:\n            # Create new map point\n            new_point = MapPoint(\n                coordinates=point_3d,\n                color=color,\n                id=self.next_point_id,\n                descriptor=descriptor\n            )\n\n            new_point.observations.append({\n                \'camera_pose\': camera_pose.copy(),\n                \'pixel_coordinates\': pixel_coordinates.copy()\n            })\n            new_point.tracking_count = 1\n            new_point.first_observation_frame = 0\n            new_point.last_observation_frame = 0\n\n            self.map_points.append(new_point)\n            point_id = self.next_point_id\n            self.next_point_id += 1\n\n            self.dirty = True  # Mark KD-tree as needing rebuild\n\n            return point_id\n\n    def find_closest_point(self, point_3d: np.ndarray, threshold: float = 0.05) -> Optional[int]:\n        """\n        Find closest existing map point to given 3D coordinates\n\n        Args:\n            point_3d: 3D coordinates to match\n            threshold: Distance threshold for considering points as same\n\n        Returns:\n            ID of closest point if within threshold, else None\n        """\n        if not self.map_points:\n            return None\n\n        if self.dirty:\n            self.rebuild_kdtree()\n\n        # Query KD-tree for nearest neighbor\n        distances, indices = self.kdtree.query([point_3d], k=1)\n\n        if distances[0] < threshold:\n            return indices[0]\n        else:\n            return None\n\n    def rebuild_kdtree(self):\n        """Rebuild KD-tree for efficient nearest neighbor search"""\n        if self.map_points:\n            points_array = np.array([mp.coordinates for mp in self.map_points])\n            self.kdtree = KDTree(points_array)\n            self.dirty = False\n        else:\n            self.kdtree = None\n            self.dirty = False\n\n    def update_map_point_colors(self, camera_pose: np.ndarray,\n                               image: np.ndarray,\n                               camera_matrix: np.ndarray):\n        """\n        Update colors of map points based on latest image\n\n        Args:\n            camera_pose: Current camera pose\n            image: Current image\n            camera_matrix: Camera intrinsic matrix\n        """\n        R = camera_pose[:3, :3]\n        t = camera_pose[:3, 3]\n\n        for point in self.map_points:\n            # Project 3D point to 2D\n            projected, _ = cv2.projectPoints(\n                point.coordinates.reshape(1, 1, 3),\n                cv2.Rodrigues(R)[0],\n                t,\n                camera_matrix,\n                None\n            )\n\n            x, y = int(projected[0, 0, 0]), int(projected[0, 0, 1])\n\n            # Check if projection is within image bounds\n            if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:\n                # Get color from image\n                if len(image.shape) == 3:  # Color image\n                    color = image[y, x]\n                    if point.color is None:\n                        point.color = color.astype(float)\n                    else:\n                        # Average with existing color\n                        point.color = 0.7 * point.color + 0.3 * color.astype(float)\n\n    def prune_bad_points(self):\n        """Remove bad map points based on various criteria"""\n        good_points = []\n\n        for point in self.map_points:\n            # Criteria for keeping points:\n            # 1. Sufficient number of observations\n            # 2. Good reprojection error\n            # 3. Not too old without observation\n\n            if (len(point.observations) >= 2 and\n                point.tracking_count >= 3 and\n                point.last_observation_frame >= point.first_observation_frame):\n\n                # Calculate average reprojection error\n                avg_error = self.calculate_reprojection_error(point)\n                if avg_error <= self.max_reprojection_error:\n                    good_points.append(point)\n\n        self.map_points = good_points\n        self.dirty = True\n\n    def calculate_reprojection_error(self, point: MapPoint) -> float:\n        """Calculate average reprojection error for a map point"""\n        if len(point.observations) < 2:\n            return float(\'inf\')\n\n        errors = []\n        for obs in point.observations:\n            pose = obs[\'camera_pose\']\n            expected_pixel = obs[\'pixel_coordinates\']\n\n            # Project 3D point to 2D\n            R = pose[:3, :3]\n            t = pose[:3, 3]\n\n            projected, _ = cv2.projectPoints(\n                point.coordinates.reshape(1, 1, 3),\n                cv2.Rodrigues(R)[0],\n                t,\n                self.camera_matrix,\n                None\n            )\n\n            actual_pixel = projected[0, 0]\n            error = np.linalg.norm(actual_pixel - expected_pixel)\n            errors.append(error)\n\n        return np.mean(errors) if errors else float(\'inf\')\n\n    def get_triangulated_points(self) -> np.ndarray:\n        """Get all triangulated map points as numpy array"""\n        if not self.map_points:\n            return np.array([]).reshape(0, 3)\n\n        return np.array([point.coordinates for point in self.map_points])\n\n    def save_map(self, filepath: str):\n        """Save map to file"""\n        map_data = {\n            \'points\': [\n                {\n                    \'coordinates\': point.coordinates,\n                    \'color\': point.color,\n                    \'id\': point.id,\n                    \'tracking_count\': point.tracking_count,\n                    \'observations\': point.observations\n                }\n                for point in self.map_points\n            ],\n            \'next_point_id\': self.next_point_id\n        }\n\n        with open(filepath, \'wb\') as f:\n            pickle.dump(map_data, f)\n\n    def load_map(self, filepath: str):\n        """Load map from file"""\n        with open(filepath, \'rb\') as f:\n            map_data = pickle.load(f)\n\n        self.map_points = []\n        for point_data in map_data[\'points\']:\n            point = MapPoint(\n                coordinates=point_data[\'coordinates\'],\n                color=point_data[\'color\'],\n                id=point_data[\'id\']\n            )\n            point.tracking_count = point_data[\'tracking_count\']\n            point.observations = point_data[\'observations\']\n            self.map_points.append(point)\n\n        self.next_point_id = map_data[\'next_point_id\']\n        self.dirty = True\n'})}),"\n",(0,r.jsx)(n.h4,{id:"step-2-loop-closure-detection",children:"Step 2: Loop Closure Detection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# vslam_lab_3/loop_closure.py\nimport numpy as np\nimport cv2\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.spatial.distance import cdist\nfrom typing import List, Dict, Tuple, Optional\nimport time\n\nclass LoopClosureDetector:\n    def __init__(self,\n                 db_size: int = 1000,\n                 min_loop_matches: int = 20,\n                 max_pose_distance: float = 5.0,\n                 max_angle_difference: float = 30.0):\n        """\n        Initialize loop closure detector\n\n        Args:\n            db_size: Maximum size of place recognition database\n            min_loop_matches: Minimum matches required for loop closure\n            max_pose_distance: Maximum distance for potential loop closure (meters)\n            max_angle_difference: Maximum angle difference for potential loop closure (degrees)\n        """\n        self.db_size = db_size\n        self.min_loop_matches = min_loop_matches\n        self.max_pose_distance = max_pose_distance\n        self.max_angle_difference = np.radians(max_angle_difference)\n\n        # Place recognition database\n        self.place_descriptors = []  # List of keyframe descriptors\n        self.place_poses = []        # Corresponding poses\n        self.place_timestamps = []   # Timestamps\n        self.place_ids = []          # Unique IDs\n\n        # Bag-of-Words vocabulary (simplified approach)\n        self.vocabulary = None\n        self.vocabulary_initialized = False\n\n        # Matching parameters\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n        self.loop_candidates = []\n        self.loop_threshold = 0.7   # Descriptor similarity threshold\n\n    def add_keyframe(self, descriptor: np.ndarray, pose: np.ndarray,\n                     timestamp: float, keyframe_id: int):\n        """\n        Add a keyframe to the place recognition database\n\n        Args:\n            descriptor: Feature descriptor for the keyframe\n            pose: Camera pose at this keyframe\n            timestamp: Timestamp of the keyframe\n            keyframe_id: Unique ID for the keyframe\n        """\n        if len(self.place_descriptors) >= self.db_size:\n            # Remove oldest entry\n            self.place_descriptors.pop(0)\n            self.place_poses.pop(0)\n            self.place_timestamps.pop(0)\n            self.place_ids.pop(0)\n\n        self.place_descriptors.append(descriptor)\n        self.place_poses.append(pose)\n        self.place_timestamps.append(timestamp)\n        self.place_ids.append(keyframe_id)\n\n    def detect_loop_closure(self, current_descriptor: np.ndarray,\n                           current_pose: np.ndarray) -> Optional[Tuple[int, np.ndarray, float]]:\n        """\n        Detect potential loop closure with previous keyframes\n\n        Args:\n            current_descriptor: Descriptor of current frame\n            current_pose: Current camera pose\n\n        Returns:\n            Tuple of (keyframe_id, relative_transform, confidence) if loop detected, else None\n        """\n        if len(self.place_descriptors) < 2:\n            return None\n\n        # Find potential matches in database\n        candidates = self.find_place_candidates(current_descriptor, current_pose)\n\n        if not candidates:\n            return None\n\n        # Verify each candidate using geometric consistency\n        for candidate_id, candidate_pose in candidates:\n            # Get corresponding descriptor\n            candidate_desc = self.place_descriptors[self.place_ids.index(candidate_id)]\n\n            # Match current and candidate descriptors\n            matches = self.match_descriptors(current_descriptor, candidate_desc)\n\n            if len(matches) >= self.min_loop_matches:\n                # Estimate relative transformation\n                transform, inliers = self.estimate_geometric_consistency(\n                    current_descriptor, candidate_desc, matches, current_pose, candidate_pose\n                )\n\n                if transform is not None and len(inliers) >= self.min_loop_matches:\n                    confidence = len(inliers) / len(matches)  # Geometric consistency ratio\n                    return candidate_id, transform, confidence\n\n        return None\n\n    def find_place_candidates(self, current_descriptor: np.ndarray,\n                             current_pose: np.ndarray) -> List[Tuple[int, np.ndarray]]:\n        """\n        Find potential place recognition candidates\n\n        Args:\n            current_descriptor: Descriptor of current frame\n            current_pose: Current camera pose\n\n        Returns:\n            List of (keyframe_id, pose) tuples for potential matches\n        """\n        candidates = []\n\n        for i, (desc, pose, kf_id) in enumerate(zip(self.place_descriptors, self.place_poses, self.place_ids)):\n            # Quick geometric check\n            pos_diff = np.linalg.norm(current_pose[:3, 3] - pose[:3, 3])\n\n            if pos_diff > self.max_pose_distance:\n                continue  # Too far apart geometrically\n\n            # Check orientation difference\n            R_current = current_pose[:3, :3]\n            R_prev = pose[:3, :3]\n\n            # Calculate rotation difference\n            R_diff = R_current @ R_prev.T\n            trace = np.trace(R_diff)\n            angle_diff = np.arccos(np.clip((trace - 1) / 2, -1, 1))\n\n            if angle_diff > self.max_angle_difference:\n                continue  # Too different in orientation\n\n            # Descriptor similarity check\n            matches = self.match_descriptors(current_descriptor, desc)\n            if len(matches) > 0:\n                similarity = len(matches) / max(current_descriptor.shape[0], desc.shape[0])\n                if similarity > self.loop_threshold:\n                    candidates.append((kf_id, pose))\n\n        return candidates\n\n    def match_descriptors(self, desc1: np.ndarray, desc2: np.ndarray) -> List[cv2.DMatch]:\n        """Match two sets of descriptors"""\n        if desc1 is None or desc2 is None or desc1.size == 0 or desc2.size == 0:\n            return []\n\n        try:\n            matches = self.matcher.knnMatch(desc1, desc2, k=2)\n\n            # Apply Lowe\'s ratio test\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance < 0.75 * n.distance:\n                        good_matches.append(m)\n\n            return good_matches\n        except cv2.error:\n            return []\n\n    def estimate_geometric_consistency(self, desc1: np.ndarray, desc2: np.ndarray,\n                                     matches: List[cv2.DMatch],\n                                     pose1: np.ndarray, pose2: np.ndarray) -> Tuple[Optional[np.ndarray], List[int]]:\n        """\n        Estimate geometric consistency of matches using essential matrix\n\n        Args:\n            desc1, desc2: Descriptors\n            matches: Good matches between descriptors\n            pose1, pose2: Corresponding poses\n\n        Returns:\n            Tuple of (relative_transform, inlier_indices) or (None, [])\n        """\n        if len(matches) < 8:\n            return None, []\n\n        # Extract matched points\n        pts1 = np.float32([desc1[m.queryIdx] for m in matches]).reshape(-1, 1, 2)\n        pts2 = np.float32([desc2[m.trainIdx] for m in matches]).reshape(-1, 1, 2)\n\n        # Estimate essential matrix with RANSAC\n        E, mask = cv2.findEssentialMat(\n            pts1, pts2,\n            cameraMatrix=np.eye(3),  # Use identity, relative pose already accounts for intrinsics\n            method=cv2.RANSAC,\n            prob=0.999,\n            threshold=1.0\n        )\n\n        if E is None:\n            return None, []\n\n        # Recover relative pose\n        _, R_rel, t_rel, mask_new = cv2.recoverPose(E, pts1, pts2, mask=mask)\n\n        # Create relative transformation matrix\n        T_rel = np.eye(4)\n        T_rel[:3, :3] = R_rel\n        T_rel[:3, 3] = t_rel.ravel()\n\n        # Get inlier indices\n        inliers = []\n        if mask_new is not None:\n            inliers = [i for i, is_inlier in enumerate(mask_new.flatten()) if is_inlier]\n\n        return T_rel, inliers\n\n    def optimize_with_loop_closure(self, trajectory: List[np.ndarray],\n                                  loop_constraints: List[Tuple[int, int, np.ndarray]]) -> List[np.ndarray]:\n        """\n        Optimize trajectory using loop closure constraints\n\n        Args:\n            trajectory: List of poses forming the trajectory\n            loop_constraints: List of (frame_i, frame_j, relative_transform) constraints\n\n        Returns:\n            Optimized trajectory\n        """\n        if not loop_constraints:\n            return trajectory\n\n        # Simple pose graph optimization (could be replaced with more sophisticated methods)\n        optimized_trajectory = trajectory.copy()\n\n        # Apply loop closure corrections iteratively\n        for _ in range(5):  # Multiple iterations for better optimization\n            for frame_i, frame_j, relative_transform in loop_constraints:\n                if frame_i < len(optimized_trajectory) and frame_j < len(optimized_trajectory):\n                    # Calculate current relative transform\n                    T_i_inv = np.linalg.inv(optimized_trajectory[frame_i])\n                    current_rel = T_i_inv @ optimized_trajectory[frame_j]\n\n                    # Calculate error\n                    error_T = relative_transform @ np.linalg.inv(current_rel)\n\n                    # Apply correction (simple averaging approach)\n                    correction_factor = 0.1  # Small correction factor\n                    corrected_T = optimized_trajectory[frame_j] @ (\n                        np.eye(4) + correction_factor * (error_T - np.eye(4))\n                    )\n\n                    optimized_trajectory[frame_j] = corrected_T\n\n        return optimized_trajectory\n\n    def initialize_vocabulary(self, all_descriptors: List[np.ndarray], vocab_size: int = 1000):\n        """\n        Initialize vocabulary for bag-of-words place recognition\n\n        Args:\n            all_descriptors: List of all descriptors from keyframes\n            vocab_size: Size of vocabulary\n        """\n        if not all_descriptors:\n            return\n\n        # Concatenate all descriptors\n        all_desc = np.vstack(all_descriptors)\n\n        # Use K-means clustering to create vocabulary\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.1)\n        _, labels, centers = cv2.kmeans(\n            all_desc.astype(np.float32),\n            vocab_size,\n            None,\n            criteria,\n            3,\n            cv2.KMEANS_PP_CENTERS\n        )\n\n        self.vocabulary = centers\n        self.vocabulary_initialized = True\n\n    def get_visual_words(self, descriptor: np.ndarray) -> List[int]:\n        """Convert descriptor to visual words using vocabulary"""\n        if not self.vocabulary_initialized:\n            return []\n\n        # Find nearest visual words for each feature\n        distances = cdist(descriptor, self.vocabulary, metric=\'euclidean\')\n        visual_words = np.argmin(distances, axis=1)\n\n        return visual_words.tolist()\n\n    def compute_similarity_score(self, desc1: np.ndarray, desc2: np.ndarray) -> float:\n        """Compute similarity score between two descriptors"""\n        matches = self.match_descriptors(desc1, desc2)\n        if len(matches) == 0:\n            return 0.0\n\n        # Compute ratio of good matches\n        total_features = max(desc1.shape[0], desc2.shape[0])\n        return len(matches) / total_features\n'})}),"\n",(0,r.jsx)(n.h3,{id:"lab-exercise-3-map-building-and-loop-closure-testing",children:"Lab Exercise 3: Map Building and Loop Closure Testing"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create a map from a sequence of images with known motion"}),"\n",(0,r.jsx)(n.li,{children:"Test loop closure detection on a trajectory that returns to start"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the impact of different parameters on map quality"}),"\n",(0,r.jsx)(n.li,{children:"Implement map optimization using loop closure constraints"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"expected-results-2",children:"Expected Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Working map building system with 3D point cloud"}),"\n",(0,r.jsx)(n.li,{children:"Functional loop closure detection"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of map optimization techniques"}),"\n",(0,r.jsx)(n.li,{children:"Ability to evaluate mapping system performance"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"lab-4-isaac-sim-integration",children:"Lab 4: Isaac Sim Integration"}),"\n",(0,r.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,r.jsx)(n.p,{children:"Integrate the VSLAM system with Isaac Sim to create a complete simulation pipeline for humanoid robot navigation."}),"\n",(0,r.jsx)(n.h3,{id:"implementation-steps-3",children:"Implementation Steps"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-isaac-sim-environment-setup",children:"Step 1: Isaac Sim Environment Setup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# vslam_lab_4/isaac_integration.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.viewports import set_camera_view\nimport numpy as np\nimport cv2\nfrom vslam_lab_3.map_builder import MapBuilder, MapPoint\nfrom vslam_lab_3.loop_closure import LoopClosureDetector\n\nclass IsaacVSLAMIntegration:\n    def __init__(self):\n        # Initialize Isaac Sim world\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Initialize VSLAM components\n        self.camera_matrix = np.array([\n            [600.0, 0.0, 320.0],  # fx, 0, cx\n            [0.0, 600.0, 240.0],  # 0, fy, cy\n            [0.0, 0.0, 1.0]       # 0, 0, 1\n        ])\n\n        self.map_builder = MapBuilder()\n        self.loop_detector = LoopClosureDetector()\n        self.vslam_pipeline = VSLAMPipeline(self.camera_matrix)\n\n        # Robot state\n        self.robot = None\n        self.camera = None\n        self.trajectory = []\n        self.keyframes = []\n\n        # Simulation parameters\n        self.keyframe_interval = 10  # Frames between keyframes\n        self.frame_count = 0\n        self.current_pose = np.eye(4)\n\n    def setup_environment(self):\n        """Set up Isaac Sim environment with robot and camera"""\n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n\n        # Add robot (using a simple wheeled robot for this example)\n        self.robot = self.world.scene.add(\n            Robot(\n                prim_path="/World/Robot",\n                name="slam_robot",\n                usd_path="/Isaac/Robots/TurtleBot3Burger/turtlebot3_burger.usd",\n                position=[0, 0, 0.1],\n                orientation=[0, 0, 0, 1]\n            )\n        )\n\n        # Add camera to robot\n        self.camera = Camera(\n            prim_path="/World/Robot/chassis/camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        self.world.scene.add(self.camera)\n\n        # Set up lighting\n        from omni.isaac.core.utils.prims import create_prim\n        create_prim(\n            prim_path="/World/Light",\n            prim_type="DistantLight",\n            position=np.array([0, 0, 10]),\n            attributes={"color": np.array([0.8, 0.8, 0.8])}\n        )\n\n    def run_simulation(self, num_steps: int = 1000):\n        """Run VSLAM simulation in Isaac Sim"""\n        self.world.reset()\n\n        for step in range(num_steps):\n            self.world.step(render=True)\n\n            # Get camera image\n            image = self.get_camera_image()\n\n            if image is not None:\n                # Get current robot pose (ground truth)\n                robot_pose = self.get_robot_pose()\n\n                # Process with VSLAM\n                self.process_vslam_step(image, robot_pose, step)\n\n                # Update visualization\n                if step % 50 == 0:\n                    self.update_visualization()\n\n            self.frame_count += 1\n\n    def get_camera_image(self):\n        """Get current camera image from Isaac Sim"""\n        try:\n            # Get image data from Isaac Sim camera\n            image_data = self.camera.get_rgb()\n            return image_data\n        except Exception as e:\n            print(f"Error getting camera image: {e}")\n            return None\n\n    def get_robot_pose(self):\n        """Get current robot pose from Isaac Sim"""\n        try:\n            position, orientation = self.robot.get_world_pose()\n\n            # Convert to 4x4 transformation matrix\n            R = self.quaternion_to_rotation_matrix(orientation)\n            T = np.eye(4)\n            T[:3, :3] = R\n            T[:3, 3] = position\n\n            return T\n        except Exception as e:\n            print(f"Error getting robot pose: {e}")\n            return np.eye(4)\n\n    def quaternion_to_rotation_matrix(self, q):\n        """Convert quaternion to rotation matrix"""\n        w, x, y, z = q\n\n        R = np.array([\n            [1 - 2*y*y - 2*z*z, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n            [2*x*y + 2*z*w, 1 - 2*x*x - 2*z*z, 2*y*z - 2*x*w],\n            [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x*x - 2*y*y]\n        ])\n\n        return R\n\n    def process_vslam_step(self, image, ground_truth_pose, frame_num):\n        """Process one VSLAM step"""\n        # Process with VSLAM pipeline\n        results = self.vslam_pipeline.process_frame_pair(\n            self.get_previous_image() if hasattr(self, \'_prev_image\') else image,\n            image\n        )\n\n        if results[\'success\']:\n            # Update current pose estimate\n            self.current_pose = results[\'absolute_pose\']\n\n            # Store trajectory\n            self.trajectory.append({\n                \'frame\': frame_num,\n                \'estimated_pose\': self.current_pose.copy(),\n                \'ground_truth_pose\': ground_truth_pose\n            })\n\n            # Add keyframe if needed\n            if frame_num % self.keyframe_interval == 0:\n                self.add_keyframe(image, self.current_pose, frame_num)\n\n            # Check for loop closure\n            if len(self.keyframes) > 10:  # Need enough keyframes for meaningful comparison\n                loop_result = self.check_for_loop_closure(image, self.current_pose, frame_num)\n\n                if loop_result:\n                    print(f"Loop closure detected at frame {frame_num}")\n                    keyframe_id, transform, confidence = loop_result\n                    self.handle_loop_closure(keyframe_id, transform)\n\n        # Store current image for next iteration\n        self._prev_image = image\n\n    def add_keyframe(self, image, pose, frame_num):\n        """Add current frame as keyframe"""\n        # Extract features for keyframe\n        keypoints, descriptors = self.vslam_pipeline.detector.detect_and_compute(image)\n\n        keyframe = {\n            \'frame_num\': frame_num,\n            \'pose\': pose,\n            \'image\': image,\n            \'keypoints\': keypoints,\n            \'descriptors\': descriptors,\n            \'timestamp\': self.world.current_time\n        }\n\n        self.keyframes.append(keyframe)\n\n        # Add to loop closure detector\n        if descriptors is not None and descriptors.size > 0:\n            self.loop_detector.add_keyframe(descriptors, pose, self.world.current_time, frame_num)\n\n    def check_for_loop_closure(self, image, current_pose, frame_num):\n        """Check for potential loop closure"""\n        # Extract features from current image\n        keypoints, descriptors = self.vslam_pipeline.detector.detect_and_compute(image)\n\n        if descriptors is not None and descriptors.size > 0:\n            # Check for loop closure\n            loop_result = self.loop_detector.detect_loop_closure(descriptors, current_pose)\n            return loop_result\n\n        return None\n\n    def handle_loop_closure(self, keyframe_id, transform):\n        """Handle detected loop closure"""\n        # Update map and trajectory based on loop closure\n        # This would involve optimizing the map and trajectory\n        print(f"Handling loop closure with keyframe {keyframe_id}")\n\n    def update_visualization(self):\n        """Update visualization of map and trajectory"""\n        # This would update Isaac Sim visualization\n        # For now, just print status\n        print(f"Processed {self.frame_count} frames, {len(self.map_builder.map_points)} map points, "\n              f"{len(self.trajectory)} trajectory points")\n\n    def evaluate_performance(self):\n        """Evaluate VSLAM performance against ground truth"""\n        if not self.trajectory:\n            print("No trajectory data for evaluation")\n            return\n\n        # Calculate trajectory errors\n        position_errors = []\n        orientation_errors = []\n\n        for traj_point in self.trajectory:\n            est_pose = traj_point[\'estimated_pose\']\n            gt_pose = traj_point[\'ground_truth_pose\']\n\n            # Position error\n            pos_err = np.linalg.norm(est_pose[:3, 3] - gt_pose[:3, 3])\n            position_errors.append(pos_err)\n\n            # Orientation error\n            R_est = est_pose[:3, :3]\n            R_gt = gt_pose[:3, :3]\n\n            # Calculate rotation error using Frobenius norm\n            R_error = R_est @ R_gt.T - np.eye(3)\n            rot_err = np.linalg.norm(R_error, \'fro\')\n            orientation_errors.append(rot_err)\n\n        # Calculate statistics\n        avg_pos_error = np.mean(position_errors) if position_errors else 0\n        max_pos_error = np.max(position_errors) if position_errors else 0\n        avg_rot_error = np.mean(orientation_errors) if orientation_errors else 0\n\n        print(f"\\nVSLAM Performance Evaluation:")\n        print(f"- Average position error: {avg_pos_error:.3f}m")\n        print(f"- Maximum position error: {max_pos_error:.3f}m")\n        print(f"- Average orientation error: {avg_rot_error:.3f} (Frobenius norm)")\n        print(f"- Total trajectory points: {len(self.trajectory)}")\n        print(f"- Total map points: {len(self.map_builder.map_points)}")\n\ndef main():\n    """Main function to run Isaac Sim VSLAM integration"""\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\'Isaac Sim VSLAM Integration\')\n    parser.add_argument(\'--steps\', type=int, default=1000, help=\'Number of simulation steps\')\n    args = parser.parse_args()\n\n    # Initialize integration\n    vslam_integration = IsaacVSLAMIntegration()\n\n    # Set up environment\n    vslam_integration.setup_environment()\n\n    # Run simulation\n    vslam_integration.run_simulation(args.steps)\n\n    # Evaluate performance\n    vslam_integration.evaluate_performance()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(n.h4,{id:"step-2-ros-2-bridge-integration",children:"Step 2: ROS 2 Bridge Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# vslam_lab_4/ros2_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, PointStamped\nfrom nav_msgs.msg import Odometry\nfrom visualization_msgs.msg import MarkerArray, Marker\nfrom std_msgs.msg import Header, ColorRGBA\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport tf2_ros\nfrom geometry_msgs.msg import TransformStamped\nfrom vslam_lab_2.pose_estimator import VSLAMPipeline\nfrom vslam_lab_3.map_builder import MapBuilder\nfrom vslam_lab_3.loop_closure import LoopClosureDetector\n\nclass IsaacVSLAMROS2Bridge(Node):\n    def __init__(self):\n        super().__init__('isaac_vslam_ros2_bridge')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Initialize VSLAM components\n        self.camera_matrix = np.array([\n            [600.0, 0.0, 320.0],\n            [0.0, 600.0, 240.0],\n            [0.0, 0.0, 1.0]\n        ])\n\n        self.vslam_pipeline = VSLAMPipeline(self.camera_matrix)\n        self.map_builder = MapBuilder()\n        self.loop_detector = LoopClosureDetector()\n\n        # TF broadcaster\n        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)\n\n        # Publishers\n        self.pose_pub = self.create_publisher(PoseStamped, '/vslam/pose', 10)\n        self.odom_pub = self.create_publisher(Odometry, '/vslam/odometry', 10)\n        self.map_pub = self.create_publisher(MarkerArray, '/vslam/map', 10)\n        self.traj_pub = self.create_publisher(Marker, '/vslam/trajectory', 10)\n        self.vis_pub = self.create_publisher(Image, '/vslam/visualization', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/isaac_sim/camera/rgb/image', self.image_callback, 10)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/isaac_sim/camera/rgb/camera_info', self.camera_info_callback, 10)\n\n        # Internal state\n        self.camera_info = None\n        self.current_pose = np.eye(4)\n        self.trajectory = []\n        self.frame_count = 0\n        self.prev_image = None\n\n        # Processing parameters\n        self.keyframe_interval = 10\n        self.processing_enabled = True\n\n        self.get_logger().info('Isaac VSLAM ROS2 Bridge Initialized')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Process camera calibration information\"\"\"\n        if self.camera_info is None:\n            # Initialize camera matrix from camera info\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\n\n            # Reinitialize VSLAM with proper camera matrix\n            self.vslam_pipeline = VSLAMPipeline(self.camera_matrix)\n\n            self.get_logger().info('Camera calibration received and VSLAM initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images for VSLAM\"\"\"\n        if not self.processing_enabled or self.camera_info is None:\n            return\n\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Process with VSLAM\n            if self.prev_image is not None:\n                results = self.vslam_pipeline.process_frame_pair(self.prev_image, cv_image)\n\n                if results['success']:\n                    # Update current pose\n                    self.current_pose = results['absolute_pose']\n\n                    # Store trajectory\n                    self.trajectory.append({\n                        'timestamp': msg.header.stamp,\n                        'pose': self.current_pose.copy()\n                    })\n\n                    # Add keyframe if needed\n                    if self.frame_count % self.keyframe_interval == 0:\n                        self.add_keyframe(cv_image, self.current_pose, msg.header.stamp)\n\n                    # Check for loop closure\n                    if len(self.vslam_pipeline.keyframes) > 10:\n                        self.check_loop_closure(cv_image, self.current_pose, msg.header)\n\n                    # Publish results\n                    self.publish_vslam_results(msg.header)\n\n                    # Broadcast TF\n                    self.broadcast_transform(msg.header)\n\n            # Store current image for next iteration\n            self.prev_image = cv_image.copy()\n            self.frame_count += 1\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing VSLAM frame: {e}')\n\n    def add_keyframe(self, image, pose, timestamp):\n        \"\"\"Add current frame as keyframe for mapping and loop closure\"\"\"\n        # Extract features\n        keypoints, descriptors = self.vslam_pipeline.detector.detect_and_compute(image)\n\n        # Add to VSLAM pipeline\n        keyframe_data = {\n            'image': image,\n            'pose': pose,\n            'keypoints': keypoints,\n            'descriptors': descriptors,\n            'timestamp': timestamp\n        }\n\n        self.vslam_pipeline.keyframes.append(keyframe_data)\n\n        # Add to loop closure detector\n        if descriptors is not None and descriptors.size > 0:\n            self.loop_detector.add_keyframe(\n                descriptors, pose,\n                timestamp.sec + timestamp.nanosec * 1e-9,\n                len(self.vslam_pipeline.keyframes) - 1\n            )\n\n    def check_loop_closure(self, image, current_pose, header):\n        \"\"\"Check for loop closure opportunities\"\"\"\n        # Extract features from current image\n        keypoints, descriptors = self.vslam_pipeline.detector.detect_and_compute(image)\n\n        if descriptors is not None and descriptors.size > 0:\n            # Check for loop closure\n            loop_result = self.loop_detector.detect_loop_closure(descriptors, current_pose)\n\n            if loop_result:\n                keyframe_id, transform, confidence = loop_result\n                self.get_logger().info(\n                    f'Loop closure detected! Keyframe: {keyframe_id}, '\n                    f'Confidence: {confidence:.3f}'\n                )\n\n                # Handle loop closure (would involve optimization)\n                self.handle_loop_closure(keyframe_id, transform, header)\n\n    def handle_loop_closure(self, keyframe_id, transform, header):\n        \"\"\"Handle detected loop closure\"\"\"\n        # In a full implementation, this would trigger map optimization\n        # For this example, just log the event\n        self.get_logger().info(f'Loop closure handled for keyframe {keyframe_id}')\n\n    def publish_vslam_results(self, header):\n        \"\"\"Publish VSLAM results to ROS topics\"\"\"\n        # Publish pose estimate\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.header.frame_id = 'vslam_map'\n\n        pose_msg.pose.position.x = float(self.current_pose[0, 3])\n        pose_msg.pose.position.y = float(self.current_pose[1, 3])\n        pose_msg.pose.position.z = float(self.current_pose[2, 3])\n\n        # Convert rotation matrix to quaternion\n        R = self.current_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n        pose_msg.pose.orientation.w = qw\n        pose_msg.pose.orientation.x = qx\n        pose_msg.pose.orientation.y = qy\n        pose_msg.pose.orientation.z = qz\n\n        self.pose_pub.publish(pose_msg)\n\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header = header\n        odom_msg.header.frame_id = 'vslam_map'\n        odom_msg.child_frame_id = 'vslam_camera'\n\n        odom_msg.pose.pose = pose_msg.pose\n\n        # Set velocity based on recent movement (simplified)\n        if len(self.trajectory) > 1:\n            prev_pose = self.trajectory[-2]['pose']\n            dt = 0.1  # Assume 10Hz\n            linear_vel = (self.current_pose[:3, 3] - prev_pose[:3, 3]) / dt\n            odom_msg.twist.twist.linear.x = linear_vel[0]\n            odom_msg.twist.twist.linear.y = linear_vel[1]\n            odom_msg.twist.twist.linear.z = linear_vel[2]\n\n        self.odom_pub.publish(odom_msg)\n\n        # Publish map visualization\n        self.publish_map_visualization(header)\n\n        # Publish trajectory visualization\n        self.publish_trajectory_visualization(header)\n\n    def rotation_matrix_to_quaternion(self, R):\n        \"\"\"Convert rotation matrix to quaternion\"\"\"\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        return qw, qx, qy, qz\n\n    def broadcast_transform(self, header):\n        \"\"\"Broadcast TF transform for VSLAM results\"\"\"\n        t = TransformStamped()\n\n        t.header.stamp = header.stamp\n        t.header.frame_id = 'vslam_map'\n        t.child_frame_id = 'vslam_camera'\n\n        t.transform.translation.x = float(self.current_pose[0, 3])\n        t.transform.translation.y = float(self.current_pose[1, 3])\n        t.transform.translation.z = float(self.current_pose[2, 3])\n\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\n        t.transform.rotation.w = qw\n        t.transform.rotation.x = qx\n        t.transform.rotation.y = qy\n        t.transform.rotation.z = qz\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def publish_map_visualization(self, header):\n        \"\"\"Publish map points as visualization markers\"\"\"\n        marker_array = MarkerArray()\n\n        # Create markers for map points\n        for i, map_point in enumerate(self.map_builder.map_points[:1000]):  # Limit for performance\n            marker = Marker()\n            marker.header = header\n            marker.header.frame_id = 'vslam_map'\n            marker.ns = 'vslam_map'\n            marker.id = i\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n\n            marker.pose.position.x = float(map_point.coordinates[0])\n            marker.pose.position.y = float(map_point.coordinates[1])\n            marker.pose.position.z = float(map_point.coordinates[2])\n\n            marker.pose.orientation.w = 1.0\n            marker.pose.orientation.x = 0.0\n            marker.pose.orientation.y = 0.0\n            marker.pose.orientation.z = 0.0\n\n            marker.scale.x = 0.05\n            marker.scale.y = 0.05\n            marker.scale.z = 0.05\n\n            # Color based on tracking count\n            if map_point.tracking_count > 10:\n                marker.color.r = 0.0\n                marker.color.g = 1.0\n                marker.color.b = 0.0\n            else:\n                marker.color.r = 1.0\n                marker.color.g = 0.0\n                marker.color.b = 0.0\n\n            marker.color.a = 1.0\n\n            marker_array.markers.append(marker)\n\n        self.map_pub.publish(marker_array)\n\n    def publish_trajectory_visualization(self, header):\n        \"\"\"Publish trajectory as visualization marker\"\"\"\n        marker = Marker()\n        marker.header = header\n        marker.header.frame_id = 'vslam_map'\n        marker.ns = 'vslam_trajectory'\n        marker.id = 0\n        marker.type = Marker.LINE_STRIP\n        marker.action = Marker.ADD\n\n        # Add trajectory points\n        for traj_point in self.trajectory[-100:]:  # Last 100 points\n            pose = traj_point['pose']\n            point = PointStamped()\n            point.point.x = float(pose[0, 3])\n            point.point.y = float(pose[1, 3])\n            point.point.z = float(pose[2, 3])\n            marker.points.append(point.point)\n\n        marker.scale.x = 0.02  # Line width\n        marker.color.r = 0.0\n        marker.color.g = 0.0\n        marker.color.b = 1.0\n        marker.color.a = 1.0\n\n        self.traj_pub.publish(marker)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vslam_bridge = IsaacVSLAMROS2Bridge()\n\n    try:\n        rclpy.spin(vslam_bridge)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vslam_bridge.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"lab-exercise-4-complete-vslam-integration",children:"Lab Exercise 4: Complete VSLAM Integration"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate VSLAM system with Isaac Sim environment"}),"\n",(0,r.jsx)(n.li,{children:"Connect Isaac Sim to ROS 2 using the bridge"}),"\n",(0,r.jsx)(n.li,{children:"Test VSLAM performance in simulated environment"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate results against ground truth from Isaac Sim"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"expected-results-3",children:"Expected Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complete integration pipeline from Isaac Sim to ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"Working VSLAM system in simulation environment"}),"\n",(0,r.jsx)(n.li,{children:"Real-time performance with proper visualization"}),"\n",(0,r.jsx)(n.li,{children:"Quantified performance metrics against ground truth"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization-and-evaluation",children:"Performance Optimization and Evaluation"}),"\n",(0,r.jsx)(n.h3,{id:"real-time-performance-considerations",children:"Real-time Performance Considerations"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Performance optimization for VSLAM\nimport time\nimport threading\nfrom collections import deque\nimport psutil\nimport GPUtil\n\nclass VSLAMPerformanceOptimizer:\n    def __init__(self):\n        self.frame_times = deque(maxlen=30)  # Last 30 frame times\n        self.feature_times = deque(maxlen=30)\n        self.pose_times = deque(maxlen=30)\n        self.mapping_times = deque(maxlen=30)\n\n        self.target_fps = 30\n        self.current_fps = 0\n        self.feature_count_target = 1000\n\n        # Resource monitoring\n        self.cpu_usage = deque(maxlen=30)\n        self.gpu_usage = deque(maxlen=30)\n        self.memory_usage = deque(maxlen=30)\n\n        # Adaptive parameters\n        self.adaptive_params = {\n            'max_features': 1000,\n            'matching_threshold': 0.75,\n            'ransac_threshold': 1.0,\n            'bundle_adjustment_frequency': 10\n        }\n\n    def monitor_performance(self):\n        \"\"\"Monitor system performance in separate thread\"\"\"\n        while True:\n            # CPU usage\n            cpu_percent = psutil.cpu_percent(interval=1)\n            self.cpu_usage.append(cpu_percent)\n\n            # Memory usage\n            memory_percent = psutil.virtual_memory().percent\n            self.memory_usage.append(memory_percent)\n\n            # GPU usage (if available)\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                gpu_percent = gpus[0].load * 100\n                self.gpu_usage.append(gpu_percent)\n            else:\n                self.gpu_usage.append(0)\n\n            # Calculate current FPS\n            if len(self.frame_times) > 1:\n                avg_frame_time = sum(self.frame_times) / len(self.frame_times)\n                self.current_fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n\n    def adaptive_processing(self, image):\n        \"\"\"Adapt processing based on performance\"\"\"\n        start_time = time.time()\n\n        # Adjust feature detection based on performance\n        if self.current_fps < self.target_fps * 0.8:\n            # Reduce feature count to improve performance\n            self.adaptive_params['max_features'] = max(500,\n                int(self.adaptive_params['max_features'] * 0.9))\n        elif self.current_fps > self.target_fps * 1.1:\n            # Increase feature count for better accuracy\n            self.adaptive_params['max_features'] = min(2000,\n                int(self.adaptive_params['max_features'] * 1.1))\n\n        # Process features\n        features_start = time.time()\n        keypoints, descriptors = self.detect_features_adaptive(image)\n        self.feature_times.append(time.time() - features_start)\n\n        # Continue with rest of VSLAM pipeline\n        processing_time = time.time() - start_time\n        self.frame_times.append(processing_time)\n\n        return keypoints, descriptors\n\n    def detect_features_adaptive(self, image):\n        \"\"\"Detect features with adaptive parameters\"\"\"\n        detector = cv2.ORB_create(\n            nfeatures=self.adaptive_params['max_features'],\n            scaleFactor=1.2,\n            nlevels=4,  # Reduce levels for speed\n            edgeThreshold=19,\n            patchSize=19,\n            fastThreshold=20\n        )\n\n        keypoints, descriptors = detector.detectAndCompute(image, None)\n        return keypoints, descriptors\n\n    def get_performance_metrics(self):\n        \"\"\"Get current performance metrics\"\"\"\n        metrics = {\n            'fps': self.current_fps,\n            'avg_frame_time': sum(self.frame_times) / len(self.frame_times) if self.frame_times else 0,\n            'avg_feature_time': sum(self.feature_times) / len(self.feature_times) if self.feature_times else 0,\n            'avg_pose_time': sum(self.pose_times) / len(self.pose_times) if self.pose_times else 0,\n            'avg_mapping_time': sum(self.mapping_times) / len(self.mapping_times) if self.mapping_times else 0,\n            'cpu_usage_avg': sum(self.cpu_usage) / len(self.cpu_usage) if self.cpu_usage else 0,\n            'gpu_usage_avg': sum(self.gpu_usage) / len(self.gpu_usage) if self.gpu_usage else 0,\n            'memory_usage_avg': sum(self.memory_usage) / len(self.memory_usage) if self.memory_usage else 0\n        }\n\n        return metrics\n\n    def optimize_parameters(self):\n        \"\"\"Optimize parameters based on performance\"\"\"\n        metrics = self.get_performance_metrics()\n\n        # Adjust parameters based on performance\n        if metrics['fps'] < self.target_fps * 0.7:\n            # Significantly below target, aggressive optimization needed\n            self.adaptive_params['bundle_adjustment_frequency'] = max(20,\n                self.adaptive_params['bundle_adjustment_frequency'] + 5)\n        elif metrics['fps'] > self.target_fps * 1.05:\n            # Above target, can afford more processing\n            self.adaptive_params['bundle_adjustment_frequency'] = max(5,\n                self.adaptive_params['bundle_adjustment_frequency'] - 1)\n\n        return self.adaptive_params\n"})}),"\n",(0,r.jsx)(n.h2,{id:"evaluation-and-validation",children:"Evaluation and Validation"}),"\n",(0,r.jsx)(n.h3,{id:"vslam-evaluation-framework",children:"VSLAM Evaluation Framework"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# VSLAM evaluation and validation\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport matplotlib.pyplot as plt\n\nclass VSLAMEvaluator:\n    def __init__(self):\n        self.estimated_trajectory = []\n        self.ground_truth_trajectory = []\n        self.estimated_map = []\n        self.ground_truth_map = []\n        self.timestamps = []\n\n    def add_estimates(self, est_pose, gt_pose, timestamp):\n        \"\"\"Add pose estimates for evaluation\"\"\"\n        self.estimated_trajectory.append(est_pose[:3, 3])  # Position only\n        self.ground_truth_trajectory.append(gt_pose[:3, 3])\n        self.timestamps.append(timestamp)\n\n    def calculate_ate(self):\n        \"\"\"Calculate Absolute Trajectory Error\"\"\"\n        if len(self.estimated_trajectory) < 2:\n            return float('inf'), float('inf')\n\n        est_traj = np.array(self.estimated_trajectory)\n        gt_traj = np.array(self.ground_truth_trajectory)\n\n        # Align trajectories using Umeyama algorithm\n        est_aligned, R_align, t_align, s_align = self.align_trajectory(est_traj, gt_traj)\n\n        # Calculate ATE\n        errors = np.linalg.norm(est_aligned - gt_traj, axis=1)\n        rmse = np.sqrt(np.mean(errors**2))\n        mean_error = np.mean(errors)\n\n        return rmse, mean_error\n\n    def calculate_rpe(self):\n        \"\"\"Calculate Relative Pose Error\"\"\"\n        if len(self.estimated_trajectory) < 3:\n            return float('inf'), float('inf')\n\n        est_traj = np.array(self.estimated_trajectory)\n        gt_traj = np.array(self.ground_truth_trajectory)\n\n        # Calculate relative poses\n        est_rel_poses = []\n        gt_rel_poses = []\n\n        for i in range(1, len(est_traj)):\n            est_rel = est_traj[i] - est_traj[i-1]\n            gt_rel = gt_traj[i] - gt_traj[i-1]\n\n            est_rel_poses.append(est_rel)\n            gt_rel_poses.append(gt_rel)\n\n        est_rel = np.array(est_rel_poses)\n        gt_rel = np.array(gt_rel_poses)\n\n        # Calculate RPE\n        errors = np.linalg.norm(est_rel - gt_rel, axis=1)\n        rmse = np.sqrt(np.mean(errors**2))\n        mean_error = np.mean(errors)\n\n        return rmse, mean_error\n\n    def align_trajectory(self, est_traj, gt_traj):\n        \"\"\"Align estimated trajectory to ground truth using Umeyama algorithm\"\"\"\n        # Calculate centroids\n        est_centroid = np.mean(est_traj, axis=0)\n        gt_centroid = np.mean(gt_traj, axis=0)\n\n        # Center trajectories\n        est_centered = est_traj - est_centroid\n        gt_centered = gt_traj - gt_centroid\n\n        # Calculate correlation matrix\n        H = np.dot(gt_centered.T, est_centered)\n\n        # Singular value decomposition\n        U, S, Vt = np.linalg.svd(H)\n\n        # Calculate rotation matrix\n        R = np.dot(U, Vt)\n        if np.linalg.det(R) < 0:\n            Vt[-1, :] *= -1\n            R = np.dot(U, Vt)\n\n        # Calculate scale\n        var_a = np.mean(np.sum(est_centered**2, axis=1))\n        scale = np.trace(np.dot(S, np.diag([1, 1, np.sign(np.linalg.det(R))]))) / var_a\n\n        # Calculate translation\n        t = gt_centroid - scale * np.dot(R, est_centroid)\n\n        # Apply transformation\n        est_aligned = scale * np.dot(est_traj, R.T) + t\n\n        return est_aligned, R, t, scale\n\n    def calculate_orientation_error(self):\n        \"\"\"Calculate orientation error between estimated and ground truth poses\"\"\"\n        if len(self.estimated_trajectory) < 2:\n            return float('inf'), float('inf')\n\n        # This would require full pose matrices, not just positions\n        # Implementation would compare rotation matrices/quaternions\n        pass\n\n    def plot_results(self):\n        \"\"\"Plot evaluation results\"\"\"\n        if not self.estimated_trajectory or not self.ground_truth_trajectory:\n            print(\"No trajectory data to plot\")\n            return\n\n        est_traj = np.array(self.estimated_trajectory)\n        gt_traj = np.array(self.ground_truth_trajectory)\n\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Plot trajectories\n        axes[0, 0].plot(gt_traj[:, 0], gt_traj[:, 1], 'g-', label='Ground Truth', linewidth=2)\n        axes[0, 0].plot(est_traj[:, 0], est_traj[:, 1], 'r-', label='Estimated', linewidth=2)\n        axes[0, 0].set_title('Trajectory Comparison')\n        axes[0, 0].set_xlabel('X (m)')\n        axes[0, 0].set_ylabel('Y (m)')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True)\n\n        # Plot position errors over time\n        if len(est_traj) == len(gt_traj):\n            position_errors = np.linalg.norm(est_traj - gt_traj, axis=1)\n            axes[0, 1].plot(position_errors)\n            axes[0, 1].set_title('Position Error Over Time')\n            axes[0, 1].set_xlabel('Frame')\n            axes[0, 1].set_ylabel('Error (m)')\n            axes[0, 1].grid(True)\n\n        # Plot X, Y, Z components separately\n        time_axis = range(len(est_traj))\n        axes[1, 0].plot(time_axis, gt_traj[:, 0], 'g-', label='GT X')\n        axes[1, 0].plot(time_axis, est_traj[:, 0], 'r-', label='Est X')\n        axes[1, 0].set_title('X Position Over Time')\n        axes[1, 0].set_xlabel('Frame')\n        axes[1, 0].set_ylabel('X (m)')\n        axes[1, 0].legend()\n        axes[1, 0].grid(True)\n\n        axes[1, 1].plot(time_axis, gt_traj[:, 1], 'g-', label='GT Y')\n        axes[1, 1].plot(time_axis, est_traj[:, 1], 'r-', label='Est Y')\n        axes[1, 1].set_title('Y Position Over Time')\n        axes[1, 1].set_xlabel('Frame')\n        axes[1, 1].set_ylabel('Y (m)')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n    def generate_evaluation_report(self):\n        \"\"\"Generate comprehensive evaluation report\"\"\"\n        ate_rmse, ate_mean = self.calculate_ate()\n        rpe_rmse, rpe_mean = self.calculate_rpe()\n\n        report = f\"\"\"\nVSLAM Evaluation Report\n=======================\nTrajectory Length: {len(self.estimated_trajectory)} poses\nTotal Distance: {np.sum(np.linalg.norm(np.diff(self.ground_truth_trajectory, axis=0), axis=1)):.2f}m\n\nAbsolute Trajectory Error (ATE):\n- RMSE: {ate_rmse:.4f}m\n- Mean: {ate_mean:.4f}m\n- Median: {np.median(np.linalg.norm(np.array(self.estimated_trajectory) - np.array(self.ground_truth_trajectory), axis=1)):.4f}m\n\nRelative Pose Error (RPE):\n- RMSE: {rpe_rmse:.4f}m\n- Mean: {rpe_mean:.4f}m\n\nPerformance:\n- Average FPS: {self.average_fps:.2f}\n- Feature Processing Time: {self.avg_feature_time:.3f}s\n- Pose Estimation Time: {self.avg_pose_time:.3f}s\n- Total Processing Time: {self.avg_total_time:.3f}s\n\nRobustness:\n- Tracking Success Rate: {self.tracking_success_rate:.2f}%\n- Map Point Survival Rate: {self.map_survival_rate:.2f}%\n- Loop Closure Success Rate: {self.loop_closure_rate:.2f}%\n\nOverall Rating: {self.calculate_overall_score():.2f}/10.0\n        \"\"\"\n\n        return report\n\n    def calculate_overall_score(self):\n        \"\"\"Calculate overall VSLAM performance score\"\"\"\n        # Weighted combination of different metrics\n        ate_score = max(0, 10 - (self.ate_rmse * 5))  # Higher error = lower score\n        rpe_score = max(0, 10 - (self.rpe_rmse * 10))\n        fps_score = min(10, (self.average_fps / 30) * 10)  # Target 30 FPS\n        robustness_score = self.tracking_success_rate\n\n        overall_score = (ate_score * 0.3 + rpe_score * 0.3 +\n                        fps_score * 0.2 + robustness_score * 0.2)\n\n        return min(10, max(0, overall_score))\n"})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting-and-best-practices",children:"Troubleshooting and Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"common-vslam-issues-and-solutions",children:"Common VSLAM Issues and Solutions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Troubleshooting guide for VSLAM systems\nclass VSLAMTroubleshooter:\n    def __init__(self):\n        self.common_issues = {\n            'tracking_lost': {\n                'symptoms': ['No pose output', 'Large jumps in trajectory', 'Feature depletion'],\n                'causes': ['Fast motion', 'Low texture', 'Poor lighting'],\n                'solutions': [\n                    'Reduce motion speed',\n                    'Move to textured environment',\n                    'Improve lighting conditions',\n                    'Increase feature count'\n                ]\n            },\n            'drift_accumulation': {\n                'symptoms': ['Trajectory divergence', 'Growing error over time'],\n                'causes': ['Integration errors', 'Small loop closures', 'Insufficient optimization'],\n                'solutions': [\n                    'Implement loop closure',\n                    'Regular bundle adjustment',\n                    'Use IMU integration',\n                    'Global optimization'\n                ]\n            },\n            'map_degradation': {\n                'symptoms': ['Decreasing map quality', 'Poor relocalization'],\n                'causes': ['Map growing too large', 'Bad measurements accumulating'],\n                'solutions': [\n                    'Map management and pruning',\n                    'Local map window',\n                    'Quality-based filtering',\n                    'Relocalization system'\n                ]\n            }\n        }\n\n    def diagnose_issue(self, symptoms):\n        \"\"\"Diagnose VSLAM issues based on symptoms\"\"\"\n        possible_issues = []\n\n        for issue_name, issue_data in self.common_issues.items():\n            symptom_matches = sum(1 for symptom in symptoms if symptom in issue_data['symptoms'])\n            if symptom_matches > 0:\n                confidence = symptom_matches / len(issue_data['symptoms'])\n                possible_issues.append({\n                    'issue': issue_name,\n                    'confidence': confidence,\n                    'solutions': issue_data['solutions']\n                })\n\n        return sorted(possible_issues, key=lambda x: x['confidence'], reverse=True)\n\n    def performance_monitoring(self):\n        \"\"\"Monitor VSLAM system performance\"\"\"\n        # Track key metrics\n        metrics = {\n            'feature_count': [],\n            'match_ratio': [],\n            'tracking_inliers': [],\n            'processing_time': [],\n            'map_size': [],\n            'keyframe_rate': []\n        }\n\n        # Set thresholds for alerting\n        thresholds = {\n            'low_features': 50,      # Alert if fewer than 50 features\n            'low_matches': 0.1,      # Alert if match ratio < 10%\n            'low_inliers': 0.3,      # Alert if inlier ratio < 30%\n            'high_time': 0.1,        # Alert if processing > 100ms\n            'large_map': 10000       # Alert if map > 10k points\n        }\n\n        return metrics, thresholds\n"})}),"\n",(0,r.jsx)(n.h2,{id:"practical-lab-complete-vslam-system",children:"Practical Lab: Complete VSLAM System"}),"\n",(0,r.jsx)(n.h3,{id:"lab-objective",children:"Lab Objective"}),"\n",(0,r.jsx)(n.p,{children:"Implement a complete VSLAM system that integrates all components: feature detection, pose estimation, mapping, loop closure, and evaluation."}),"\n",(0,r.jsx)(n.h3,{id:"implementation-steps-4",children:"Implementation Steps"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up Isaac Sim environment with robot and camera"}),"\n",(0,r.jsx)(n.li,{children:"Implement complete VSLAM pipeline with all components"}),"\n",(0,r.jsx)(n.li,{children:"Integrate with ROS 2 for communication and visualization"}),"\n",(0,r.jsx)(n.li,{children:"Test system in various environments and conditions"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate performance using ground truth from simulation"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Working complete VSLAM system"}),"\n",(0,r.jsx)(n.li,{children:"Real-time performance with reasonable accuracy"}),"\n",(0,r.jsx)(n.li,{children:"Proper ROS 2 integration and visualization"}),"\n",(0,r.jsx)(n.li,{children:"Quantified performance metrics"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Explain the differences between feature-based, direct, and semi-direct VSLAM approaches."}),"\n",(0,r.jsx)(n.li,{children:"How does loop closure detection improve VSLAM system accuracy?"}),"\n",(0,r.jsx)(n.li,{children:"What are the key challenges in real-time VSLAM implementation?"}),"\n",(0,r.jsx)(n.li,{children:"How do you evaluate VSLAM system performance quantitatively?"}),"\n",(0,r.jsx)(n.li,{children:"What are the main factors affecting VSLAM robustness in real environments?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"After mastering VSLAM systems, students should proceed to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Advanced navigation for humanoid robots"}),"\n",(0,r.jsx)(n.li,{children:"Vision-Language-Action system integration"}),"\n",(0,r.jsx)(n.li,{children:"Sim-to-real transfer techniques"}),"\n",(0,r.jsx)(n.li,{children:"Deep learning enhanced perception systems"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This comprehensive practical lab provides hands-on experience with complete VSLAM system development, essential for Physical AI and Humanoid Robotics applications."})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var a=t(6540);const r={},s=a.createContext(r);function i(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);