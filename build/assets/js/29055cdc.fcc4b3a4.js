"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[7758],{6903:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-3/isaac-ros-integration","title":"Isaac ROS Integration","description":"Overview","source":"@site/docs/module-3/isaac-ros-integration.md","sourceDirName":"module-3","slug":"/module-3/isaac-ros-integration","permalink":"/docs/module-3/isaac-ros-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3/isaac-ros-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Introduction","permalink":"/docs/module-3/isaac-sim-introduction"},"next":{"title":"VSLAM Systems","permalink":"/docs/module-3/vslam-systems"}}');var i=s(4848),t=s(8453);const r={sidebar_position:4},o="Isaac ROS Integration",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Isaac ROS Architecture",id:"isaac-ros-architecture",level:3},{value:"Isaac ROS Components",id:"isaac-ros-components",level:3},{value:"Integration Benefits",id:"integration-benefits",level:3},{value:"Isaac ROS Setup and Configuration",id:"isaac-ros-setup-and-configuration",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Installation Process",id:"installation-process",level:3},{value:"Basic Configuration",id:"basic-configuration",level:3},{value:"Isaac ROS Image Pipeline",id:"isaac-ros-image-pipeline",level:2},{value:"Image Acquisition and Processing",id:"image-acquisition-and-processing",level:3},{value:"Advanced Image Processing Pipeline",id:"advanced-image-processing-pipeline",level:3},{value:"Isaac ROS Depth Pipeline",id:"isaac-ros-depth-pipeline",level:2},{value:"Depth Processing and Point Cloud Generation",id:"depth-processing-and-point-cloud-generation",level:3},{value:"Isaac ROS Lidar Pipeline",id:"isaac-ros-lidar-pipeline",level:2},{value:"LiDAR Simulation and Processing",id:"lidar-simulation-and-processing",level:3},{value:"Isaac ROS Control Pipeline",id:"isaac-ros-control-pipeline",level:2},{value:"Robot Control Integration",id:"robot-control-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Isaac ROS Performance Considerations",id:"isaac-ros-performance-considerations",level:3},{value:"Troubleshooting and Best Practices",id:"troubleshooting-and-best-practices",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Practical Lab: Complete Isaac ROS Integration",id:"practical-lab-complete-isaac-ros-integration",level:2},{value:"Lab Objective",id:"lab-objective",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Step 1: Set up Isaac Sim Environment",id:"step-1-set-up-isaac-sim-environment",level:4},{value:"Lab Exercise: Isaac ROS Integration",id:"lab-exercise-isaac-ros-integration",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Review Questions",id:"review-questions",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"isaac-ros-integration",children:"Isaac ROS Integration"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS Integration provides the bridge between NVIDIA Isaac Sim's photorealistic simulation capabilities and the Robot Operating System (ROS 2) ecosystem. This integration enables seamless development, testing, and deployment of AI-powered robotic systems by combining Isaac Sim's high-fidelity physics and rendering with ROS 2's comprehensive robotics framework."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this section, students will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand the architecture and components of Isaac ROS integration"}),"\n",(0,i.jsx)(n.li,{children:"Set up and configure Isaac ROS bridge for simulation-to-robot transfer"}),"\n",(0,i.jsx)(n.li,{children:"Implement Isaac ROS components for perception and control"}),"\n",(0,i.jsx)(n.li,{children:"Integrate Isaac Sim with ROS 2 communication systems"}),"\n",(0,i.jsx)(n.li,{children:"Optimize Isaac ROS performance for real-time applications"}),"\n",(0,i.jsx)(n.li,{children:"Validate simulation results against real-world performance"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-architecture",children:"Isaac ROS Architecture"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS Bridge"}),": Bidirectional communication layer between Isaac Sim and ROS 2"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Message Conversion"}),": Automatic conversion between Isaac Sim and ROS 2 message formats"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TF Management"}),": Coordinate frame management and transformation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Simulation"}),": Accurate simulation of real-world sensors with ROS 2 interfaces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control Integration"}),": Seamless integration of robot control systems"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-components",children:"Isaac ROS Components"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Image Pipeline"}),": Camera image processing with ROS 2 integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Pipeline"}),": Depth image and point cloud processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lidar Pipeline"}),": LiDAR data processing and conversion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control Pipeline"}),": Robot control command processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception Pipeline"}),": AI-based perception system integration"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"integration-benefits",children:"Integration Benefits"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Photorealistic Simulation"}),": High-fidelity rendering for realistic perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hardware Acceleration"}),": GPU-accelerated simulation and perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Seamless Transfer"}),": Direct integration with ROS 2 workflows"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Realistic Sensors"}),": Accurate simulation of real-world sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AI Training"}),": Synthetic data generation for AI model training"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-setup-and-configuration",children:"Isaac ROS Setup and Configuration"}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# System requirements\n- NVIDIA GPU with RTX support (RTX 2060 or higher recommended)\n- CUDA 11.8 or higher\n- Isaac Sim 2023.1 or higher\n- ROS 2 Humble Hawksbill\n- Ubuntu 22.04 LTS\n\n# Install Isaac ROS dependencies\nsudo apt update\nsudo apt install nvidia-isaac-ros-dev\nsudo apt install nvidia-isaac-ros-gazebo-interfaces\nsudo apt install nvidia-isaac-ros-pointcloud-utils\n"})}),"\n",(0,i.jsx)(n.h3,{id:"installation-process",children:"Installation Process"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS packages\nsudo apt install ros-humble-isaac-ros-*\n\n# Verify installation\ndpkg -l | grep isaac-ros\n\n# Install additional dependencies\nsudo apt install ros-humble-ros-gz\nsudo apt install ros-humble-image-transport-plugins\nsudo apt install ros-humble-compressed-image-transport\n"})}),"\n",(0,i.jsx)(n.h3,{id:"basic-configuration",children:"Basic Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Isaac ROS configuration file\nisaac_ros_common:\n  ros__parameters:\n    # Performance settings\n    enable_profiler: false\n    profiler_filename: "/tmp/isaac_ros_profile.json"\n\n    # Memory management\n    use_pinned_memory: true\n    max_memory_allocation_mb: 4096\n\n    # Communication settings\n    qos_history: 1  # KEEP_LAST\n    qos_depth: 10\n    qos_reliability: 1  # RELIABLE\n    qos_durability: 2  # TRANSIENT_LOCAL\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-image-pipeline",children:"Isaac ROS Image Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"image-acquisition-and-processing",children:"Image Acquisition and Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Isaac ROS Image Pipeline Implementation\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import TransformStamped\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\nclass IsaacImagePipeline(Node):\n    def __init__(self):\n        super().__init__('isaac_image_pipeline')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers\n        self.image_pub = self.create_publisher(Image, 'camera/image_raw', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, 'camera/camera_info', 10)\n\n        # Subscribers\n        self.isaac_image_sub = self.create_subscription(\n            Image,\n            '/isaac_sim/camera/rgb/image',\n            self.isaac_image_callback,\n            10\n        )\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Camera parameters (from Isaac Sim)\n        self.camera_matrix = np.array([\n            [600.0, 0.0, 320.0],  # fx, 0, cx\n            [0.0, 600.0, 240.0],  # 0, fy, cy\n            [0.0, 0.0, 1.0]       # 0, 0, 1\n        ])\n\n        self.distortion_coeffs = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n\n        # Processing parameters\n        self.enable_preprocessing = True\n        self.preprocessing_methods = {\n            'denoising': True,\n            'enhancement': False,\n            'rectification': True\n        }\n\n        self.get_logger().info('Isaac Image Pipeline Initialized')\n\n    def isaac_image_callback(self, msg):\n        \"\"\"Process image from Isaac Sim and republish for ROS 2\"\"\"\n        try:\n            # Convert Isaac Sim image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Apply preprocessing if enabled\n            if self.enable_preprocessing:\n                cv_image = self.preprocess_image(cv_image)\n\n            # Convert back to ROS 2 format\n            processed_msg = self.cv_bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')\n            processed_msg.header = msg.header\n\n            # Publish processed image\n            self.image_pub.publish(processed_msg)\n\n            # Publish camera info\n            self.publish_camera_info(msg.header)\n\n            # Broadcast camera transform\n            self.broadcast_camera_transform(msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing Isaac image: {e}')\n\n    def preprocess_image(self, image):\n        \"\"\"Apply preprocessing to improve image quality\"\"\"\n        processed_image = image.copy()\n\n        if self.preprocessing_methods['denoising']:\n            # Apply denoising\n            processed_image = cv2.fastNlMeansDenoisingColored(\n                processed_image, None, 10, 10, 7, 21\n            )\n\n        if self.preprocessing_methods['enhancement']:\n            # Apply contrast enhancement\n            lab = cv2.cvtColor(processed_image, cv2.COLOR_BGR2LAB)\n            l, a, b = cv2.split(lab)\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            l = clahe.apply(l)\n            lab = cv2.merge([l, a, b])\n            processed_image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n\n        if self.preprocessing_methods['rectification']:\n            # Apply camera rectification (simplified)\n            processed_image = cv2.undistort(\n                processed_image, self.camera_matrix, self.distortion_coeffs\n            )\n\n        return processed_image\n\n    def publish_camera_info(self, header):\n        \"\"\"Publish camera calibration information\"\"\"\n        camera_info_msg = CameraInfo()\n        camera_info_msg.header = header\n        camera_info_msg.header.frame_id = 'camera_rgb_optical_frame'\n\n        # Set camera parameters\n        camera_info_msg.width = 640\n        camera_info_msg.height = 480\n        camera_info_msg.distortion_model = 'plumb_bob'\n\n        # Distortion coefficients\n        camera_info_msg.d = self.distortion_coeffs.tolist()\n\n        # Camera matrix\n        camera_info_msg.k = self.camera_matrix.flatten().tolist()\n\n        # Rectification matrix (identity for monocular)\n        camera_info_msg.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n\n        # Projection matrix\n        camera_info_msg.p = [\n            self.camera_matrix[0, 0], 0.0, self.camera_matrix[0, 2], 0.0,\n            0.0, self.camera_matrix[1, 1], self.camera_matrix[1, 2], 0.0,\n            0.0, 0.0, 1.0, 0.0\n        ]\n\n        self.camera_info_pub.publish(camera_info_msg)\n\n    def broadcast_camera_transform(self, header):\n        \"\"\"Broadcast camera optical frame transform\"\"\"\n        t = TransformStamped()\n\n        t.header.stamp = header.stamp\n        t.header.frame_id = 'camera_link'\n        t.child_frame_id = 'camera_rgb_optical_frame'\n\n        # Camera optical frame is rotated from camera link\n        # (X right, Y down, Z forward convention)\n        t.transform.translation.x = 0.0\n        t.transform.translation.y = 0.0\n        t.transform.translation.z = 0.0\n\n        # Rotate from camera_link (X right, Y up, Z forward) to optical frame\n        # This is a 90 degree rotation around X axis\n        import math\n        t.transform.rotation.x = 0.0\n        t.transform.rotation.y = -math.sqrt(2)/2\n        t.transform.rotation.z = 0.0\n        t.transform.rotation.w = math.sqrt(2)/2\n\n        self.tf_broadcaster.sendTransform(t)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-image-processing-pipeline",children:"Advanced Image Processing Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Advanced Isaac ROS image processing with AI integration\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CompressedImage\nfrom vision_msgs.msg import Detection2DArray, ClassificationResult\nfrom std_msgs.msg import Float32MultiArray\nfrom isaac_ros_tensor_list_interfaces.msg import TensorList\nfrom isaac_ros_detectnet_interfaces.msg import Detection2DArray as IsaacDetection2DArray\nimport torch\nimport torchvision.transforms as transforms\n\nclass IsaacAIImageProcessor(Node):\n    def __init__(self):\n        super().__init__('isaac_ai_image_processor')\n\n        # Publishers\n        self.detection_pub = self.create_publisher(Detection2DArray, 'detections', 10)\n        self.classification_pub = self.create_publisher(ClassificationResult, 'classification', 10)\n        self.tensor_pub = self.create_publisher(TensorList, 'tensor_outputs', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n\n        # AI model (using TorchVision as example)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.detection_model = self.load_detection_model()\n        self.classification_model = self.load_classification_model()\n\n        # Image preprocessing\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((224, 224)),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n        # Performance monitoring\n        self.frame_count = 0\n        self.processing_times = []\n\n        self.get_logger().info('Isaac AI Image Processor Initialized')\n\n    def load_detection_model(self):\n        \"\"\"Load pre-trained detection model\"\"\"\n        try:\n            # Using TorchVision's pre-trained model as example\n            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n            model.to(self.device)\n            model.eval()\n            return model\n        except Exception as e:\n            self.get_logger().error(f'Error loading detection model: {e}')\n            return None\n\n    def load_classification_model(self):\n        \"\"\"Load pre-trained classification model\"\"\"\n        try:\n            model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n            model.to(self.device)\n            model.eval()\n            return model\n        except Exception as e:\n            self.get_logger().error(f'Error loading classification model: {e}')\n            return None\n\n    def image_callback(self, msg):\n        \"\"\"Process image with AI models\"\"\"\n        start_time = self.get_clock().now().nanoseconds * 1e-9\n\n        try:\n            # Convert ROS image to tensor\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            tensor_image = self.preprocess_image_tensor(cv_image)\n\n            # Run detection\n            if self.detection_model:\n                detections = self.run_detection(tensor_image, cv_image.shape[:2])\n                if detections:\n                    self.publish_detections(detections, msg.header)\n\n            # Run classification\n            if self.classification_model:\n                classification = self.run_classification(tensor_image)\n                if classification:\n                    self.publish_classification(classification, msg.header)\n\n            # Monitor performance\n            end_time = self.get_clock().now().nanoseconds * 1e-9\n            processing_time = end_time - start_time\n            self.processing_times.append(processing_time)\n\n            if len(self.processing_times) > 100:\n                self.processing_times.pop(0)\n\n            self.frame_count += 1\n\n            # Log performance every 100 frames\n            if self.frame_count % 100 == 0:\n                avg_time = sum(self.processing_times) / len(self.processing_times)\n                fps = 1.0 / avg_time if avg_time > 0 else 0\n                self.get_logger().info(\n                    f'AI Processing: {fps:.1f} FPS, avg time: {avg_time*1000:.1f}ms'\n                )\n\n        except Exception as e:\n            self.get_logger().error(f'Error in AI processing: {e}')\n\n    def preprocess_image_tensor(self, image):\n        \"\"\"Preprocess image for AI models\"\"\"\n        # Convert BGR to RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Convert to tensor and normalize\n        tensor = self.transform(image_rgb).unsqueeze(0)\n        tensor = tensor.to(self.device)\n\n        return tensor\n\n    def run_detection(self, tensor_image, image_shape):\n        \"\"\"Run object detection on image\"\"\"\n        try:\n            with torch.no_grad():\n                results = self.detection_model(tensor_image)\n\n            # Process detection results\n            detections = []\n            for result in results.xyxy[0]:  # yolov5 results format\n                x1, y1, x2, y2, conf, cls = result\n                detection = {\n                    'bbox': [float(x1), float(y1), float(x2-x1), float(y2-y1)],\n                    'confidence': float(conf),\n                    'class_id': int(cls),\n                    'class_name': self.get_class_name(int(cls))\n                }\n                detections.append(detection)\n\n            return detections\n\n        except Exception as e:\n            self.get_logger().error(f'Detection error: {e}')\n            return None\n\n    def run_classification(self, tensor_image):\n        \"\"\"Run image classification\"\"\"\n        try:\n            with torch.no_grad():\n                outputs = self.classification_model(tensor_image)\n                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n\n                # Get top prediction\n                top_prob, top_class = torch.topk(probabilities, 1)\n\n                classification = {\n                    'class_id': int(top_class.item()),\n                    'confidence': float(top_prob.item()),\n                    'class_name': self.get_imagenet_class_name(int(top_class.item()))\n                }\n\n                return classification\n\n        except Exception as e:\n            self.get_logger().error(f'Classification error: {e}')\n            return None\n\n    def get_class_name(self, class_id):\n        \"\"\"Get class name for detection model\"\"\"\n        # This would map to actual class names\n        # Using generic names as placeholder\n        class_names = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n            'bus', 'train', 'truck', 'boat', 'traffic light',\n            # ... more classes\n        ]\n        return class_names[class_id] if class_id < len(class_names) else f'class_{class_id}'\n\n    def get_imagenet_class_name(self, class_id):\n        \"\"\"Get ImageNet class name\"\"\"\n        # This would map to ImageNet class names\n        # Using generic name as placeholder\n        return f'imagenet_class_{class_id}'\n\n    def publish_detections(self, detections, header):\n        \"\"\"Publish detection results to ROS 2\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for det in detections:\n            detection_2d = Detection2D()\n            detection_2d.header = header\n\n            # Set bounding box\n            bbox = det['bbox']\n            detection_2d.bbox.center.x = bbox[0] + bbox[2] / 2.0\n            detection_2d.bbox.center.y = bbox[1] + bbox[3] / 2.0\n            detection_2d.bbox.size_x = bbox[2]\n            detection_2d.bbox.size_y = bbox[3]\n\n            # Set classification result\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = str(det['class_id'])\n            hypothesis.hypothesis.score = det['confidence']\n\n            detection_2d.results.append(hypothesis)\n            detection_array.detections.append(detection_2d)\n\n        self.detection_pub.publish(detection_array)\n\n    def publish_classification(self, classification, header):\n        \"\"\"Publish classification results to ROS 2\"\"\"\n        result = ClassificationResult()\n        result.header = header\n        result.header.frame_id = 'camera_rgb_optical_frame'\n\n        # Set classification data\n        result.class_label = classification['class_name']\n        result.score = classification['confidence']\n\n        self.classification_pub.publish(result)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-depth-pipeline",children:"Isaac ROS Depth Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"depth-processing-and-point-cloud-generation",children:"Depth Processing and Point Cloud Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Isaac ROS depth pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, PointField, CameraInfo\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import Header\nfrom sensor_msgs_py import point_cloud2\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport struct\n\nclass IsaacDepthPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_depth_pipeline\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers\n        self.depth_pub = self.create_publisher(Image, \'camera/depth/image_raw\', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, \'camera/depth/points\', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, \'camera/depth/camera_info\', 10)\n\n        # Subscribers\n        self.depth_sub = self.create_subscription(\n            Image, \'/isaac_sim/camera/depth/image\', self.depth_callback, 10)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/isaac_sim/camera/depth/camera_info\', self.camera_info_callback, 10)\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.depth_scale = 1.0  # meters per depth unit\n\n        # Processing parameters\n        self.enable_pointcloud_generation = True\n        self.pointcloud_decimation = 4  # Generate point cloud from every 4th pixel\n        self.min_depth = 0.1  # meters\n        self.max_depth = 10.0  # meters\n\n        self.get_logger().info(\'Isaac Depth Pipeline Initialized\')\n\n    def camera_info_callback(self, msg):\n        """Update camera parameters from camera info"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.get_logger().debug(\'Updated camera matrix from camera info\')\n\n    def depth_callback(self, msg):\n        """Process depth image from Isaac Sim"""\n        try:\n            # Convert depth image to numpy array\n            depth_array = self.ros_image_to_depth_array(msg)\n\n            # Validate depth values\n            depth_array = self.validate_depth_values(depth_array)\n\n            # Publish depth image\n            self.depth_pub.publish(msg)\n\n            # Generate point cloud if enabled\n            if self.enable_pointcloud_generation and self.camera_matrix is not None:\n                pointcloud_msg = self.generate_pointcloud(\n                    depth_array, self.camera_matrix, msg.header\n                )\n                if pointcloud_msg:\n                    self.pointcloud_pub.publish(pointcloud_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing depth image: {e}\')\n\n    def ros_image_to_depth_array(self, depth_msg):\n        """Convert ROS depth image message to numpy array"""\n        if depth_msg.encoding == \'32FC1\':\n            # 32-bit float depth image\n            depth_data = np.frombuffer(depth_msg.data, dtype=np.float32)\n            depth_array = depth_data.reshape((depth_msg.height, depth_msg.width))\n        elif depth_msg.encoding == \'16UC1\':\n            # 16-bit unsigned integer depth image (millimeters)\n            depth_data = np.frombuffer(depth_msg.data, dtype=np.uint16)\n            depth_array = depth_data.astype(np.float32) / 1000.0  # Convert mm to m\n        else:\n            self.get_logger().error(f\'Unsupported depth encoding: {depth_msg.encoding}\')\n            return np.zeros((depth_msg.height, depth_msg.width), dtype=np.float32)\n\n        return depth_array\n\n    def validate_depth_values(self, depth_array):\n        """Validate and clean depth values"""\n        # Replace invalid values (NaN, infinity) with 0\n        depth_array = np.nan_to_num(depth_array, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # Apply depth range limits\n        depth_array = np.clip(depth_array, self.min_depth, self.max_depth)\n\n        return depth_array\n\n    def generate_pointcloud(self, depth_array, camera_matrix, header):\n        """Generate point cloud from depth image"""\n        height, width = depth_array.shape\n\n        # Get camera parameters\n        fx = camera_matrix[0, 0]\n        fy = camera_matrix[1, 1]\n        cx = camera_matrix[0, 2]\n        cy = camera_matrix[1, 2]\n\n        # Create coordinate grids\n        u_coords, v_coords = np.meshgrid(\n            np.arange(width), np.arange(height)\n        )\n\n        # Convert pixel coordinates to 3D points\n        z = depth_array  # Depth values\n        x = (u_coords - cx) * z / fx\n        y = (v_coords - cy) * z / fy\n\n        # Apply decimation for performance\n        x_decim = x[::self.pointcloud_decimation, ::self.pointcloud_decimation]\n        y_decim = y[::self.pointcloud_decimation, ::self.pointcloud_decimation]\n        z_decim = z[::self.pointcloud_decimation, ::self.pointcloud_decimation]\n\n        # Create valid mask (remove invalid depth points)\n        valid_mask = z_decim > 0  # Only positive depth values\n\n        # Extract valid points\n        x_valid = x_decim[valid_mask]\n        y_valid = y_decim[valid_mask]\n        z_valid = z_decim[valid_mask]\n\n        # Create PointCloud2 message\n        points = np.column_stack((x_valid, y_valid, z_valid)).astype(np.float32)\n\n        if len(points) == 0:\n            return None\n\n        # Create PointCloud2 message\n        fields = [\n            PointField(name=\'x\', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'y\', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'z\', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n\n        header.frame_id = \'camera_depth_optical_frame\'\n        pointcloud_msg = point_cloud2.create_cloud(header, fields, points)\n\n        return pointcloud_msg\n\n    def pointcloud_to_depth_image(self, pointcloud_msg):\n        """Convert point cloud back to depth image (for validation)"""\n        points = []\n        for point in point_cloud2.read_points(pointcloud_msg, field_names=("x", "y", "z")):\n            points.append(point)\n\n        points = np.array(points)\n\n        # This would project 3D points back to 2D image coordinates\n        # Implementation would depend on camera parameters\n        pass\n\n    def filter_pointcloud(self, pointcloud_msg):\n        """Apply filtering to point cloud data"""\n        # Convert to numpy array for processing\n        points_gen = point_cloud2.read_points(pointcloud_msg, field_names=("x", "y", "z"))\n        points = np.array(list(points_gen)).astype(np.float32)\n\n        # Apply statistical outlier removal (simplified)\n        if len(points) > 100:\n            # Calculate distances to neighbors (simplified approach)\n            # In practice, use proper PCL or similar library\n            pass\n\n        # Apply voxel grid filtering (simplified)\n        # In practice, use proper downsampling techniques\n        filtered_points = points[::2]  # Downsample by factor of 2\n\n        # Create new point cloud message\n        fields = [\n            PointField(name=\'x\', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'y\', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'z\', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n\n        filtered_cloud = point_cloud2.create_cloud(\n            pointcloud_msg.header, fields, filtered_points\n        )\n\n        return filtered_cloud\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-lidar-pipeline",children:"Isaac ROS Lidar Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"lidar-simulation-and-processing",children:"LiDAR Simulation and Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Isaac ROS LiDAR pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2, PointField\nfrom geometry_msgs.msg import Point32\nfrom std_msgs.msg import Header\nfrom sensor_msgs_py import point_cloud2\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass IsaacLidarPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_lidar_pipeline\')\n\n        # Publishers\n        self.scan_pub = self.create_publisher(LaserScan, \'scan\', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, \'lidar/points\', 10)\n\n        # Subscribers\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \'/isaac_sim/lidar/scan\', self.lidar_callback, 10)\n\n        # LiDAR parameters\n        self.lidar_params = {\n            \'range_min\': 0.1,\n            \'range_max\': 25.0,\n            \'angle_min\': -np.pi,\n            \'angle_max\': np.pi,\n            \'angle_increment\': np.pi / 180.0,  # 1 degree\n            \'time_increment\': 0.0,\n            \'scan_time\': 0.1\n        }\n\n        # Processing parameters\n        self.enable_scan_denoising = True\n        self.enable_dynamic_filtering = True\n        self.max_range_threshold = 20.0  # Filter out ranges beyond this\n        self.min_range_threshold = 0.2   # Filter out ranges below this\n\n        self.get_logger().info(\'Isaac LiDAR Pipeline Initialized\')\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data from Isaac Sim"""\n        try:\n            # Validate ranges\n            validated_msg = self.validate_lidar_data(msg)\n\n            # Apply denoising if enabled\n            if self.enable_scan_denoising:\n                validated_msg = self.denoise_scan(validated_msg)\n\n            # Apply dynamic filtering if enabled\n            if self.enable_dynamic_filtering:\n                validated_msg = self.filter_dynamic_objects(validated_msg)\n\n            # Publish processed scan\n            self.scan_pub.publish(validated_msg)\n\n            # Convert to point cloud and publish\n            pointcloud_msg = self.scan_to_pointcloud(validated_msg)\n            if pointcloud_msg:\n                self.pointcloud_pub.publish(pointcloud_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing LiDAR data: {e}\')\n\n    def validate_lidar_data(self, scan_msg):\n        """Validate and clean LiDAR scan data"""\n        validated_msg = LaserScan()\n        validated_msg.header = scan_msg.header\n        validated_msg.angle_min = scan_msg.angle_min\n        validated_msg.angle_max = scan_msg.angle_max\n        validated_msg.angle_increment = scan_msg.angle_increment\n        validated_msg.time_increment = scan_msg.time_increment\n        validated_msg.scan_time = scan_msg.scan_time\n        validated_msg.range_min = scan_msg.range_min\n        validated_msg.range_max = scan_msg.range_max\n\n        # Process ranges\n        ranges = np.array(scan_msg.ranges)\n        ranges = np.nan_to_num(ranges, nan=np.inf, posinf=np.inf, neginf=0.0)\n\n        # Apply range thresholds\n        ranges = np.where(ranges < self.min_range_threshold, 0.0, ranges)\n        ranges = np.where(ranges > self.max_range_threshold, np.inf, ranges)\n\n        validated_msg.ranges = ranges.tolist()\n        validated_msg.intensities = scan_msg.intensities\n\n        return validated_msg\n\n    def denoise_scan(self, scan_msg):\n        """Apply denoising to LiDAR scan"""\n        # Convert to numpy array for processing\n        ranges = np.array(scan_msg.ranges)\n\n        # Apply median filter to remove noise\n        # Pad the array to handle edges\n        padded_ranges = np.pad(ranges, 1, mode=\'edge\')\n        filtered_ranges = np.zeros_like(ranges)\n\n        for i in range(len(ranges)):\n            # Get neighborhood (3-point window)\n            neighborhood = padded_ranges[i:i+3]\n            # Remove invalid values (inf, 0) from consideration\n            valid_values = neighborhood[np.isfinite(neighborhood) & (neighborhood > 0)]\n\n            if len(valid_values) > 0:\n                filtered_ranges[i] = np.median(valid_values)\n            else:\n                filtered_ranges[i] = ranges[i]  # Keep original if no valid neighbors\n\n        scan_msg.ranges = filtered_ranges.tolist()\n        return scan_msg\n\n    def filter_dynamic_objects(self, scan_msg):\n        """Filter out dynamic objects from scan (simplified approach)"""\n        # This would typically require temporal information\n        # For this example, we\'ll use a simple approach based on range consistency\n        # across multiple consecutive scans\n\n        # In a real implementation, this would compare with previous scans\n        # and use motion models to identify dynamic objects\n\n        # For now, just apply basic range-based filtering\n        ranges = np.array(scan_msg.ranges)\n\n        # Identify potential dynamic objects (rapidly changing ranges)\n        # This is a simplified approach - real implementation would be more sophisticated\n        filtered_ranges = ranges.copy()\n\n        # In practice, use temporal consistency checks\n        # Compare with previous scan to identify inconsistencies\n\n        scan_msg.ranges = filtered_ranges.tolist()\n        return scan_msg\n\n    def scan_to_pointcloud(self, scan_msg):\n        """Convert LaserScan to PointCloud2"""\n        if not scan_msg.ranges:\n            return None\n\n        # Calculate angles for each range measurement\n        angles = np.arange(\n            scan_msg.angle_min,\n            scan_msg.angle_max + scan_msg.angle_increment,\n            scan_msg.angle_increment\n        )\n\n        # Create points from ranges and angles\n        points = []\n        for i, (range_val, angle) in enumerate(zip(scan_msg.ranges, angles)):\n            if 0 < range_val < float(\'inf\'):\n                x = range_val * np.cos(angle)\n                y = range_val * np.sin(angle)\n                z = 0.0  # Assume LiDAR is level\n                points.append([x, y, z])\n\n        if not points:\n            return None\n\n        # Create PointCloud2 message\n        points_array = np.array(points, dtype=np.float32)\n\n        fields = [\n            PointField(name=\'x\', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'y\', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'z\', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n\n        header = scan_msg.header\n        header.frame_id = \'lidar_link\'  # Adjust as needed\n\n        pointcloud_msg = point_cloud2.create_cloud(header, fields, points_array)\n        return pointcloud_msg\n\n    def pointcloud_to_scan(self, pointcloud_msg):\n        """Convert PointCloud2 to LaserScan (reverse operation)"""\n        # Extract points from point cloud\n        points_gen = point_cloud2.read_points(\n            pointcloud_msg,\n            field_names=("x", "y", "z"),\n            skip_nans=True\n        )\n\n        points = [(x, y, z) for x, y, z in points_gen]\n\n        if not points:\n            return None\n\n        # Convert to polar coordinates\n        ranges_and_angles = []\n        for x, y, z in points:\n            range_val = np.sqrt(x**2 + y**2)\n            angle = np.arctan2(y, x)\n            ranges_and_angles.append((range_val, angle))\n\n        # Sort by angle\n        ranges_and_angles.sort(key=lambda x: x[1])\n\n        # Create LaserScan message\n        scan_msg = LaserScan()\n        scan_msg.header = pointcloud_msg.header\n        scan_msg.angle_min = ranges_and_angles[0][1] if ranges_and_angles else -np.pi\n        scan_msg.angle_max = ranges_and_angles[-1][1] if ranges_and_angles else np.pi\n        scan_msg.angle_increment = 0.01  # Adjust as needed\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 25.0\n\n        # Populate ranges (this is simplified - real implementation would need proper binning)\n        scan_msg.ranges = [range_val for range_val, angle in ranges_and_angles]\n\n        return scan_msg\n\n    def create_virtual_lidar(self, robot_pose, environment_data):\n        """Create virtual LiDAR readings from environment data"""\n        # This would simulate LiDAR based on robot pose and environment\n        # In practice, this is handled by Isaac Sim\'s physics engine\n        pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-control-pipeline",children:"Isaac ROS Control Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"robot-control-integration",children:"Robot Control Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Isaac ROS control pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\nfrom tf2_ros import TransformListener, Buffer\nimport tf2_geometry_msgs\n\nclass IsaacControlPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_control_pipeline\')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/isaac_sim/cmd_vel\', 10)\n        self.joint_cmd_pub = self.create_publisher(Float64MultiArray, \'/isaac_sim/joint_commands\', 10)\n\n        # Subscribers\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'cmd_vel\', self.cmd_vel_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, \'odom\', self.odom_callback, 10)\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'joint_states\', self.joint_state_callback, 10)\n\n        # TF\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Robot state\n        self.current_pose = np.eye(4)\n        self.current_velocity = np.zeros(6)  # [vx, vy, vz, wx, wy, wz]\n        self.joint_positions = {}\n        self.joint_velocities = {}\n\n        # Control parameters\n        self.max_linear_vel = 1.0  # m/s\n        self.max_angular_vel = 1.0  # rad/s\n        self.control_frequency = 50  # Hz\n        self.safety_margin = 0.1  # meters\n\n        # Safety system\n        self.emergency_stop = False\n        self.safety_enabled = True\n\n        # Create control timer\n        self.control_timer = self.create_timer(1.0/self.control_frequency, self.control_loop)\n\n        self.get_logger().info(\'Isaac Control Pipeline Initialized\')\n\n    def cmd_vel_callback(self, msg):\n        """Handle velocity commands from ROS 2"""\n        if self.emergency_stop:\n            # Ignore commands during emergency stop\n            self.stop_robot()\n            return\n\n        # Validate and limit command\n        cmd_vel = Twist()\n        cmd_vel.linear.x = max(-self.max_linear_vel, min(self.max_linear_vel, msg.linear.x))\n        cmd_vel.linear.y = max(-self.max_linear_vel, min(self.max_linear_vel, msg.linear.y))\n        cmd_vel.linear.z = max(-self.max_linear_vel, min(self.max_linear_vel, msg.linear.z))\n        cmd_vel.angular.z = max(-self.max_angular_vel, min(self.max_angular_vel, msg.angular.z))\n\n        # Publish to Isaac Sim\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        self.get_logger().debug(\n            f\'Command: linear=({cmd_vel.linear.x:.2f}, {cmd_vel.linear.y:.2f}, {cmd_vel.linear.z:.2f}), \'\n            f\'angular=({cmd_vel.angular.x:.2f}, {cmd_vel.angular.y:.2f}, {cmd_vel.angular.z:.2f})\'\n        )\n\n    def odom_callback(self, msg):\n        """Update robot pose from odometry"""\n        # Extract position\n        self.current_pose[0, 3] = msg.pose.pose.position.x\n        self.current_pose[1, 3] = msg.pose.pose.position.y\n        self.current_pose[2, 3] = msg.pose.pose.position.z\n\n        # Extract orientation (quaternion to rotation matrix)\n        quat = msg.pose.pose.orientation\n        self.current_pose[:3, :3] = self.quaternion_to_rotation_matrix([\n            quat.x, quat.y, quat.z, quat.w\n        ])\n\n        # Extract linear velocity\n        self.current_velocity[0] = msg.twist.twist.linear.x\n        self.current_velocity[1] = msg.twist.twist.linear.y\n        self.current_velocity[2] = msg.twist.twist.linear.z\n        self.current_velocity[3] = msg.twist.twist.angular.x\n        self.current_velocity[4] = msg.twist.twist.angular.y\n        self.current_velocity[5] = msg.twist.twist.angular.z\n\n    def joint_state_callback(self, msg):\n        """Update joint states"""\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n            if i < len(msg.velocity):\n                self.joint_velocities[name] = msg.velocity[i]\n\n    def control_loop(self):\n        """Main control loop"""\n        # Check safety conditions\n        if self.safety_enabled:\n            self.check_safety_conditions()\n\n        # Process any control updates\n        self.update_control_system()\n\n    def check_safety_conditions(self):\n        """Check safety conditions and enforce limits"""\n        # This would typically check:\n        # - Joint limits\n        # - Collision avoidance\n        # - Velocity limits\n        # - Position limits\n\n        # Example: Check if robot is too close to obstacles\n        # (This would require sensor data integration)\n        pass\n\n    def update_control_system(self):\n        """Update control system state"""\n        # This would update PID controllers, trajectory tracking, etc.\n        # For now, just log the current state\n        position = self.current_pose[:3, 3]\n        orientation = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\n\n        self.get_logger().debug(\n            f\'Robot State: pos=({position[0]:.2f}, {position[1]:.2f}, {position[2]:.2f}), \'\n            f\'orient=({orientation[0]:.3f}, {orientation[1]:.3f}, {orientation[2]:.3f}, {orientation[3]:.3f})\'\n        )\n\n    def quaternion_to_rotation_matrix(self, quat):\n        """Convert quaternion [x, y, z, w] to rotation matrix"""\n        x, y, z, w = quat\n\n        R = np.array([\n            [1 - 2*(y*y + z*z), 2*(x*y - w*z), 2*(x*z + w*y)],\n            [2*(x*y + w*z), 1 - 2*(x*x + z*z), 2*(y*z - w*x)],\n            [2*(x*z - w*y), 2*(y*z + w*x), 1 - 2*(x*x + y*y)]\n        ])\n\n        return R\n\n    def rotation_matrix_to_quaternion(self, R):\n        """Convert rotation matrix to quaternion [x, y, z, w]"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * w\n            w = 0.25 * s\n            x = (R[2, 1] - R[1, 2]) / s\n            y = (R[0, 2] - R[2, 0]) / s\n            z = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                w = (R[2, 1] - R[1, 2]) / s\n                x = 0.25 * s\n                y = (R[0, 1] + R[1, 0]) / s\n                z = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                w = (R[0, 2] - R[2, 0]) / s\n                x = (R[0, 1] + R[1, 0]) / s\n                y = 0.25 * s\n                z = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                w = (R[1, 0] - R[0, 1]) / s\n                x = (R[0, 2] + R[2, 0]) / s\n                y = (R[1, 2] + R[2, 1]) / s\n                z = 0.25 * s\n\n        return np.array([x, y, z, w])\n\n    def stop_robot(self):\n        """Send stop command to robot"""\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n    def emergency_stop(self):\n        """Activate emergency stop"""\n        self.emergency_stop = True\n        self.stop_robot()\n        self.get_logger().warn(\'EMERGENCY STOP ACTIVATED\')\n\n    def release_emergency_stop(self):\n        """Release emergency stop"""\n        self.emergency_stop = False\n        self.get_logger().info(\'Emergency stop released\')\n\n    def send_joint_commands(self, joint_positions, joint_velocities=None, joint_efforts=None):\n        """Send joint position/velocity/effort commands"""\n        cmd_msg = Float64MultiArray()\n\n        # Fill with position commands (simplified)\n        cmd_msg.data = list(joint_positions.values())\n\n        self.joint_cmd_pub.publish(cmd_msg)\n\n    def transform_pose(self, pose, target_frame, source_frame):\n        """Transform pose between coordinate frames"""\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                target_frame, source_frame, rclpy.time.Time()\n            )\n            transformed_pose = tf2_geometry_msgs.do_transform_pose(pose, transform)\n            return transformed_pose\n        except Exception as e:\n            self.get_logger().error(f\'Transform error: {e}\')\n            return None\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-performance-considerations",children:"Isaac ROS Performance Considerations"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Performance optimization for Isaac ROS\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan\nfrom std_msgs.msg import UInt32\nimport time\nfrom collections import deque\nimport threading\nimport psutil\nimport GPUtil\n\nclass IsaacROSPerformanceOptimizer(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_performance_optimizer')\n\n        # Publishers for performance metrics\n        self.fps_pub = self.create_publisher(UInt32, 'performance/fps', 10)\n        self.cpu_pub = self.create_publisher(UInt32, 'performance/cpu_percent', 10)\n        self.gpu_pub = self.create_publisher(UInt32, 'performance/gpu_percent', 10)\n\n        # Performance monitoring\n        self.frame_times = deque(maxlen=30)  # Last 30 frame times\n        self.processing_times = deque(maxlen=30)\n        self.memory_usage = deque(maxlen=30)\n        self.cpu_usage = deque(maxlen=30)\n\n        # Performance targets\n        self.target_fps = 30\n        self.max_processing_time = 0.033  # 33ms for 30 FPS\n        self.max_memory_percent = 80.0\n\n        # Adaptive processing parameters\n        self.adaptive_params = {\n            'image_decimation': 1,  # Process every Nth frame\n            'pointcloud_decimation': 4,  # Process every 4th point\n            'feature_count': 1000,  # Number of features to track\n            'bundle_adjustment_freq': 10  # BA every N keyframes\n        }\n\n        # Threading for performance monitoring\n        self.monitoring_thread = threading.Thread(target=self.monitor_performance)\n        self.monitoring_thread.daemon = True\n        self.monitoring_thread.start()\n\n        # Performance timer\n        self.perf_timer = self.create_timer(1.0, self.publish_performance_metrics)\n\n        self.get_logger().info('Isaac ROS Performance Optimizer Initialized')\n\n    def monitor_performance(self):\n        \"\"\"Monitor system performance in separate thread\"\"\"\n        while rclpy.ok():\n            # CPU usage\n            cpu_percent = psutil.cpu_percent(interval=0.1)\n            self.cpu_usage.append(cpu_percent)\n\n            # Memory usage\n            memory_percent = psutil.virtual_memory().percent\n            self.memory_usage.append(memory_percent)\n\n            # GPU usage if available\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                gpu_percent = gpus[0].load * 100\n                self.gpu_usage.append(gpu_percent)\n            else:\n                self.gpu_usage.append(0)\n\n            time.sleep(0.1)\n\n    def adaptive_processing(self, image_data, sensor_type='image'):\n        \"\"\"Adapt processing based on performance\"\"\"\n        start_time = time.time()\n\n        if sensor_type == 'image':\n            # Adjust processing based on performance\n            if self.current_fps < self.target_fps * 0.8:\n                # Reduce processing load\n                self.adaptive_params['feature_count'] = max(500,\n                    int(self.adaptive_params['feature_count'] * 0.9))\n                self.adaptive_params['image_decimation'] = min(5,\n                    self.adaptive_params['image_decimation'] + 1)\n            elif self.current_fps > self.target_fps * 1.1:\n                # Can afford more processing\n                self.adaptive_params['feature_count'] = min(2000,\n                    int(self.adaptive_params['feature_count'] * 1.1))\n                self.adaptive_params['image_decimation'] = max(1,\n                    self.adaptive_params['image_decimation'] - 1)\n\n            # Apply decimation\n            if self.adaptive_params['image_decimation'] > 1:\n                # Process every Nth frame\n                if self.frame_count % self.adaptive_params['image_decimation'] != 0:\n                    return None  # Skip processing\n\n            # Process image with adjusted parameters\n            processed_result = self.process_image_adaptive(image_data)\n\n        elif sensor_type == 'lidar':\n            # Similar adaptation for LiDAR processing\n            if self.adaptive_params['pointcloud_decimation'] > 1:\n                # Apply decimation to LiDAR data\n                processed_result = self.decimate_pointcloud(image_data)\n\n        processing_time = time.time() - start_time\n        self.processing_times.append(processing_time)\n\n        return processed_result\n\n    def process_image_adaptive(self, image_data):\n        \"\"\"Process image with adaptive parameters\"\"\"\n        # Extract features with adaptive count\n        feature_detector = cv2.ORB_create(\n            nfeatures=self.adaptive_params['feature_count'],\n            scaleFactor=1.2,\n            nlevels=8,\n            edgeThreshold=31,\n            patchSize=31,\n            fastThreshold=20\n        )\n\n        keypoints, descriptors = feature_detector.detectAndCompute(image_data, None)\n        return keypoints, descriptors\n\n    def decimate_pointcloud(self, pointcloud_data):\n        \"\"\"Decimate point cloud for performance\"\"\"\n        # Reduce point cloud density\n        decimation_factor = self.adaptive_params['pointcloud_decimation']\n        return pointcloud_data[::decimation_factor]\n\n    def get_current_performance(self):\n        \"\"\"Get current performance metrics\"\"\"\n        if len(self.frame_times) > 1:\n            avg_frame_time = sum(self.frame_times) / len(self.frame_times)\n            current_fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n        else:\n            current_fps = 0\n\n        avg_processing_time = sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0\n        avg_cpu_usage = sum(self.cpu_usage) / len(self.cpu_usage) if self.cpu_usage else 0\n        avg_memory_usage = sum(self.memory_usage) / len(self.memory_usage) if self.memory_usage else 0\n\n        return {\n            'fps': current_fps,\n            'avg_processing_time': avg_processing_time,\n            'cpu_usage': avg_cpu_usage,\n            'memory_usage': avg_memory_usage,\n            'adaptive_params': self.adaptive_params.copy()\n        }\n\n    def publish_performance_metrics(self):\n        \"\"\"Publish performance metrics\"\"\"\n        perf_metrics = self.get_current_performance()\n\n        # Publish FPS\n        fps_msg = UInt32()\n        fps_msg.data = int(perf_metrics['fps'])\n        self.fps_pub.publish(fps_msg)\n\n        # Publish CPU usage\n        cpu_msg = UInt32()\n        cpu_msg.data = int(perf_metrics['cpu_usage'])\n        self.cpu_pub.publish(cpu_msg)\n\n        # Publish GPU usage if available\n        if hasattr(self, 'gpu_usage') and self.gpu_usage:\n            gpu_msg = UInt32()\n            gpu_msg.data = int(sum(self.gpu_usage) / len(self.gpu_usage))\n            self.gpu_pub.publish(gpu_msg)\n\n        # Log performance if degraded\n        if perf_metrics['fps'] < self.target_fps * 0.7:\n            self.get_logger().warn(\n                f'Performance degraded: {perf_metrics[\"fps\"]:.1f} FPS '\n                f'(target: {self.target_fps}), '\n                f'CPU: {perf_metrics[\"cpu_usage\"]:.1f}%, '\n                f'Memory: {perf_metrics[\"memory_usage\"]:.1f}%'\n            )\n\n    def optimize_pipeline(self):\n        \"\"\"Optimize entire pipeline based on performance\"\"\"\n        current_perf = self.get_current_performance()\n\n        # Adjust pipeline parameters based on performance\n        if current_perf['fps'] < self.target_fps * 0.5:\n            # Significantly below target - aggressive optimization\n            self.get_logger().warn('Significant performance degradation detected - applying aggressive optimization')\n            self.adaptive_params['feature_count'] = max(200, self.adaptive_params['feature_count'] // 2)\n            self.adaptive_params['image_decimation'] = min(10, self.adaptive_params['image_decimation'] * 2)\n            self.adaptive_params['pointcloud_decimation'] = min(16, self.adaptive_params['pointcloud_decimation'] * 2)\n        elif current_perf['fps'] > self.target_fps * 1.2:\n            # Above target - can afford more processing\n            self.adaptive_params['feature_count'] = min(2000, self.adaptive_params['feature_count'] * 1.1)\n            self.adaptive_params['image_decimation'] = max(1, self.adaptive_params['image_decimation'] // 1.1)\n\n    def get_performance_recommendations(self):\n        \"\"\"Get performance optimization recommendations\"\"\"\n        current_perf = self.get_current_performance()\n        recommendations = []\n\n        if current_perf['fps'] < self.target_fps * 0.8:\n            recommendations.append('Reduce feature count to improve FPS')\n            recommendations.append('Increase image decimation for lower resolution processing')\n\n        if current_perf['cpu_usage'] > 80:\n            recommendations.append('High CPU usage - consider offloading to GPU')\n\n        if current_perf['memory_usage'] > 85:\n            recommendations.append('High memory usage - implement memory management')\n\n        return recommendations\n"})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-and-best-practices",children:"Troubleshooting and Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Isaac ROS troubleshooting guide\nclass IsaacROSTroubleshooter:\n    def __init__(self):\n        self.known_issues = {\n            'connection_timeout': {\n                'symptoms': ['Cannot connect to Isaac Sim', 'Timeout errors'],\n                'causes': ['Network issues', 'Isaac Sim not running', 'Port conflicts'],\n                'solutions': [\n                    'Verify Isaac Sim is running',\n                    'Check network connectivity',\n                    'Ensure correct ports are open',\n                    'Restart Isaac Sim and ROS bridge'\n                ]\n            },\n            'gpu_not_detected': {\n                'symptoms': ['CUDA errors', 'GPU not utilized'],\n                'causes': ['Driver issues', 'CUDA version mismatch', 'GPU not properly configured'],\n                'solutions': [\n                    'Update NVIDIA drivers',\n                    'Verify CUDA installation',\n                    'Check Isaac Sim GPU requirements',\n                    'Install proper Isaac ROS GPU packages'\n                ]\n            },\n            'performance_degradation': {\n                'symptoms': ['Low FPS', 'High latency', 'Memory leaks'],\n                'causes': ['Insufficient hardware', 'Inefficient algorithms', 'Memory management issues'],\n                'solutions': [\n                    'Optimize processing parameters',\n                    'Implement adaptive processing',\n                    'Add performance monitoring',\n                    'Upgrade hardware if needed'\n                ]\n            },\n            'sensor_data_issues': {\n                'symptoms': ['No sensor data', 'Corrupted data', 'Wrong coordinate frames'],\n                'causes': ['Incorrect sensor configuration', 'TF issues', 'Message format problems'],\n                'solutions': [\n                    'Verify sensor configuration in Isaac Sim',\n                    'Check TF tree and transforms',\n                    'Validate message formats',\n                    'Test sensor separately'\n                ]\n            }\n        }\n\n    def diagnose_issue(self, error_message):\n        \"\"\"Diagnose issue based on error message\"\"\"\n        for issue_type, issue_data in self.known_issues.items():\n            for symptom in issue_data['symptoms']:\n                if symptom.lower() in error_message.lower():\n                    return {\n                        'issue_type': issue_type,\n                        'symptoms': issue_data['symptoms'],\n                        'causes': issue_data['causes'],\n                        'solutions': issue_data['solutions']\n                    }\n\n        return {'issue_type': 'unknown', 'solutions': ['Check general troubleshooting steps']}\n\n    def check_system_compatibility(self):\n        \"\"\"Check system compatibility with Isaac ROS requirements\"\"\"\n        import subprocess\n        import platform\n\n        checks = {\n            'os_compatible': self.check_os_compatibility(),\n            'gpu_available': self.check_gpu_availability(),\n            'cuda_installed': self.check_cuda_installation(),\n            'driver_version': self.check_driver_version(),\n            'memory_sufficient': self.check_memory(),\n            'disk_space': self.check_disk_space()\n        }\n\n        return checks\n\n    def check_os_compatibility(self):\n        \"\"\"Check if OS is compatible with Isaac ROS\"\"\"\n        os_name = platform.system().lower()\n        os_version = platform.release()\n\n        # Isaac ROS officially supports Ubuntu 20.04/22.04\n        if os_name == 'linux':\n            try:\n                with open('/etc/os-release', 'r') as f:\n                    os_info = f.read()\n                    if 'ubuntu' in os_info.lower() and ('20.04' in os_info or '22.04' in os_info):\n                        return True\n            except:\n                pass\n\n        return False\n\n    def check_gpu_availability(self):\n        \"\"\"Check if compatible GPU is available\"\"\"\n        try:\n            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'],\n                                  capture_output=True, text=True, timeout=10)\n            if result.returncode == 0 and 'RTX' in result.stdout:\n                return True\n        except:\n            pass\n\n        return False\n\n    def check_cuda_installation(self):\n        \"\"\"Check if CUDA is properly installed\"\"\"\n        try:\n            result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n            return result.returncode == 0\n        except:\n            return False\n\n    def check_driver_version(self):\n        \"\"\"Check if driver version is compatible\"\"\"\n        try:\n            result = subprocess.run(['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader,nounits'],\n                                  capture_output=True, text=True, timeout=5)\n            if result.returncode == 0:\n                version_str = result.stdout.strip()\n                version_parts = version_str.split('.')\n                if len(version_parts) >= 2:\n                    major_version = int(version_parts[0])\n                    # Isaac ROS requires relatively recent drivers\n                    return major_version >= 470\n        except:\n            pass\n\n        return False\n\n    def check_memory(self):\n        \"\"\"Check if system has sufficient memory\"\"\"\n        import psutil\n        memory_gb = psutil.virtual_memory().total / (1024**3)\n        return memory_gb >= 16  # Isaac ROS recommends 16GB+\n\n    def check_disk_space(self):\n        \"\"\"Check if sufficient disk space is available\"\"\"\n        import shutil\n        total, used, free = shutil.disk_usage(\"/\")\n        free_gb = free / (1024**3)\n        return free_gb >= 50  # Recommend at least 50GB free\n\n    def generate_system_report(self):\n        \"\"\"Generate comprehensive system compatibility report\"\"\"\n        checks = self.check_system_compatibility()\n\n        report = f\"\"\"\nIsaac ROS System Compatibility Report\n=====================================\n\nSystem Checks:\n- OS Compatible: {'\u2713' if checks['os_compatible'] else '\u2717'}\n- GPU Available: {'\u2713' if checks['gpu_available'] else '\u2717'}\n- CUDA Installed: {'\u2713' if checks['cuda_installed'] else '\u2717'}\n- Driver Version: {'\u2713' if checks['driver_version'] else '\u2717'}\n- Memory Sufficient: {'\u2713' if checks['memory_sufficient'] else '\u2717'}\n- Disk Space: {'\u2713' if checks['disk_space'] else '\u2717'}\n\nRecommendations:\n\"\"\"\n        if not checks['os_compatible']:\n            report += \"- Upgrade to Ubuntu 20.04 or 22.04\\n\"\n        if not checks['gpu_available']:\n            report += \"- Install NVIDIA RTX GPU\\n\"\n        if not checks['cuda_installed']:\n            report += \"- Install CUDA toolkit\\n\"\n        if not checks['driver_version']:\n            report += \"- Update NVIDIA drivers\\n\"\n        if not checks['memory_sufficient']:\n            report += \"- Upgrade to 16GB+ RAM\\n\"\n        if not checks['disk_space']:\n            report += \"- Free up disk space (need 50GB+)\\n\"\n\n        return report\n"})}),"\n",(0,i.jsx)(n.h2,{id:"practical-lab-complete-isaac-ros-integration",children:"Practical Lab: Complete Isaac ROS Integration"}),"\n",(0,i.jsx)(n.h3,{id:"lab-objective",children:"Lab Objective"}),"\n",(0,i.jsx)(n.p,{children:"Implement a complete Isaac ROS system with image processing, depth analysis, and robot control integration."}),"\n",(0,i.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h4,{id:"step-1-set-up-isaac-sim-environment",children:"Step 1: Set up Isaac Sim Environment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Complete Isaac ROS integration example\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.sensor import Camera\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\n\nclass IsaacROSCompleteIntegration(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_complete_integration\')\n\n        # Initialize Isaac Sim world\n        self.world = World(stage_units_in_meters=1.0)\n\n        # ROS 2 publishers and subscribers\n        self.image_pub = self.create_publisher(Image, \'camera/image_raw\', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, \'camera/camera_info\', 10)\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'cmd_vel\', self.cmd_vel_callback, 10)\n\n        # Isaac Sim components\n        self.camera = None\n        self.robot = None\n\n        # Integration state\n        self.isaac_connected = False\n        self.ros_connected = True\n\n        # Performance monitoring\n        self.frame_count = 0\n        self.last_published_time = self.get_clock().now()\n\n        self.get_logger().info(\'Isaac ROS Complete Integration Node Started\')\n\n    def setup_isaac_environment(self):\n        """Set up Isaac Sim environment with robot and sensors"""\n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n\n        # Add robot\n        self.robot = self.world.scene.add(\n            Robot(\n                prim_path="/World/Robot",\n                name="isaac_robot",\n                usd_path="/Isaac/Robots/TurtleBot3Burger/turtlebot3_burger.usd",\n                position=[0, 0, 0.1],\n                orientation=[0, 0, 0, 1]\n            )\n        )\n\n        # Add camera to robot\n        self.camera = Camera(\n            prim_path="/World/Robot/chassis/camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n        self.world.scene.add(self.camera)\n\n        # Set up lighting\n        from omni.isaac.core.utils.prims import create_prim\n        create_prim(\n            prim_path="/World/Light",\n            prim_type="DistantLight",\n            position=[0, 0, 10],\n            attributes={"color": [0.8, 0.8, 0.8]}\n        )\n\n        self.isaac_connected = True\n        self.get_logger().info(\'Isaac Sim environment set up successfully\')\n\n    def run_simulation(self, steps=1000):\n        """Run Isaac Sim with ROS 2 integration"""\n        self.world.reset()\n\n        for step in range(steps):\n            self.world.step(render=True)\n\n            # Process Isaac Sim data and publish to ROS\n            if self.isaac_connected:\n                self.process_isaac_data()\n\n            # Check for ROS commands\n            rclpy.spin_once(self, timeout_sec=0)\n\n    def process_isaac_data(self):\n        """Process Isaac Sim sensor data and publish to ROS"""\n        try:\n            # Get camera image from Isaac Sim\n            camera_image = self.camera.get_rgba()\n\n            if camera_image is not None:\n                # Convert Isaac image to ROS Image message\n                ros_image = self.isaac_to_ros_image(camera_image)\n\n                # Publish image\n                self.image_pub.publish(ros_image)\n\n                # Publish camera info\n                self.publish_camera_info(ros_image.header)\n\n                # Performance monitoring\n                self.frame_count += 1\n                current_time = self.get_clock().now()\n                if (current_time - self.last_published_time).nanoseconds > 1e9:  # 1 second\n                    fps = self.frame_count / ((current_time - self.last_published_time).nanoseconds / 1e9)\n                    self.get_logger().info(f\'Published {self.frame_count} frames, FPS: {fps:.1f}\')\n                    self.frame_count = 0\n                    self.last_published_time = current_time\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing Isaac data: {e}\')\n\n    def isaac_to_ros_image(self, isaac_image):\n        """Convert Isaac Sim image to ROS Image message"""\n        import numpy as np\n        from cv_bridge import CvBridge\n\n        # Isaac image format may need conversion\n        # This is a simplified example - actual format depends on Isaac Sim version\n        image_data = np.array(isaac_image)\n\n        # Convert to ROS Image using CV Bridge\n        bridge = CvBridge()\n        ros_image = bridge.cv2_to_imgmsg(image_data, encoding=\'rgba8\')\n\n        # Set header\n        ros_image.header.stamp = self.get_clock().now().to_msg()\n        ros_image.header.frame_id = \'camera_rgb_optical_frame\'\n\n        return ros_image\n\n    def publish_camera_info(self, header):\n        """Publish camera calibration information"""\n        camera_info = CameraInfo()\n        camera_info.header = header\n        camera_info.header.frame_id = \'camera_rgb_optical_frame\'\n\n        # Set camera parameters (adjust based on actual Isaac Sim camera)\n        camera_info.width = 640\n        camera_info.height = 480\n        camera_info.distortion_model = \'plumb_bob\'\n        camera_info.d = [0.0, 0.0, 0.0, 0.0, 0.0]  # Distortion coefficients\n        camera_info.k = [525.0, 0.0, 319.5, 0.0, 525.0, 239.5, 0.0, 0.0, 1.0]  # Camera matrix\n        camera_info.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  # Rectification matrix\n        camera_info.p = [525.0, 0.0, 319.5, 0.0, 0.0, 525.0, 239.5, 0.0, 0.0, 0.0, 1.0, 0.0]  # Projection matrix\n\n        self.camera_info_pub.publish(camera_info)\n\n    def cmd_vel_callback(self, msg):\n        """Handle velocity commands from ROS"""\n        if self.robot is not None:\n            # Apply velocity command to Isaac Sim robot\n            # This would involve controlling the robot in Isaac Sim\n            linear_x = msg.linear.x\n            angular_z = msg.angular.z\n\n            # In Isaac Sim, you would apply these velocities to the robot\n            # The exact method depends on the robot model and control interface\n            self.apply_robot_velocity(linear_x, angular_z)\n\n    def apply_robot_velocity(self, linear_x, angular_z):\n        """Apply velocity to Isaac Sim robot"""\n        # This is a placeholder - actual implementation depends on robot model\n        # You would typically use Isaac Sim\'s control interfaces\n        self.get_logger().debug(f\'Applying velocity: linear_x={linear_x}, angular_z={angular_z}\')\n\ndef main(args=None):\n    # Initialize ROS 2\n    rclpy.init(args=args)\n\n    # Initialize Isaac Sim (this would be done in Isaac Sim\'s application)\n    # For this example, we assume Isaac Sim is already running\n\n    # Create integration node\n    integration_node = IsaacROSCompleteIntegration()\n\n    try:\n        # Set up Isaac environment\n        integration_node.setup_isaac_environment()\n\n        # Run simulation\n        integration_node.run_simulation(steps=1000)\n\n    except KeyboardInterrupt:\n        pass\n    finally:\n        integration_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"lab-exercise-isaac-ros-integration",children:"Lab Exercise: Isaac ROS Integration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up Isaac Sim with a mobile robot model"}),"\n",(0,i.jsx)(n.li,{children:"Implement ROS 2 bridge for sensor data (camera, LiDAR, IMU)"}),"\n",(0,i.jsx)(n.li,{children:"Create control interface for robot movement"}),"\n",(0,i.jsx)(n.li,{children:"Test integration with RViz2 visualization"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate performance and optimize parameters"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Working Isaac Sim to ROS 2 integration"}),"\n",(0,i.jsx)(n.li,{children:"Real-time sensor data publishing"}),"\n",(0,i.jsx)(n.li,{children:"Robot control from ROS 2 commands"}),"\n",(0,i.jsx)(n.li,{children:"Proper coordinate frame transformations"}),"\n",(0,i.jsx)(n.li,{children:"Performance within acceptable limits"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Explain the architecture of Isaac ROS and its main components."}),"\n",(0,i.jsx)(n.li,{children:"How do you configure Isaac ROS for optimal performance with GPU acceleration?"}),"\n",(0,i.jsx)(n.li,{children:"What are the key differences between Isaac ROS and traditional ROS packages?"}),"\n",(0,i.jsx)(n.li,{children:"How do you troubleshoot common Isaac ROS integration issues?"}),"\n",(0,i.jsx)(n.li,{children:"What are the best practices for optimizing Isaac ROS pipeline performance?"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"After mastering Isaac ROS integration, students should proceed to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Advanced perception systems with Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"VSLAM implementation for humanoid robots"}),"\n",(0,i.jsx)(n.li,{children:"Navigation systems with Isaac ROS"}),"\n",(0,i.jsx)(n.li,{children:"Sim-to-real transfer techniques"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This comprehensive guide to Isaac ROS integration provides the foundation for creating sophisticated AI-powered robotic systems that leverage both Isaac Sim's photorealistic capabilities and ROS 2's extensive robotics ecosystem."})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var a=s(6540);const i={},t=a.createContext(i);function r(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);