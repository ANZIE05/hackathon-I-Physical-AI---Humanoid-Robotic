"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[2173],{5035:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2/sensor-simulation","title":"Sensor Simulation","description":"Overview","source":"@site/docs/module-2/sensor-simulation.md","sourceDirName":"module-2","slug":"/module-2/sensor-simulation","permalink":"/docs/module-2/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Physics Simulation","permalink":"/docs/module-2/physics-simulation"},"next":{"title":"Gazebo Workflows","permalink":"/docs/module-2/gazebo-workflows"}}');var a=i(4848),r=i(8453);const o={sidebar_position:3},t="Sensor Simulation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Sensor Physics and Modeling",id:"sensor-physics-and-modeling",level:3},{value:"Sensor Categories",id:"sensor-categories",level:3},{value:"Realism vs. Performance Trade-offs",id:"realism-vs-performance-trade-offs",level:3},{value:"Vision Sensor Simulation",id:"vision-sensor-simulation",level:2},{value:"Camera Configuration",id:"camera-configuration",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"Stereo Camera Configuration",id:"stereo-camera-configuration",level:3},{value:"Range Sensor Simulation",id:"range-sensor-simulation",level:2},{value:"LiDAR Configuration",id:"lidar-configuration",level:3},{value:"2D LiDAR Configuration",id:"2d-lidar-configuration",level:3},{value:"Sonar/IR Sensor Configuration",id:"sonarir-sensor-configuration",level:3},{value:"Inertial Sensor Simulation",id:"inertial-sensor-simulation",level:2},{value:"IMU Configuration",id:"imu-configuration",level:3},{value:"GPS Sensor Configuration",id:"gps-sensor-configuration",level:3},{value:"Advanced Sensor Features",id:"advanced-sensor-features",level:2},{value:"Custom Sensor Plugins",id:"custom-sensor-plugins",level:3},{value:"Sensor Fusion Simulation",id:"sensor-fusion-simulation",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Sensor Message Types",id:"sensor-message-types",level:3},{value:"Sensor Calibration",id:"sensor-calibration",level:3},{value:"Noise Modeling and Realism",id:"noise-modeling-and-realism",level:2},{value:"Noise Types and Parameters",id:"noise-types-and-parameters",level:3},{value:"Environmental Effects",id:"environmental-effects",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Sensor Update Rates",id:"sensor-update-rates",level:3},{value:"Sensor Filtering",id:"sensor-filtering",level:3},{value:"Validation and Calibration",id:"validation-and-calibration",level:2},{value:"Sensor Model Validation",id:"sensor-model-validation",level:3},{value:"Calibration Procedures",id:"calibration-procedures",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Sensor Data Problems",id:"sensor-data-problems",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"Practical Lab: Multi-Sensor Integration",id:"practical-lab-multi-sensor-integration",level:2},{value:"Lab Objective",id:"lab-objective",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Expected Outcome",id:"expected-outcome",level:3},{value:"Review Questions",id:"review-questions",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"sensor-simulation",children:"Sensor Simulation"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation is a critical component of realistic robotic simulation, providing synthetic sensor data that closely matches real-world sensors. This section covers the simulation of various sensor types, configuration options, and integration with ROS 2 for Physical AI applications."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this section, students will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Configure and simulate various sensor types in Gazebo"}),"\n",(0,a.jsx)(n.li,{children:"Understand the physics behind sensor simulation"}),"\n",(0,a.jsx)(n.li,{children:"Calibrate simulated sensors to match real-world characteristics"}),"\n",(0,a.jsx)(n.li,{children:"Integrate sensor data with ROS 2 communication"}),"\n",(0,a.jsx)(n.li,{children:"Validate sensor simulation accuracy and performance"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-physics-and-modeling",children:"Sensor Physics and Modeling"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ray Tracing"}),": Simulation of light/radio wave propagation for cameras and LiDAR"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise Modeling"}),": Addition of realistic noise to sensor data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Distortion"}),": Modeling of sensor-specific distortions (lens, range, etc.)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temporal Effects"}),": Latency, update rates, and synchronization"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-categories",children:"Sensor Categories"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Vision Sensors"}),": Cameras, depth cameras, stereo cameras"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range Sensors"}),": LiDAR, sonar, infrared, ToF sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Inertial Sensors"}),": IMU, gyroscope, accelerometer"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Other Sensors"}),": GPS, magnetometer, force/torque, contact sensors"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"realism-vs-performance-trade-offs",children:"Realism vs. Performance Trade-offs"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Accuracy"}),": How closely simulation matches real sensor behavior"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance"}),": Computational cost of sensor simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Calibration"}),": Parameters to match real sensor characteristics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validation"}),": Testing against real sensor data"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"vision-sensor-simulation",children:"Vision Sensor Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"camera-configuration",children:"Camera Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="camera" type="camera">\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>\n  <camera name="head_camera">\n    \x3c!-- Camera intrinsic parameters --\x3e\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees in radians --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>    \x3c!-- Near clipping plane --\x3e\n      <far>100</far>      \x3c!-- Far clipping plane --\x3e\n    </clip>\n\n    \x3c!-- Lens distortion --\x3e\n    <distortion>\n      <k1>0.0</k1>       \x3c!-- Radial distortion coefficient --\x3e\n      <k2>0.0</k2>\n      <k3>0.0</k3>\n      <p1>0.0</p1>       \x3c!-- Tangential distortion coefficient --\x3e\n      <p2>0.0</p2>\n      <center>0.5 0.5</center>  \x3c!-- Principal point (normalized) --\x3e\n    </distortion>\n  </camera>\n\n  \x3c!-- Noise model --\x3e\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.007</stddev>\n  </noise>\n\n  <visualize>true</visualize>\n  <topic>camera/image_raw</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>\n  <camera name="depth_head">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>320</width>\n      <height>240</height>\n      <format>R16G16B16</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n\n  \x3c!-- Noise for depth measurements --\x3e\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\n  </noise>\n\n  <visualize>true</visualize>\n  \x3c!-- Multiple topics for different data types --\x3e\n  <topic>camera/depth/image_raw</topic>\n  <point_cloud_topic>camera/depth/points</point_cloud_topic>\n  <camera_info_topic>camera/camera_info</camera_info_topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"stereo-camera-configuration",children:"Stereo Camera Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="stereo_camera" type="multicamera">\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>\n\n  \x3c!-- Left camera --\x3e\n  <camera name="left">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n\n  \x3c!-- Right camera (offset from left) --\x3e\n  <camera name="right">\n    <pose>0.2 0 0 0 0 0</pose>  \x3c!-- Baseline: 20cm --\x3e\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n\n  <visualize>true</visualize>\n  <topic>stereo_camera/left/image_raw</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"range-sensor-simulation",children:"Range Sensor Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-configuration",children:"LiDAR Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="3d_lidar" type="ray">\n  <always_on>1</always_on>\n  <update_rate>10</update_rate>\n  <ray>\n    \x3c!-- Horizontal scan pattern --\x3e\n    <scan>\n      <horizontal>\n        <samples>640</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>  \x3c!-- -180 degrees --\x3e\n        <max_angle>3.14159</max_angle>   \x3c!-- 180 degrees --\x3e\n      </horizontal>\n      \x3c!-- Vertical scan pattern (for 3D LiDAR) --\x3e\n      <vertical>\n        <samples>16</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.2618</min_angle>  \x3c!-- -15 degrees --\x3e\n        <max_angle>0.2618</max_angle>   \x3c!-- 15 degrees --\x3e\n      </vertical>\n    </scan>\n\n    \x3c!-- Range properties --\x3e\n    <range>\n      <min>0.1</min>\n      <max>30</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n\n  \x3c!-- Noise model for range measurements --\x3e\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\n  </noise>\n\n  <visualize>true</visualize>\n  <topic>laser_scan</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2d-lidar-configuration",children:"2D LiDAR Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="2d_lidar" type="ray">\n  <always_on>1</always_on>\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>  \x3c!-- 0.5 degree resolution --\x3e\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>  \x3c!-- -180 degrees --\x3e\n        <max_angle>3.14159</max_angle>   \x3c!-- 180 degrees --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>20</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n\n  \x3c!-- Add intensity information --\x3e\n  <always_on>1</always_on>\n  <visualize>true</visualize>\n  <topic>scan</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sonarir-sensor-configuration",children:"Sonar/IR Sensor Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="sonar_sensor" type="ray">\n  <always_on>1</always_on>\n  <update_rate>20</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.1745</min_angle>  \x3c!-- -10 degrees --\x3e\n        <max_angle>0.1745</max_angle>   \x3c!-- 10 degrees --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.05</min>    \x3c!-- 5cm minimum --\x3e\n      <max>4</max>       \x3c!-- 4m maximum --\x3e\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n\n  \x3c!-- Higher noise for sonar sensors --\x3e\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.05</stddev>  \x3c!-- 5cm standard deviation --\x3e\n  </noise>\n\n  <visualize>false</visualize>\n  <topic>sonar_range</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"inertial-sensor-simulation",children:"Inertial Sensor Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"imu-configuration",children:"IMU Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>1</always_on>\n  <update_rate>100</update_rate>\n  <imu>\n    \x3c!-- Noise parameters for each measurement type --\x3e\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>  \x3c!-- 0.01 rad/s standard deviation --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.1</stddev>  \x3c!-- 0.1 m/s\xb2 standard deviation --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.1</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.1</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n\n  <visualize>false</visualize>\n  <topic>imu/data</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"gps-sensor-configuration",children:"GPS Sensor Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="gps_sensor" type="gps">\n  <always_on>1</always_on>\n  <update_rate>1</update_rate>\n  <gps>\n    \x3c!-- Noise characteristics --\x3e\n    <noise>\n      <position>\n        <horizontal>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2.0</stddev>  \x3c!-- 2m horizontal accuracy --\x3e\n          </noise>\n        </horizontal>\n        <vertical>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>4.0</stddev>  \x3c!-- 4m vertical accuracy --\x3e\n          </noise>\n        </vertical>\n      </position>\n    </noise>\n  </gps>\n\n  <visualize>false</visualize>\n  <topic>gps/fix</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"advanced-sensor-features",children:"Advanced Sensor Features"}),"\n",(0,a.jsx)(n.h3,{id:"custom-sensor-plugins",children:"Custom Sensor Plugins"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Custom sensor for specialized applications --\x3e\n<sensor name="custom_sensor" type="custom_type">\n  <plugin name="custom_sensor_plugin" filename="libCustomSensorPlugin.so">\n    <sensor_type>force_torque</sensor_type>\n    <update_rate>100</update_rate>\n    <topic>custom_sensor/data</topic>\n    <noise_stddev>0.01</noise_stddev>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sensor-fusion-simulation",children:"Sensor Fusion Simulation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Simulate fused sensor data --\x3e\n<sensor name="sensor_fusion" type="sensor_fusion">\n  <always_on>1</always_on>\n  <update_rate>50</update_rate>\n\n  \x3c!-- Simulated fusion of multiple inputs --\x3e\n  <fusion>\n    <input_topic>imu/data</input_topic>\n    <input_topic>gps/fix</input_topic>\n    <input_topic>odometry/filtered</input_topic>\n  </fusion>\n\n  <topic>state_estimator/pose</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-message-types",children:"Sensor Message Types"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example of processing sensor data in ROS 2\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu, NavSatFix\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass SensorProcessor(Node):\n    def __init__(self):\n        super().__init__(\'sensor_processor\')\n\n        self.bridge = CvBridge()\n\n        # Subscribers for different sensor types\n        self.camera_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.camera_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'scan\', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, \'imu/data\', self.imu_callback, 10)\n        self.gps_sub = self.create_subscription(\n            NavSatFix, \'gps/fix\', self.gps_callback, 10)\n\n    def camera_callback(self, msg):\n        """Process camera image data"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            # Process image (e.g., object detection, feature extraction)\n            self.process_image(cv_image)\n        except Exception as e:\n            self.get_logger().error(f\'Camera processing error: {e}\')\n\n    def scan_callback(self, msg):\n        """Process LiDAR scan data"""\n        # Convert to numpy array for processing\n        ranges = np.array(msg.ranges)\n\n        # Filter out invalid ranges\n        valid_ranges = ranges[(ranges > msg.range_min) & (ranges < msg.range_max)]\n\n        # Process scan data (e.g., obstacle detection, mapping)\n        obstacles = self.detect_obstacles(valid_ranges, msg.angle_min, msg.angle_increment)\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        # Extract orientation, angular velocity, linear acceleration\n        orientation = [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\n        angular_vel = [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z]\n        linear_acc = [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]\n\n        # Process IMU data (e.g., attitude estimation, motion detection)\n        self.process_imu(orientation, angular_vel, linear_acc)\n\n    def gps_callback(self, msg):\n        """Process GPS data"""\n        latitude = msg.latitude\n        longitude = msg.longitude\n        altitude = msg.altitude\n\n        # Process GPS data (e.g., localization, navigation)\n        self.process_gps(latitude, longitude, altitude)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sensor-calibration",children:"Sensor Calibration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Calibration node for sensor parameters\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import CameraInfo\nfrom std_msgs.msg import Float64\n\nclass SensorCalibrator(Node):\n    def __init__(self):\n        super().__init__('sensor_calibrator')\n\n        # Camera calibration publisher\n        self.camera_info_pub = self.create_publisher(CameraInfo, 'camera/camera_info', 10)\n\n        # Timer for publishing calibration data\n        self.calibration_timer = self.create_timer(1.0, self.publish_calibration)\n\n        # Calibration parameters\n        self.camera_matrix = [\n            525.0, 0.0, 319.5,  # fx, 0, cx\n            0.0, 525.0, 239.5,  # 0, fy, cy\n            0.0, 0.0, 1.0       # 0, 0, 1\n        ]\n\n        self.distortion_coeffs = [0.0, 0.0, 0.0, 0.0, 0.0]  # k1, k2, p1, p2, k3\n\n    def publish_calibration(self):\n        \"\"\"Publish camera calibration information\"\"\"\n        camera_info = CameraInfo()\n        camera_info.header.stamp = self.get_clock().now().to_msg()\n        camera_info.header.frame_id = 'camera_link'\n\n        camera_info.height = 480\n        camera_info.width = 640\n        camera_info.distortion_model = 'plumb_bob'\n        camera_info.d = self.distortion_coeffs\n        camera_info.k = self.camera_matrix\n        camera_info.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n        camera_info.p = self.camera_matrix + [0.0, 0.0, 0.0, 0.0]  # 4x4 projection matrix\n\n        self.camera_info_pub.publish(camera_info)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"noise-modeling-and-realism",children:"Noise Modeling and Realism"}),"\n",(0,a.jsx)(n.h3,{id:"noise-types-and-parameters",children:"Noise Types and Parameters"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Different noise models for various sensors --\x3e\n<sensor name="realistic_camera" type="camera">\n  <camera name="head">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n\n  \x3c!-- Realistic noise model --\x3e\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>\n    \x3c!-- Additional parameters for more realistic noise --\x3e\n    <bias_mean>0.001</bias_mean>\n    <bias_stddev>0.0005</bias_stddev>\n  </noise>\n\n  <visualize>true</visualize>\n  <topic>camera/image_raw</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"environmental-effects",children:"Environmental Effects"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Simulate environmental effects on sensors --\x3e\n<sensor name="weather_affected_sensor" type="camera">\n  <camera name="weather_camera">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>50</far>  \x3c!-- Reduced visibility in bad weather --\x3e\n    </clip>\n  </camera>\n\n  \x3c!-- Additional weather effects --\x3e\n  <always_on>1</always_on>\n  <visualize>true</visualize>\n  <topic>weather_camera/image_raw</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-update-rates",children:"Sensor Update Rates"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Optimize update rates for different sensor types --\x3e\n<sdf version="1.7">\n  <model name="sensor_optimized_robot">\n    \x3c!-- High-rate sensors (100Hz) --\x3e\n    <sensor name="imu" type="imu">\n      <update_rate>100</update_rate>\n      \x3c!-- ... other IMU config --\x3e\n    </sensor>\n\n    \x3c!-- Medium-rate sensors (30Hz) --\x3e\n    <sensor name="camera" type="camera">\n      <update_rate>30</update_rate>\n      \x3c!-- ... other camera config --\x3e\n    </sensor>\n\n    \x3c!-- Low-rate sensors (10Hz) --\x3e\n    <sensor name="gps" type="gps">\n      <update_rate>10</update_rate>\n      \x3c!-- ... other GPS config --\x3e\n    </sensor>\n  </model>\n</sdf>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sensor-filtering",children:"Sensor Filtering"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Simulate hardware-level filtering --\x3e\n<sensor name="filtered_lidar" type="ray">\n  <ray>\n    \x3c!-- Reduce effective resolution to simulate hardware filtering --\x3e\n    <scan>\n      <horizontal>\n        <samples>360</samples>  \x3c!-- Reduced from 720 for 1-degree resolution --\x3e\n        <resolution>2</resolution>  \x3c!-- Effectively 2-degree resolution --\x3e\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>20</max>\n      <resolution>0.02</resolution>  \x3c!-- Reduced precision --\x3e\n    </range>\n  </ray>\n\n  <always_on>1</always_on>\n  <visualize>false</visualize>\n  <topic>filtered_scan</topic>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"validation-and-calibration",children:"Validation and Calibration"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-model-validation",children:"Sensor Model Validation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Accuracy Testing"}),": Compare simulated vs. real sensor data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise Characterization"}),": Validate noise models match real sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Timing Analysis"}),": Ensure proper update rates and latencies"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Edge Case Testing"}),": Test in challenging conditions"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"calibration-procedures",children:"Calibration Procedures"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intrinsic Calibration"}),": Internal sensor parameters (focal length, distortion)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Extrinsic Calibration"}),": Sensor position/orientation relative to robot"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temporal Calibration"}),": Synchronization between different sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environmental Calibration"}),": Adaptation to different conditions"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-data-problems",children:"Sensor Data Problems"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"No Data"}),": Check sensor plugin loading, topic names, and permissions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Incorrect Data"}),": Verify sensor configuration, coordinate frames, and units"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Timing Issues"}),": Check update rates, system load, and real-time performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise Problems"}),": Validate noise parameters and random number generation"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Slow Simulation"}),": Reduce sensor update rates or simplify sensor models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High CPU Usage"}),": Optimize sensor processing and reduce unnecessary visualization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Issues"}),": Monitor sensor data buffering and processing pipelines"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practical-lab-multi-sensor-integration",children:"Practical Lab: Multi-Sensor Integration"}),"\n",(0,a.jsx)(n.h3,{id:"lab-objective",children:"Lab Objective"}),"\n",(0,a.jsx)(n.p,{children:"Create a robot model with multiple sensor types (camera, LiDAR, IMU) and implement sensor fusion for state estimation."}),"\n",(0,a.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Create a robot model with multiple sensors"}),"\n",(0,a.jsx)(n.li,{children:"Configure sensors with realistic parameters"}),"\n",(0,a.jsx)(n.li,{children:"Implement ROS 2 nodes for sensor data processing"}),"\n",(0,a.jsx)(n.li,{children:"Create a simple sensor fusion algorithm"}),"\n",(0,a.jsx)(n.li,{children:"Validate the integrated system"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Working multi-sensor robot model"}),"\n",(0,a.jsx)(n.li,{children:"Proper ROS 2 integration for all sensors"}),"\n",(0,a.jsx)(n.li,{children:"Basic sensor fusion implementation"}),"\n",(0,a.jsx)(n.li,{children:"Demonstrated understanding of sensor simulation"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"How do you configure a camera sensor in Gazebo and what parameters affect image quality?"}),"\n",(0,a.jsx)(n.li,{children:"Explain the differences between 2D and 3D LiDAR simulation in Gazebo."}),"\n",(0,a.jsx)(n.li,{children:"What are the key considerations for IMU sensor simulation and calibration?"}),"\n",(0,a.jsx)(n.li,{children:"How do you implement sensor noise models to match real-world characteristics?"}),"\n",(0,a.jsx)(n.li,{children:"Describe the process for validating simulated sensor data against real sensors."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"After mastering sensor simulation, students should proceed to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Gazebo workflows and simulation environments"}),"\n",(0,a.jsx)(n.li,{children:"Unity visualization for robotics"}),"\n",(0,a.jsx)(n.li,{children:"Advanced perception algorithms"}),"\n",(0,a.jsx)(n.li,{children:"Sim-to-real transfer techniques"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This comprehensive understanding of sensor simulation enables the creation of realistic robotic perception systems essential for Physical AI and Humanoid Robotics applications."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const a={},r=s.createContext(a);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);