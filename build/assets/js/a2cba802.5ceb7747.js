"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[5380],{1027:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-1/ros2-practical-labs","title":"ROS 2 Practical Labs","description":"Overview","source":"@site/docs/module-1/ros2-practical-labs.md","sourceDirName":"module-1","slug":"/module-1/ros2-practical-labs","permalink":"/docs/module-1/ros2-practical-labs","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1/ros2-practical-labs.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Python Integration (rclpy)","permalink":"/docs/module-1/rclpy-integration"},"next":{"title":"ROS 2 Review Questions","permalink":"/docs/module-1/ros2-review-questions"}}');var i=s(4848),a=s(8453);const r={sidebar_position:5},o="ROS 2 Practical Labs",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Lab 1: Basic ROS 2 Communication",id:"lab-1-basic-ros-2-communication",level:2},{value:"Objective",id:"objective",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Lab Setup",id:"lab-setup",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Step 1: Create the Sensor Publisher",id:"step-1-create-the-sensor-publisher",level:4},{value:"Step 2: Create the Command Subscriber",id:"step-2-create-the-command-subscriber",level:4},{value:"Step 3: Update Package Configuration",id:"step-3-update-package-configuration",level:4},{value:"Step 4: Build and Test",id:"step-4-build-and-test",level:4},{value:"Lab Exercises",id:"lab-exercises",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Lab 2: Advanced ROS 2 with URDF and Simulation",id:"lab-2-advanced-ros-2-with-urdf-and-simulation",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Prerequisites",id:"prerequisites-1",level:3},{value:"Lab Setup",id:"lab-setup-1",level:3},{value:"Implementation Steps",id:"implementation-steps-1",level:3},{value:"Step 1: Create Robot URDF Model",id:"step-1-create-robot-urdf-model",level:4},{value:"Step 2: Create Navigation Node",id:"step-2-create-navigation-node",level:4},{value:"Step 3: Create Launch File",id:"step-3-create-launch-file",level:4},{value:"Step 4: Build and Test",id:"step-4-build-and-test-1",level:4},{value:"Lab Exercises",id:"lab-exercises-1",level:3},{value:"Expected Results",id:"expected-results-1",level:3},{value:"Lab 3: AI Integration with ROS 2",id:"lab-3-ai-integration-with-ros-2",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Prerequisites",id:"prerequisites-2",level:3},{value:"Lab Setup",id:"lab-setup-2",level:3},{value:"Implementation Steps",id:"implementation-steps-2",level:3},{value:"Step 1: Create AI Processing Node",id:"step-1-create-ai-processing-node",level:4},{value:"Step 2: Update Package Configuration",id:"step-2-update-package-configuration",level:4},{value:"Step 3: Build and Test",id:"step-3-build-and-test",level:4},{value:"Lab Exercises",id:"lab-exercises-2",level:3},{value:"Expected Results",id:"expected-results-2",level:3},{value:"Lab Report Template",id:"lab-report-template",level:2},{value:"Lab Documentation Requirements",id:"lab-documentation-requirements",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"ROS 2 Communication Issues",id:"ros-2-communication-issues",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"Simulation Issues",id:"simulation-issues",level:3},{value:"Review Questions",id:"review-questions",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"ros-2-practical-labs",children:"ROS 2 Practical Labs"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This section provides hands-on practical labs that reinforce the ROS 2 concepts learned in previous sections. Each lab builds upon the previous knowledge and provides real-world application of ROS 2 principles in Physical AI and Humanoid Robotics contexts."}),"\n",(0,i.jsx)(n.h2,{id:"lab-1-basic-ros-2-communication",children:"Lab 1: Basic ROS 2 Communication"}),"\n",(0,i.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Create a simple robot communication system with sensor publishing and command subscription to understand basic ROS 2 concepts."}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble Hawksbill installed"}),"\n",(0,i.jsx)(n.li,{children:"Basic Python knowledge"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of ROS 2 fundamentals"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create workspace\nmkdir -p ~/ros2_labs/ws_basic_communication/src\ncd ~/ros2_labs/ws_basic_communication\n\n# Create package\nros2 pkg create --build-type ament_python basic_communication_pkg --dependencies rclpy std_msgs sensor_msgs geometry_msgs\n"})}),"\n",(0,i.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h4,{id:"step-1-create-the-sensor-publisher",children:"Step 1: Create the Sensor Publisher"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"basic_communication_pkg/basic_communication_pkg/sensor_publisher.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom std_msgs.msg import String\nimport math\nimport random\n\nclass SensorPublisher(Node):\n    def __init__(self):\n        super().__init__('sensor_publisher')\n\n        # Create publisher for laser scan\n        self.laser_publisher = self.create_publisher(LaserScan, 'laser_scan', 10)\n\n        # Create publisher for status\n        self.status_publisher = self.create_publisher(String, 'robot_status', 10)\n\n        # Create timer for publishing\n        self.timer = self.create_timer(0.5, self.publish_sensor_data)\n\n        self.scan_count = 0\n        self.get_logger().info('Sensor Publisher Node Started')\n\n    def publish_sensor_data(self):\n        \"\"\"Publish simulated sensor data\"\"\"\n        # Create and populate laser scan message\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\n        scan_msg.header.frame_id = 'laser_frame'\n\n        # Set laser scan parameters\n        scan_msg.angle_min = -math.pi / 2  # -90 degrees\n        scan_msg.angle_max = math.pi / 2   # 90 degrees\n        scan_msg.angle_increment = math.pi / 180  # 1 degree\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 10.0\n\n        # Generate simulated ranges (add some randomness to simulate real sensors)\n        num_ranges = int((scan_msg.angle_max - scan_msg.angle_min) / scan_msg.angle_increment) + 1\n        ranges = []\n\n        for i in range(num_ranges):\n            angle = scan_msg.angle_min + i * scan_msg.angle_increment\n            # Simulate a wall at 2 meters in front with some noise\n            distance = 2.0 + random.uniform(-0.1, 0.1)\n            ranges.append(distance)\n\n        scan_msg.ranges = ranges\n        scan_msg.intensities = []  # No intensity data for this example\n\n        # Publish laser scan\n        self.laser_publisher.publish(scan_msg)\n\n        # Publish status message\n        status_msg = String()\n        status_msg.data = f'Scanning environment - count: {self.scan_count}'\n        self.status_publisher.publish(status_msg)\n\n        self.scan_count += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_publisher = SensorPublisher()\n\n    try:\n        rclpy.spin(sensor_publisher)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_publisher.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h4,{id:"step-2-create-the-command-subscriber",children:"Step 2: Create the Command Subscriber"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"basic_communication_pkg/basic_communication_pkg/command_subscriber.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport math\n\nclass CommandSubscriber(Node):\n    def __init__(self):\n        super().__init__('command_subscriber')\n\n        # Create subscriber for velocity commands\n        self.cmd_vel_subscription = self.create_subscription(\n            Twist, 'cmd_vel', self.cmd_vel_callback, 10)\n\n        # Create subscriber for status messages\n        self.status_subscription = self.create_subscription(\n            String, 'robot_status', self.status_callback, 10)\n\n        # Create publisher for processed commands\n        self.processed_cmd_publisher = self.create_publisher(Twist, 'processed_cmd_vel', 10)\n\n        # Create publisher for robot state\n        self.state_publisher = self.create_publisher(String, 'robot_state', 10)\n\n        # Robot state variables\n        self.linear_velocity = 0.0\n        self.angular_velocity = 0.0\n        self.robot_state = 'idle'\n\n        self.get_logger().info('Command Subscriber Node Started')\n\n    def cmd_vel_callback(self, msg):\n        \"\"\"Handle incoming velocity commands\"\"\"\n        self.linear_velocity = msg.linear.x\n        self.angular_velocity = msg.angular.z\n\n        # Process and validate commands\n        processed_cmd = self.process_command(msg)\n\n        # Publish processed command\n        self.processed_cmd_publisher.publish(processed_cmd)\n\n        # Update robot state\n        if abs(self.linear_velocity) > 0.01 or abs(self.angular_velocity) > 0.01:\n            self.robot_state = 'moving'\n        else:\n            self.robot_state = 'stopped'\n\n        # Publish robot state\n        state_msg = String()\n        state_msg.data = f'{self.robot_state} - linear: {self.linear_velocity:.2f}, angular: {self.angular_velocity:.2f}'\n        self.state_publisher.publish(state_msg)\n\n        self.get_logger().info(f'Received command: linear={self.linear_velocity:.2f}, angular={self.angular_velocity:.2f}')\n\n    def status_callback(self, msg):\n        \"\"\"Handle status messages from other nodes\"\"\"\n        self.get_logger().info(f'Robot status: {msg.data}')\n\n    def process_command(self, cmd_msg):\n        \"\"\"Process and validate incoming commands\"\"\"\n        processed_cmd = Twist()\n\n        # Apply safety limits\n        MAX_LINEAR = 1.0  # m/s\n        MAX_ANGULAR = 1.0  # rad/s\n\n        processed_cmd.linear.x = max(-MAX_LINEAR, min(MAX_LINEAR, cmd_msg.linear.x))\n        processed_cmd.angular.z = max(-MAX_ANGULAR, min(MAX_ANGULAR, cmd_msg.angular.z))\n\n        # Apply smoothing (simple low-pass filter)\n        alpha = 0.1  # Smoothing factor\n        processed_cmd.linear.x = alpha * processed_cmd.linear.x + (1 - alpha) * self.linear_velocity\n        processed_cmd.angular.z = alpha * processed_cmd.angular.z + (1 - alpha) * self.angular_velocity\n\n        return processed_cmd\n\ndef main(args=None):\n    rclpy.init(args=args)\n    command_subscriber = CommandSubscriber()\n\n    try:\n        rclpy.spin(command_subscriber)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        command_subscriber.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h4,{id:"step-3-update-package-configuration",children:"Step 3: Update Package Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["Update ",(0,i.jsx)(n.code,{children:"basic_communication_pkg/package.xml"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>basic_communication_pkg</name>\n  <version>0.0.0</version>\n  <description>Basic ROS 2 communication package for lab exercises</description>\n  <maintainer email="student@university.edu">Student</maintainer>\n  <license>Apache-2.0</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Update ",(0,i.jsx)(n.code,{children:"basic_communication_pkg/setup.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\n\npackage_name = 'basic_communication_pkg'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Student',\n    maintainer_email='student@university.edu',\n    description='Basic ROS 2 communication package for lab exercises',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'sensor_publisher = basic_communication_pkg.sensor_publisher:main',\n            'command_subscriber = basic_communication_pkg.command_subscriber:main',\n        ],\n    },\n)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"step-4-build-and-test",children:"Step 4: Build and Test"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Build the package\ncd ~/ros2_labs/ws_basic_communication\ncolcon build --packages-select basic_communication_pkg\n\n# Source the workspace\nsource install/setup.bash\n\n# Run the publisher in one terminal\nros2 run basic_communication_pkg sensor_publisher\n\n# In another terminal, run the subscriber\nros2 run basic_communication_pkg command_subscriber\n\n# In a third terminal, send commands\nros2 topic pub /cmd_vel geometry_msgs/Twist '{linear: {x: 0.5}, angular: {z: 0.2}}'\n"})}),"\n",(0,i.jsx)(n.h3,{id:"lab-exercises",children:"Lab Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Modify the sensor publisher to simulate different environments (open space, narrow corridor)"}),"\n",(0,i.jsx)(n.li,{children:"Add more sophisticated command processing in the subscriber"}),"\n",(0,i.jsx)(n.li,{children:"Implement a simple state machine to track robot behavior"}),"\n",(0,i.jsx)(n.li,{children:"Add parameter configuration for the nodes"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Two nodes communicating via ROS 2 topics"}),"\n",(0,i.jsx)(n.li,{children:"Proper logging and error handling"}),"\n",(0,i.jsx)(n.li,{children:"Working command processing with safety limits"}),"\n",(0,i.jsx)(n.li,{children:"Demonstrated understanding of basic ROS 2 concepts"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lab-2-advanced-ros-2-with-urdf-and-simulation",children:"Lab 2: Advanced ROS 2 with URDF and Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Create a simulated robot model with URDF, integrate it with ROS 2, and implement basic navigation behaviors."}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites-1",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Lab 1 completed"}),"\n",(0,i.jsx)(n.li,{children:"Gazebo installed"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of URDF basics"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-setup-1",children:"Lab Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/ros2_labs/ws_robot_model/src\ncd ~/ros2_labs/ws_robot_model\n\n# Create package\nros2 pkg create --build-type ament_python robot_model_pkg --dependencies rclpy std_msgs sensor_msgs geometry_msgs nav_msgs tf2_ros\n"})}),"\n",(0,i.jsx)(n.h3,{id:"implementation-steps-1",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h4,{id:"step-1-create-robot-urdf-model",children:"Step 1: Create Robot URDF Model"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"robot_model_pkg/robot_model_pkg/urdf/simple_robot.urdf"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="simple_robot" xmlns:xacro="http://www.ros.org/wiki/xacro">\n  \x3c!-- Properties --\x3e\n  <xacro:property name="M_PI" value="3.1415926535897931" />\n  <xacro:property name="wheel_radius" value="0.1" />\n  <xacro:property name="wheel_width" value="0.05" />\n  <xacro:property name="base_length" value="0.5" />\n  <xacro:property name="base_width" value="0.3" />\n  <xacro:property name="base_height" value="0.15" />\n  <xacro:property name="wheel_pos_x" value="0.2" />\n  <xacro:property name="wheel_pos_y" value="0.2" />\n\n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="${base_length} ${base_width} ${base_height}"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 0.8 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="${base_length} ${base_width} ${base_height}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="5"/>\n      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Left Wheel --\x3e\n  <joint name="left_wheel_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="left_wheel"/>\n    <origin xyz="${wheel_pos_x} ${wheel_pos_y} 0" rpy="${-M_PI/2} 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="left_wheel">\n    <visual>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.5"/>\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.002"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Right Wheel --\x3e\n  <joint name="right_wheel_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="right_wheel"/>\n    <origin xyz="${wheel_pos_x} ${-wheel_pos_y} 0" rpy="${-M_PI/2} 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="right_wheel">\n    <visual>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.5"/>\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.002"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Camera --\x3e\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="${base_length/2} 0 ${base_height/2}" rpy="0 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="0.8 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n    </inertial>\n  </link>\n\n</robot>\n'})}),"\n",(0,i.jsx)(n.h4,{id:"step-2-create-navigation-node",children:"Step 2: Create Navigation Node"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"robot_model_pkg/robot_model_pkg/navigation_node.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, Vector3\nfrom sensor_msgs.msg import LaserScan\nfrom nav_msgs.msg import Odometry\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nimport math\nimport numpy as np\n\nclass NavigationNode(Node):\n    def __init__(self):\n        super().__init__('navigation_node')\n\n        # Publishers\n        self.cmd_vel_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n\n        # Subscribers\n        self.scan_subscription = self.create_subscription(\n            LaserScan, 'scan', self.scan_callback, 10)\n        self.odom_subscription = self.create_subscription(\n            Odometry, 'odom', self.odom_callback, 10)\n\n        # Timers\n        self.navigation_timer = self.create_timer(0.1, self.navigation_loop)\n\n        # Robot state\n        self.latest_scan = None\n        self.robot_pose = {'x': 0.0, 'y': 0.0, 'theta': 0.0}\n        self.target_pose = {'x': 5.0, 'y': 5.0}  # Target location\n\n        # Navigation state\n        self.navigation_state = 'exploring'  # exploring, navigating, stopped\n        self.obstacle_detected = False\n        self.safety_distance = 0.5  # meters\n\n        self.get_logger().info('Navigation Node Started')\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan data\"\"\"\n        self.latest_scan = msg\n\n        # Check for obstacles in front of robot\n        if len(msg.ranges) > 0:\n            # Get front-facing ranges (\xb130 degrees)\n            front_start = len(msg.ranges) // 2 - 15\n            front_end = len(msg.ranges) // 2 + 15\n\n            if front_start >= 0 and front_end < len(msg.ranges):\n                front_ranges = msg.ranges[front_start:front_end]\n                valid_ranges = [r for r in front_ranges if 0 < r < float('inf')]\n\n                if valid_ranges:\n                    min_range = min(valid_ranges)\n                    self.obstacle_detected = min_range < self.safety_distance\n                else:\n                    self.obstacle_detected = False\n            else:\n                self.obstacle_detected = False\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot pose from odometry\"\"\"\n        self.robot_pose['x'] = msg.pose.pose.position.x\n        self.robot_pose['y'] = msg.pose.pose.position.y\n\n        # Convert quaternion to euler\n        orientation = msg.pose.pose.orientation\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        theta = math.atan2(siny_cosp, cosy_cosp)\n        self.robot_pose['theta'] = theta\n\n    def navigation_loop(self):\n        \"\"\"Main navigation logic\"\"\"\n        if self.latest_scan is None:\n            return\n\n        cmd = Twist()\n\n        if self.obstacle_detected:\n            # Emergency stop or obstacle avoidance\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.5  # Turn right to avoid obstacle\n            self.navigation_state = 'avoiding'\n        else:\n            # Navigate towards target\n            dx = self.target_pose['x'] - self.robot_pose['x']\n            dy = self.target_pose['y'] - self.robot_pose['y']\n            distance_to_target = math.sqrt(dx*dx + dy*dy)\n\n            if distance_to_target < 0.5:  # Close enough to target\n                cmd.linear.x = 0.0\n                cmd.angular.z = 0.0\n                self.navigation_state = 'reached_target'\n            else:\n                # Calculate desired heading\n                desired_theta = math.atan2(dy, dx)\n                angle_diff = desired_theta - self.robot_pose['theta']\n\n                # Normalize angle difference\n                while angle_diff > math.pi:\n                    angle_diff -= 2 * math.pi\n                while angle_diff < -math.pi:\n                    angle_diff += 2 * math.pi\n\n                # PID-like control for angular velocity\n                angular_kp = 1.0\n                cmd.angular.z = angular_kp * angle_diff\n\n                # Limit angular velocity\n                cmd.angular.z = max(-1.0, min(1.0, cmd.angular.z))\n\n                # Set linear velocity based on angular error\n                if abs(angle_diff) < 0.2:  # Only move forward if roughly aligned\n                    cmd.linear.x = 0.5\n                else:\n                    cmd.linear.x = 0.1  # Slow down when turning\n\n                self.navigation_state = 'navigating'\n\n        # Publish command\n        self.cmd_vel_publisher.publish(cmd)\n\n        # Log navigation state\n        self.get_logger().info(f'Navigation: {self.navigation_state}, '\n                              f'Pos: ({self.robot_pose[\"x\"]:.2f}, {self.robot_pose[\"y\"]:.2f}), '\n                              f'Obstacle: {self.obstacle_detected}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    navigation_node = NavigationNode()\n\n    try:\n        rclpy.spin(navigation_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        navigation_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h4,{id:"step-3-create-launch-file",children:"Step 3: Create Launch File"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"robot_model_pkg/launch/robot_navigation.launch.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Get package share directory\n    pkg_share = get_package_share_directory('robot_model_pkg')\n\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation (Gazebo) clock if true'),\n\n        # Robot state publisher\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            name='robot_state_publisher',\n            output='screen',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'robot_description': open(os.path.join(pkg_share, 'urdf', 'simple_robot.urdf')).read()\n            }]),\n\n        # Joint state publisher\n        Node(\n            package='joint_state_publisher',\n            executable='joint_state_publisher',\n            name='joint_state_publisher',\n            parameters=[{'use_sim_time': use_sim_time}]),\n\n        # Navigation node\n        Node(\n            package='robot_model_pkg',\n            executable='navigation_node',\n            name='navigation_node',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'),\n\n        # RViz2 for visualization\n        Node(\n            package='rviz2',\n            executable='rviz2',\n            name='rviz2',\n            arguments=['-d', os.path.join(pkg_share, 'config', 'robot_navigation.rviz')],\n            parameters=[{'use_sim_time': use_sim_time}],\n            condition=launch.conditions.IfCondition(LaunchConfiguration('rviz', default='true'))),\n    ])\n"})}),"\n",(0,i.jsx)(n.h4,{id:"step-4-build-and-test-1",children:"Step 4: Build and Test"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Build the package\ncd ~/ros2_labs/ws_robot_model\ncolcon build --packages-select robot_model_pkg\n\n# Source the workspace\nsource install/setup.bash\n\n# Launch the robot model\nros2 launch robot_model_pkg robot_navigation.launch.py\n"})}),"\n",(0,i.jsx)(n.h3,{id:"lab-exercises-1",children:"Lab Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Add more sophisticated navigation algorithms (Dijkstra, A*)"}),"\n",(0,i.jsx)(n.li,{children:"Implement obstacle mapping using laser scan data"}),"\n",(0,i.jsx)(n.li,{children:"Add camera processing for visual navigation"}),"\n",(0,i.jsx)(n.li,{children:"Create a simple SLAM system using the sensor data"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"expected-results-1",children:"Expected Results"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Working URDF robot model"}),"\n",(0,i.jsx)(n.li,{children:"Navigation system that can avoid obstacles"}),"\n",(0,i.jsx)(n.li,{children:"Proper integration of sensor data"}),"\n",(0,i.jsx)(n.li,{children:"Demonstrated understanding of robot state and control"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lab-3-ai-integration-with-ros-2",children:"Lab 3: AI Integration with ROS 2"}),"\n",(0,i.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Integrate a simple AI model (computer vision) with ROS 2 for object detection and navigation."}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites-2",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Labs 1 and 2 completed"}),"\n",(0,i.jsx)(n.li,{children:"Python OpenCV and NumPy installed"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of rclpy integration"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-setup-2",children:"Lab Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/ros2_labs/ws_ai_integration/src\ncd ~/ros2_labs/ws_ai_integration\n\n# Create package\nros2 pkg create --build-type ament_python ai_integration_pkg --dependencies rclpy std_msgs sensor_msgs geometry_msgs cv_bridge\n"})}),"\n",(0,i.jsx)(n.h3,{id:"implementation-steps-2",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h4,{id:"step-1-create-ai-processing-node",children:"Step 1: Create AI Processing Node"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"ai_integration_pkg/ai_integration_pkg/object_detection_node.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, Point\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport math\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n\n        # CV Bridge for image conversion\n        self.bridge = CvBridge()\n\n        # Publishers\n        self.cmd_vel_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n        self.detection_publisher = self.create_publisher(String, 'object_detections', 10)\n        self.target_publisher = self.create_publisher(Point, 'target_location', 10)\n\n        # Subscribers\n        self.image_subscription = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        self.scan_subscription = self.create_subscription(\n            LaserScan, 'scan', self.scan_callback, 10)\n\n        # Timers\n        self.ai_processing_timer = self.create_timer(0.2, self.ai_processing_loop)\n\n        # AI state\n        self.latest_image = None\n        self.latest_scan = None\n        self.detections = []\n        self.target_location = None\n        self.avoid_obstacles = True\n\n        # AI model parameters\n        self.hsv_lower_red1 = np.array([0, 50, 50])\n        self.hsv_upper_red1 = np.array([10, 255, 255])\n        self.hsv_lower_red2 = np.array([170, 50, 50])\n        self.hsv_upper_red2 = np.array([180, 255, 255])\n\n        self.get_logger().info('Object Detection Node Started')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.latest_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f'Error converting image: {e}')\n\n    def scan_callback(self, msg):\n        \"\"\"Process incoming laser scan\"\"\"\n        self.latest_scan = msg\n\n    def ai_processing_loop(self):\n        \"\"\"Main AI processing loop\"\"\"\n        if self.latest_image is not None:\n            # Run object detection\n            self.detections = self.detect_objects(self.latest_image)\n\n            # Publish detection results\n            if self.detections:\n                detection_msg = String()\n                detection_msg.data = f'Detected {len(self.detections)} objects'\n                self.detection_publisher.publish(detection_msg)\n\n                # Select target (closest red object)\n                self.target_location = self.select_target(self.detections)\n\n                if self.target_location:\n                    # Publish target location\n                    target_msg = Point()\n                    target_msg.x = self.target_location['x']\n                    target_msg.y = self.target_location['y']\n                    target_msg.z = 0.0\n                    self.target_publisher.publish(target_msg)\n\n                    # Generate navigation command\n                    cmd = self.generate_navigation_command()\n                    self.cmd_vel_publisher.publish(cmd)\n            else:\n                # No objects detected, continue exploration\n                cmd = self.explore_behavior()\n                self.cmd_vel_publisher.publish(cmd)\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects using color-based segmentation\"\"\"\n        # Convert BGR to HSV\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Create masks for red color (red wraps around in HSV)\n        mask1 = cv2.inRange(hsv, self.hsv_lower_red1, self.hsv_upper_red1)\n        mask2 = cv2.inRange(hsv, self.hsv_lower_red2, self.hsv_upper_red2)\n        mask = cv2.bitwise_or(mask1, mask2)\n\n        # Apply morphological operations to reduce noise\n        kernel = np.ones((5,5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        detections = []\n\n        for contour in contours:\n            # Filter by area (avoid tiny detections)\n            area = cv2.contourArea(contour)\n            if area > 100:  # Minimum area threshold\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n\n                # Calculate center and relative position\n                center_x = x + w // 2\n                center_y = y + h // 2\n\n                # Convert to relative coordinates (0-1)\n                img_height, img_width = image.shape[:2]\n                rel_x = center_x / img_width\n                rel_y = center_y / img_height\n\n                detection = {\n                    'center': (center_x, center_y),\n                    'bbox': (x, y, w, h),\n                    'relative_pos': (rel_x, rel_y),\n                    'area': area\n                }\n\n                detections.append(detection)\n\n        # Sort detections by area (largest first)\n        detections.sort(key=lambda x: x['area'], reverse=True)\n\n        # Visualize detections (optional)\n        vis_image = image.copy()\n        for detection in detections:\n            x, y, w, h = detection['bbox']\n            cv2.rectangle(vis_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            cv2.circle(vis_image, detection['center'], 5, (255, 0, 0), -1)\n\n        # Display visualization (for debugging)\n        cv2.imshow('Object Detection', vis_image)\n        cv2.waitKey(1)\n\n        return detections\n\n    def select_target(self, detections):\n        \"\"\"Select the best target from detections\"\"\"\n        if not detections:\n            return None\n\n        # For now, select the largest detection\n        largest_detection = detections[0]\n\n        # Convert relative position to navigation target\n        rel_x, rel_y = largest_detection['relative_pos']\n\n        # Map relative position to navigation commands\n        # This is a simplified mapping - in reality, you'd use more sophisticated logic\n        target = {\n            'x': rel_x * 2 - 1,  # Convert 0-1 to -1 to 1\n            'y': (1 - rel_y) * 2 - 1,  # Convert 0-1 to 1 to -1 (invert Y)\n            'area': largest_detection['area']\n        }\n\n        return target\n\n    def generate_navigation_command(self):\n        \"\"\"Generate navigation command based on target\"\"\"\n        cmd = Twist()\n\n        if self.target_location:\n            # Simple proportional controller\n            kp_linear = 0.5\n            kp_angular = 1.0\n\n            # Target relative to center of image (-1 to 1)\n            target_x = self.target_location['x']  # Horizontal position\n            target_y = self.target_location['y']  # Vertical position (inverted)\n\n            # Move toward target\n            cmd.linear.x = kp_linear * min(1.0, max(-1.0, 1 - abs(target_y)))  # Move forward based on vertical position\n            cmd.angular.z = -kp_angular * target_x  # Turn toward horizontal position\n\n            # Adjust for object size (move closer to larger objects)\n            size_factor = min(1.0, self.target_location['area'] / 10000)\n            cmd.linear.x *= size_factor\n\n        return cmd\n\n    def explore_behavior(self):\n        \"\"\"Behavior when no objects are detected\"\"\"\n        cmd = Twist()\n\n        # Simple exploration pattern: move forward unless obstacle detected\n        if self.latest_scan and len(self.latest_scan.ranges) > 0:\n            # Check for obstacles in front\n            front_ranges = self.latest_scan.ranges[len(self.latest_scan.ranges)//2-10:len(self.latest_scan.ranges)//2+10]\n            min_range = min([r for r in front_ranges if 0 < r < float('inf')], default=float('inf'))\n\n            if min_range < 0.8:  # Obstacle too close\n                cmd.linear.x = 0.0\n                cmd.angular.z = 0.3  # Turn right\n            else:\n                cmd.linear.x = 0.3  # Move forward\n                cmd.angular.z = 0.0\n        else:\n            # Default exploration\n            cmd.linear.x = 0.2\n            cmd.angular.z = 0.0\n\n        return cmd\n\ndef main(args=None):\n    rclpy.init(args=args)\n    object_detection_node = ObjectDetectionNode()\n\n    try:\n        rclpy.spin(object_detection_node)\n    except KeyboardInterrupt:\n        cv2.destroyAllWindows()\n        pass\n    finally:\n        object_detection_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h4,{id:"step-2-update-package-configuration",children:"Step 2: Update Package Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["Update ",(0,i.jsx)(n.code,{children:"ai_integration_pkg/package.xml"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>ai_integration_pkg</name>\n  <version>0.0.0</version>\n  <description>AI integration package for ROS 2 lab exercises</description>\n  <maintainer email="student@university.edu">Student</maintainer>\n  <license>Apache-2.0</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>cv_bridge</depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Update ",(0,i.jsx)(n.code,{children:"ai_integration_pkg/setup.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\n\npackage_name = 'ai_integration_pkg'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Student',\n    maintainer_email='student@university.edu',\n    description='AI integration package for ROS 2 lab exercises',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'object_detection_node = ai_integration_pkg.object_detection_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"step-3-build-and-test",children:"Step 3: Build and Test"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Build the package\ncd ~/ros2_labs/ws_ai_integration\ncolcon build --packages-select ai_integration_pkg\n\n# Source the workspace\nsource install/setup.bash\n\n# Run the AI integration node\nros2 run ai_integration_pkg object_detection_node\n"})}),"\n",(0,i.jsx)(n.h3,{id:"lab-exercises-2",children:"Lab Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement more sophisticated object detection (using deep learning models)"}),"\n",(0,i.jsx)(n.li,{children:"Add multiple object tracking capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Integrate with a real camera feed"}),"\n",(0,i.jsx)(n.li,{children:"Create a more complex navigation behavior based on object properties"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"expected-results-2",children:"Expected Results"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Working object detection system"}),"\n",(0,i.jsx)(n.li,{children:"AI-driven navigation based on visual input"}),"\n",(0,i.jsx)(n.li,{children:"Proper integration of computer vision with ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Demonstrated understanding of AI-ROS integration"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lab-report-template",children:"Lab Report Template"}),"\n",(0,i.jsx)(n.h3,{id:"lab-documentation-requirements",children:"Lab Documentation Requirements"}),"\n",(0,i.jsx)(n.p,{children:"Each lab should include:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Objective"}),": Clear statement of what was implemented"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implementation"}),": Code snippets and explanations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Results"}),": What worked, what didn't, and why"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Analysis"}),": Discussion of challenges and solutions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Extensions"}),": Ideas for improvement or additional features"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": Does the implementation work as expected?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Code Quality"}),": Is the code well-structured and documented?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understanding"}),": Does the student demonstrate understanding of concepts?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Creativity"}),": Are there innovative solutions or extensions?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Problem-Solving"}),": How effectively were challenges addressed?"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-communication-issues",children:"ROS 2 Communication Issues"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Node Discovery"}),": Ensure proper network configuration and domain IDs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Topic Names"}),": Verify topic names match between publishers and subscribers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Message Types"}),": Ensure correct message types are used"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"QoS Settings"}),": Check QoS policies match between nodes"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CPU Usage"}),": Monitor CPU usage and optimize processing loops"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Leaks"}),": Use proper cleanup and avoid circular references"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Timing"}),": Ensure proper timer intervals for real-time performance"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"simulation-issues",children:"Simulation Issues"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Loading"}),": Verify URDF files are properly formatted"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physics Parameters"}),": Check mass, inertia, and friction values"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Data"}),": Validate sensor ranges and data quality"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"How do you debug communication issues between ROS 2 nodes?"}),"\n",(0,i.jsx)(n.li,{children:"What are common performance bottlenecks in Python ROS 2 nodes?"}),"\n",(0,i.jsx)(n.li,{children:"How do you handle real-time constraints in Python-based robotic systems?"}),"\n",(0,i.jsx)(n.li,{children:"What are the advantages of using launch files for system deployment?"}),"\n",(0,i.jsx)(n.li,{children:"How would you extend the basic navigation system to handle dynamic obstacles?"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"After completing these practical labs, students should be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design and implement complex ROS 2 systems"}),"\n",(0,i.jsx)(n.li,{children:"Integrate AI/ML models with robotic platforms"}),"\n",(0,i.jsx)(n.li,{children:"Create robust and efficient robotic applications"}),"\n",(0,i.jsx)(n.li,{children:"Apply learned concepts to real-world robotics challenges"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These hands-on labs provide essential practical experience that bridges the gap between theoretical knowledge and real-world robotic system implementation."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var t=s(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);