"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[5068],{7969:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"capstone/end-to-end-pipeline","title":"End-to-End Pipeline","description":"Overview","source":"@site/docs/capstone/end-to-end-pipeline.md","sourceDirName":"capstone","slug":"/capstone/end-to-end-pipeline","permalink":"/docs/capstone/end-to-end-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone/end-to-end-pipeline.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Capstone: Autonomous Humanoid Architecture","permalink":"/docs/capstone/capstone-architecture"},"next":{"title":"Capstone Project Evaluation","permalink":"/docs/capstone/capstone-evaluation"}}');var s=t(4848),a=t(8453);const o={sidebar_position:2},r="End-to-End Pipeline",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Pipeline Architecture",id:"pipeline-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:2},{value:"Speech Recognition Module",id:"speech-recognition-module",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Task Planning and Execution",id:"task-planning-and-execution",level:2},{value:"High-Level Planner",id:"high-level-planner",level:3},{value:"Perception Integration",id:"perception-integration",level:2},{value:"Multi-Sensor Fusion Node",id:"multi-sensor-fusion-node",level:3},{value:"Safety and Validation Layer",id:"safety-and-validation-layer",level:2},{value:"Safety Monitor Node",id:"safety-monitor-node",level:3},{value:"System Integration and Launch",id:"system-integration-and-launch",level:2},{value:"Main Launch File",id:"main-launch-file",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Real-time Considerations",id:"real-time-considerations",level:3},{value:"Resource Management",id:"resource-management",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Configuration Management",id:"configuration-management",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"end-to-end-pipeline",children:"End-to-End Pipeline"})}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(e.p,{children:"This section details the complete end-to-end pipeline for the autonomous humanoid system with conversational AI. The pipeline integrates all modules learned throughout the course into a cohesive, functional system."}),"\n",(0,s.jsx)(e.h2,{id:"pipeline-architecture",children:"Pipeline Architecture"}),"\n",(0,s.jsx)(e.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    User Interaction                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Voice Input    \u2502  \u2502  Text Input     \u2502  \u2502  Gesture    \u2502 \u2502\n\u2502  \u2502  Recognition    \u2502  \u2502  Processing     \u2502  \u2502  Recognition\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502 Natural Language Input\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Natural Language Processing                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Speech-to-    \u2502  \u2502  Intent         \u2502  \u2502  Context    \u2502 \u2502\n\u2502  \u2502  Text          \u2502  \u2502  Classification  \u2502  \u2502  Modeling   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502 Processed Intent\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Task Planning & Reasoning                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  High-level     \u2502  \u2502  Action         \u2502  \u2502  Safety      \u2502 \u2502\n\u2502  \u2502  Planning       \u2502  \u2502  Sequencing     \u2502  \u2502  Validation  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502 Action Commands\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   ROS 2 Control Layer                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Navigation     \u2502  \u2502  Manipulation   \u2502  \u2502  Humanoid   \u2502 \u2502\n\u2502  \u2502  (Nav2)         \u2502  \u2502  (MoveIt)       \u2502  \u2502  Control    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502 Low-level Commands\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Robot Execution Layer                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Perception     \u2502  \u2502  Actuation      \u2502  \u2502  Feedback   \u2502 \u2502\n\u2502  \u2502  (Sensors)      \u2502  \u2502  (Motors)       \u2502  \u2502  Loop       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.h2,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,s.jsx)(e.h3,{id:"speech-recognition-module",children:"Speech Recognition Module"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport speech_recognition as sr\n\nclass VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__('voice_to_action_node')\n        self.subscription = self.create_subscription(\n            AudioData,\n            'audio_input',\n            self.audio_callback,\n            10)\n        self.command_publisher = self.create_publisher(\n            String,\n            'voice_command',\n            10)\n\n        self.recognizer = sr.Recognizer()\n        self.recognizer.energy_threshold = 3000\n\n    def audio_callback(self, msg):\n        # Convert audio data to text\n        audio_data = sr.AudioData(\n            msg.data,\n            sample_rate=msg.sample_rate,\n            sample_width=msg.sample_width\n        )\n\n        try:\n            text = self.recognizer.recognize_google(audio_data)\n            self.get_logger().info(f'Recognized: {text}')\n\n            # Publish recognized text\n            cmd_msg = String()\n            cmd_msg.data = text\n            self.command_publisher.publish(cmd_msg)\n\n        except sr.UnknownValueError:\n            self.get_logger().warn('Could not understand audio')\n        except sr.RequestError as e:\n            self.get_logger().error(f'Error: {e}')\n"})}),"\n",(0,s.jsx)(e.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import openai\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass NLUProcessor(Node):\n    def __init__(self):\n        super().__init__(\'nlu_processor\')\n        self.subscription = self.create_subscription(\n            String,\n            \'voice_command\',\n            self.command_callback,\n            10)\n        self.intent_publisher = self.create_publisher(\n            String,\n            \'intent\',\n            10)\n\n        # OpenAI API configuration\n        self.client = openai.OpenAI(api_key=\'your-api-key\')\n\n    def command_callback(self, msg):\n        # Process natural language command\n        user_command = msg.data\n\n        # Define the system prompt for intent classification\n        system_prompt = """\n        You are a robot command interpreter. Your role is to:\n        1. Classify the intent of the user command\n        2. Extract relevant parameters\n        3. Return a structured JSON response\n\n        Intents include:\n        - navigation: Move to a location\n        - manipulation: Pick up, place, or manipulate an object\n        - information: Ask for information about the environment\n        - interaction: Social interaction commands\n        - system: System control commands\n        """\n\n        try:\n            response = self.client.chat.completions.create(\n                model="gpt-3.5-turbo",\n                messages=[\n                    {"role": "system", "content": system_prompt},\n                    {"role": "user", "content": f"Command: {user_command}"}\n                ],\n                response_format={"type": "json_object"}\n            )\n\n            intent_data = response.choices[0].message.content\n            # Publish intent to next stage\n            intent_msg = String()\n            intent_msg.data = intent_data\n            self.intent_publisher.publish(intent_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'NLU processing error: {e}\')\n'})}),"\n",(0,s.jsx)(e.h2,{id:"task-planning-and-execution",children:"Task Planning and Execution"}),"\n",(0,s.jsx)(e.h3,{id:"high-level-planner",children:"High-Level Planner"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import json\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\n\nclass TaskPlanner(Node):\n    def __init__(self):\n        super().__init__('task_planner')\n        self.subscription = self.create_subscription(\n            String,\n            'intent',\n            self.intent_callback,\n            10)\n\n        # Publishers for different action types\n        self.nav_publisher = self.create_publisher(\n            PoseStamped,\n            'navigation_goal',\n            10)\n        self.manipulation_publisher = self.create_publisher(\n            String,\n            'manipulation_command',\n            10)\n\n    def intent_callback(self, msg):\n        try:\n            intent_data = json.loads(msg.data)\n            intent_type = intent_data.get('intent', 'unknown')\n\n            if intent_type == 'navigation':\n                self.handle_navigation(intent_data)\n            elif intent_type == 'manipulation':\n                self.handle_manipulation(intent_data)\n            elif intent_type == 'information':\n                self.handle_information(intent_data)\n            else:\n                self.get_logger().warn(f'Unknown intent: {intent_type}')\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in intent data')\n\n    def handle_navigation(self, intent_data):\n        # Extract navigation parameters\n        location = intent_data.get('parameters', {}).get('location', 'unknown')\n\n        # Convert location to coordinates (simplified)\n        pose = self.get_location_pose(location)\n\n        # Publish navigation goal\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = 'map'\n        goal_msg.pose = pose\n        self.nav_publisher.publish(goal_msg)\n\n    def handle_manipulation(self, intent_data):\n        # Extract manipulation parameters\n        action = intent_data.get('parameters', {}).get('action', 'unknown')\n        object_name = intent_data.get('parameters', {}).get('object', 'unknown')\n\n        # Create manipulation command\n        cmd_msg = String()\n        cmd_msg.data = f\"{action} {object_name}\"\n        self.manipulation_publisher.publish(cmd_msg)\n\n    def get_location_pose(self, location):\n        # Simplified location mapping\n        locations = {\n            'kitchen': [1.0, 2.0, 0.0],\n            'living room': [3.0, 1.0, 0.0],\n            'bedroom': [0.0, 4.0, 0.0],\n            'office': [2.0, 0.0, 0.0]\n        }\n\n        if location in locations:\n            x, y, theta = locations[location]\n            pose = Pose()\n            pose.position.x = x\n            pose.position.y = y\n            pose.position.z = 0.0\n            pose.orientation = self.euler_to_quaternion(0, 0, theta)\n            return pose\n        else:\n            return Pose()  # Default pose\n\n    def euler_to_quaternion(self, roll, pitch, yaw):\n        # Convert Euler angles to quaternion\n        import math\n        cy = math.cos(yaw * 0.5)\n        sy = math.sin(yaw * 0.5)\n        cp = math.cos(pitch * 0.5)\n        sp = math.sin(pitch * 0.5)\n        cr = math.cos(roll * 0.5)\n        sr = math.sin(roll * 0.5)\n\n        w = cr * cp * cy + sr * sp * sy\n        x = sr * cp * cy - cr * sp * sy\n        y = cr * sp * cy + sr * cp * sy\n        z = cr * cp * sy - sr * sp * cy\n\n        return Quaternion(x=x, y=y, z=z, w=w)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"perception-integration",children:"Perception Integration"}),"\n",(0,s.jsx)(e.h3,{id:"multi-sensor-fusion-node",children:"Multi-Sensor Fusion Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan\nfrom geometry_msgs.msg import PointStamped\nimport cv2\nimport numpy as np\n\nclass PerceptionFusion(Node):\n    def __init__(self):\n        super().__init__(\'perception_fusion\')\n\n        # Multiple sensor subscriptions\n        self.image_sub = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10)\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.lidar_callback,\n            10)\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2,\n            \'pointcloud\',\n            self.pointcloud_callback,\n            10)\n\n        # Perception result publisher\n        self.perception_pub = self.create_publisher(\n            String,\n            \'perception_results\',\n            10)\n\n    def image_callback(self, msg):\n        # Process image data\n        image = self.ros_image_to_cv2(msg)\n\n        # Object detection using OpenCV or custom model\n        detections = self.detect_objects(image)\n\n        # Publish detection results\n        self.publish_perception_results(detections)\n\n    def lidar_callback(self, msg):\n        # Process LIDAR data for obstacle detection\n        ranges = np.array(msg.ranges)\n        obstacles = self.detect_obstacles(ranges)\n\n        # Publish obstacle information\n        self.publish_perception_results(obstacles)\n\n    def pointcloud_callback(self, msg):\n        # Process point cloud data for 3D understanding\n        pointcloud_data = self.process_pointcloud(msg)\n\n        # Extract 3D features\n        features = self.extract_3d_features(pointcloud_data)\n\n        # Publish 3D features\n        self.publish_perception_results(features)\n\n    def ros_image_to_cv2(self, ros_image):\n        # Convert ROS image message to OpenCV format\n        dtype = np.uint8\n        img = np.frombuffer(ros_image.data, dtype=dtype)\n        img = img.reshape(ros_image.height, ros_image.width, 3)\n        return img\n\n    def detect_objects(self, image):\n        # Simplified object detection\n        # In practice, use YOLO, SSD, or custom model\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect simple shapes or use pre-trained model\n        # Return detected objects with confidence scores\n        return {"objects": [], "confidence": 0.0}\n\n    def detect_obstacles(self, ranges):\n        # Detect obstacles from LIDAR ranges\n        min_distance = 0.5  # meters\n        obstacle_angles = []\n\n        for i, range_val in enumerate(ranges):\n            if range_val < min_distance and not np.isnan(range_val):\n                obstacle_angles.append(i)\n\n        return {"obstacles": obstacle_angles, "count": len(obstacle_angles)}\n\n    def process_pointcloud(self, pointcloud_msg):\n        # Process point cloud data\n        # Extract 3D information\n        return {"points": [], "features": []}\n\n    def extract_3d_features(self, pointcloud_data):\n        # Extract 3D features from point cloud\n        return {"surfaces": [], "objects": [], "boundaries": []}\n\n    def publish_perception_results(self, results):\n        # Publish combined perception results\n        results_msg = String()\n        results_msg.data = str(results)\n        self.perception_pub.publish(results_msg)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"safety-and-validation-layer",children:"Safety and Validation Layer"}),"\n",(0,s.jsx)(e.h3,{id:"safety-monitor-node",children:"Safety Monitor Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist\nimport numpy as np\n\nclass SafetyMonitor(Node):\n    def __init__(self):\n        super().__init__('safety_monitor')\n\n        # Subscribe to commands and sensor data\n        self.cmd_sub = self.create_subscription(\n            Twist,\n            'cmd_vel',\n            self.command_callback,\n            10)\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.lidar_callback,\n            10)\n\n        # Override command publisher for safety\n        self.safety_cmd_pub = self.create_publisher(\n            Twist,\n            'safety_cmd_vel',\n            10)\n\n        # Safety status publisher\n        self.safety_status_pub = self.create_publisher(\n            String,\n            'safety_status',\n            10)\n\n        self.safety_enabled = True\n        self.min_distance = 0.5  # meters\n        self.emergency_stop = False\n\n    def command_callback(self, msg):\n        if not self.safety_enabled:\n            self.safety_cmd_pub.publish(msg)\n            return\n\n        # Check if command is safe based on sensor data\n        if self.emergency_stop:\n            # Publish stop command\n            stop_cmd = Twist()\n            self.safety_cmd_pub.publish(stop_cmd)\n            self.get_logger().warn('EMERGENCY STOP ACTIVATED')\n            return\n\n        # Validate command based on obstacle proximity\n        safe_cmd = self.validate_command(msg)\n        self.safety_cmd_pub.publish(safe_cmd)\n\n    def lidar_callback(self, msg):\n        # Check for obstacles in path\n        ranges = np.array(msg.ranges)\n\n        # Check forward direction (simplified)\n        forward_ranges = ranges[len(ranges)//2-30:len(ranges)//2+30]\n        min_range = np.min(forward_ranges)\n\n        if min_range < self.min_distance:\n            self.emergency_stop = True\n            self.get_logger().warn(f'OBSTACLE DETECTED: {min_range:.2f}m')\n        else:\n            self.emergency_stop = False\n\n    def validate_command(self, cmd):\n        # Validate and potentially modify command for safety\n        if self.emergency_stop:\n            # Override with stop command\n            safe_cmd = Twist()\n        elif cmd.linear.x > 0 and self.is_path_blocked():\n            # Reduce speed or stop if path is blocked\n            safe_cmd = Twist()\n            safe_cmd.linear.x = min(cmd.linear.x, 0.1)  # Reduced speed\n            safe_cmd.angular.z = cmd.angular.z\n        else:\n            # Command is safe, pass through\n            safe_cmd = cmd\n\n        return safe_cmd\n\n    def is_path_blocked(self):\n        # Check if robot's path is blocked\n        # This would typically use more sophisticated path planning\n        return self.emergency_stop\n\n    def enable_safety(self):\n        self.safety_enabled = True\n        self.get_logger().info('Safety system enabled')\n\n    def disable_safety(self):\n        self.safety_enabled = False\n        self.get_logger().warn('Safety system disabled - PROCEED WITH CAUTION')\n"})}),"\n",(0,s.jsx)(e.h2,{id:"system-integration-and-launch",children:"System Integration and Launch"}),"\n",(0,s.jsx)(e.h3,{id:"main-launch-file",children:"Main Launch File"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<launch>\n  \x3c!-- Voice recognition node --\x3e\n  <node pkg="voice_to_action" exec="voice_to_action_node" name="voice_to_action">\n    <param name="model" value="vosk-model-small-en-us-0.15"/>\n  </node>\n\n  \x3c!-- Natural Language Understanding --\x3e\n  <node pkg="nlu_processor" exec="nlu_processor" name="nlu_processor">\n    <param name="openai_api_key" value="$(var openai_api_key)"/>\n  </node>\n\n  \x3c!-- Task Planner --\x3e\n  <node pkg="task_planner" exec="task_planner" name="task_planner"/>\n\n  \x3c!-- Navigation Stack --\x3e\n  <include file="$(find-pkg-share nav2_bringup)/launch/navigation_launch.py">\n    <arg name="use_sim_time" value="true"/>\n  </include>\n\n  \x3c!-- Manipulation Stack --\x3e\n  <include file="$(find-pkg-share moveit_bringup)/launch/moveit.launch.py"/>\n\n  \x3c!-- Perception Fusion --\x3e\n  <node pkg="perception_fusion" exec="perception_fusion" name="perception_fusion"/>\n\n  \x3c!-- Safety Monitor --\x3e\n  <node pkg="safety_monitor" exec="safety_monitor" name="safety_monitor">\n    <param name="min_distance" value="0.5"/>\n  </node>\n\n  \x3c!-- Humanoid Control --\x3e\n  <node pkg="humanoid_control" exec="humanoid_controller" name="humanoid_controller"/>\n\n  \x3c!-- Main coordinator --\x3e\n  <node pkg="capstone_coordinator" exec="coordinator" name="coordinator"/>\n</launch>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(e.h3,{id:"real-time-considerations",children:"Real-time Considerations"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Threading"}),": Use multi-threaded executors for concurrent processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Message Queues"}),": Optimize queue sizes for real-time performance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Computational Efficiency"}),": Optimize algorithms for real-time execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory Management"}),": Use efficient data structures and memory pools"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"CPU Affinity"}),": Assign critical nodes to specific CPU cores"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Priority Scheduling"}),": Set real-time priorities for safety-critical nodes"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory Allocation"}),": Pre-allocate memory for performance-critical operations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"I/O Optimization"}),": Optimize sensor data processing pipelines"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,s.jsx)(e.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import unittest\nfrom task_planner import TaskPlanner\n\nclass TestTaskPlanner(unittest.TestCase):\n    def setUp(self):\n        self.planner = TaskPlanner()\n\n    def test_navigation_intent(self):\n        # Test navigation intent processing\n        intent_data = \'{"intent": "navigation", "parameters": {"location": "kitchen"}}\'\n        result = self.planner.process_intent(intent_data)\n        self.assertEqual(result[\'action\'], \'navigate\')\n\n    def test_manipulation_intent(self):\n        # Test manipulation intent processing\n        intent_data = \'{"intent": "manipulation", "parameters": {"action": "pick", "object": "cup"}}\'\n        result = self.planner.process_intent(intent_data)\n        self.assertEqual(result[\'action\'], \'manipulate\')\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation Testing"}),": Test complete pipeline in Gazebo simulation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hardware-in-the-loop"}),": Test with real sensors and actuators"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Performance Testing"}),": Validate real-time performance requirements"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety Testing"}),": Verify safety system functionality"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,s.jsx)(e.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Compute"}),": NVIDIA Jetson Orin AGX or equivalent"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensors"}),": RGB-D camera, LIDAR, IMU, microphones"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Actuators"}),": Humanoid robot platform with manipulators"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Connectivity"}),": Robust network connectivity for AI services"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameter Files"}),": Organize parameters in YAML files"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Environment Variables"}),": Use environment variables for API keys"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Launch Files"}),": Create different launch configurations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Docker"}),": Containerize components for deployment"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"This end-to-end pipeline demonstrates the integration of all course modules into a comprehensive autonomous humanoid system with conversational AI capabilities. Each component builds upon the previous modules to create a sophisticated, safe, and functional robotic system."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const s={},a=i.createContext(s);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);